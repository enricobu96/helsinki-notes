---
attachments: [Clipboard_2021-12-01-12-26-01.png, Clipboard_2021-12-01-12-26-30.png, Clipboard_2021-12-01-12-26-46.png, Clipboard_2021-12-01-12-28-23.png, Clipboard_2021-12-01-12-31-22.png, Clipboard_2021-12-01-12-37-24.png, Clipboard_2021-12-01-12-38-01.png, Clipboard_2021-12-01-12-48-52.png, Clipboard_2021-12-01-12-52-17.png, Clipboard_2021-12-01-12-52-27.png, Clipboard_2021-12-01-12-56-23.png, Clipboard_2021-12-01-12-58-45.png, Clipboard_2021-12-01-13-19-47.png, Clipboard_2021-12-01-13-20-36.png, Clipboard_2021-12-01-13-20-46.png, Clipboard_2021-12-05-18-06-56.png, Clipboard_2021-12-05-18-07-26.png, Clipboard_2021-12-05-18-25-32.png, Clipboard_2021-12-05-18-26-59.png, Clipboard_2021-12-05-18-30-32.png, Clipboard_2021-12-05-18-39-32.png]
tags: [Introduction to Machine Learning]
title: Lesson 9 - 01/12
created: '2021-12-01T10:15:04.984Z'
modified: '2021-12-05T16:42:00.095Z'
---

# Lesson 9 - 01/12

## Clustering

### Basic idea (flat clustering)

Find a clustering, _i.e._ a partition, of data points such that points in the same cluster are similar or nearby, and points in the different clusters are "dissimilar" or distant.
Typically, the numbers k of clusters is selected by the user.

![](@attachment/Clipboard_2021-12-01-12-26-01.png)

![](@attachment/Clipboard_2021-12-01-12-26-46.png)

**Partition** aka **clustering**: ![](@attachment/Clipboard_2021-12-01-12-28-23.png)

![](@attachment/Clipboard_2021-12-01-12-31-22.png)

### (Detour) Non-negative matrix factorization

Another way to treat clustering

![](@attachment/Clipboard_2021-12-01-12-37-24.png)

### Hierarchical clustering

![](@attachment/Clipboard_2021-12-01-12-38-01.png)

### Motivations

- Often number one data anysis algorithm in real world (cluster observations, customers etc.)
- In biology clustering of gene expressions
- Clustering observational data from Hyytiälä
- Trying to figure out a good classification schema (do the clustering and hope that the clusters correspond to the classes)

- Reducing the number of data points - e.g., reduce complexity by replacing data points with respective cluster centroids
- Quantization (lossy compression) - save disk space by representing each point by a “close enough” prototype
- Discretization of continuous variables

### K-means clustering

![](@attachment/Clipboard_2021-12-01-12-48-52.png)
This is hard clustering.

#### Lloyd's algorithm

Heuristic for k-means

![](@attachment/Clipboard_2021-12-01-12-52-17.png)

![](@attachment/Clipboard_2021-12-01-12-52-27.png)

Convergence:

![](@attachment/Clipboard_2021-12-01-12-56-23.png)

Also, you don't need to memorize the distance matrix.

Space and time complexity:

![](@attachment/Clipboard_2021-12-01-12-58-45.png)

Effect of initialization:

![](@attachment/Clipboard_2021-12-01-13-19-47.png)

-> How to select number of clusters

![](@attachment/Clipboard_2021-12-01-13-20-36.png)

![](@attachment/Clipboard_2021-12-01-13-20-46.png)

-> Matching clusters with class variables or other clutering

![](@attachment/Clipboard_2021-12-05-18-07-26.png)

### Hierarchical clustering

Binary trees with datapoints as leaves; cutting tree at any level produces a flat clustering.
!!! Adding new points to clusters afterwards is difficult (no cluster centroids)

![](@attachment/Clipboard_2021-12-05-18-25-32.png)

The height of the horizontal connectors indicates dissimilartiy between the combined clusters.

Algorithmic clustering -> usually we have no explicit cost function to optimize; then, two ways to find the dendrogram:
![](@attachment/Clipboard_2021-12-05-18-26-59.png)

The last one is the one we use (also in the exercise set 3)

### Linkage functions

![](@attachment/Clipboard_2021-12-05-18-30-32.png)

Difference between them?

![](@attachment/Clipboard_2021-12-05-18-39-32.png)
