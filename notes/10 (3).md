---
attachments: [Clipboard_2021-10-04-10-36-10.png, Clipboard_2021-10-04-10-36-36.png, Clipboard_2021-10-04-10-37-51.png, Clipboard_2021-10-04-10-44-09.png, Clipboard_2021-10-04-11-01-02.png, Clipboard_2021-10-04-11-31-54.png]
tags: [Introduction to Data Science]
title: Lesson 9 - 04/10
created: '2021-10-04T07:26:23.479Z'
modified: '2021-10-05T13:17:07.869Z'
---

# Lesson 9 - 04/10

## AI ethics

### The moral machine experiment

![](@attachment/Clipboard_2021-10-04-10-36-36.png)

![](@attachment/Clipboard_2021-10-04-10-37-51.png)

**!** Consistency of treatment != fairness
**!** Explainability != fairness

### Brief history of fairness-aware ML

![](@attachment/Clipboard_2021-10-04-10-44-09.png)

### And now, technical arguments

**Very important principle**: If there are biases in the data on which a model is trained, unless instructed otherwise, the learned model will carry those biases forward

#### Machine Learning and discrimination

- Discrimination = inferior treatment based on ascribed group rather than individual merits
- Discriminate = distinguish (we talk about this)

Machine learning:
- Uses proxies to distinguish an individual from the mean
- Without judging what is morally right or wrong
- Can enforce contraints defined by legislation and/or social norms

![](@attachment/Clipboard_2021-10-04-11-01-02.png)
We don't want `y` related to `s`.

#### Which variables are fair to use? In which circumstances?

- How about using gender -> in credit scoring, in sports prediction, in medical diagnosis

##### Redlining!

Variables are usually correlated with each other:
- Postal code with rance
- Working hours with gender
- Level of education with ethnicity

![](@attachment/Clipboard_2021-10-04-11-31-54.png)
