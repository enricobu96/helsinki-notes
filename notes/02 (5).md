---
attachments: [Clipboard_2022-02-03-19-33-44.png, Clipboard_2022-02-03-19-36-24.png, Clipboard_2022-02-03-19-39-22.png, Clipboard_2022-02-03-19-41-08.png, Clipboard_2022-02-03-19-41-35.png, Clipboard_2022-02-03-19-45-02.png, Clipboard_2022-02-03-20-21-40.png, Clipboard_2022-02-03-20-22-56.png, Clipboard_2022-02-04-11-03-17.png, Clipboard_2022-02-04-11-03-30.png]
tags: [Introduction to Deep Learning]
title: Lesson 5 - 02/02
created: '2022-02-03T17:22:35.102Z'
modified: '2022-02-04T09:03:30.872Z'
---

# Lesson 5 - 02/02

## Cross-entropy

![](@attachment/Clipboard_2022-02-03-19-33-44.png)

But actually we'll look at this one (that is exactly the same)

![](@attachment/Clipboard_2022-02-03-19-36-24.png)

Composed by two parts: $p(x)$ and $log(1/p(x))$. This is a weighted average according to the inverse of logarithm; the result is ![](@attachment/Clipboard_2022-02-03-19-39-22.png)

This is the negative log likelihood: ![](@attachment/Clipboard_2022-02-03-19-41-08.png)

![](@attachment/Clipboard_2022-02-03-19-41-35.png)

We can think of $k$ as the amount of information.

And this is cross-entropy
![](@attachment/Clipboard_2022-02-03-19-45-02.png)

## Back again on SGD

![](@attachment/Clipboard_2022-02-03-20-21-40.png)

![](@attachment/Clipboard_2022-02-03-20-22-56.png)

![](@attachment/Clipboard_2022-02-04-11-03-17.png)

![](@attachment/Clipboard_2022-02-04-11-03-30.png)
