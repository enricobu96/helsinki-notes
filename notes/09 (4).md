---
attachments: [Clipboard_2021-09-09-12-18-01.png, Clipboard_2021-09-09-12-24-46.png, Clipboard_2021-09-09-12-25-38.png, Clipboard_2021-09-09-12-29-13.png, Clipboard_2021-09-09-12-34-04.png, Clipboard_2021-09-09-12-36-48.png, Clipboard_2021-09-09-12-37-59.png, Clipboard_2021-09-09-12-43-21.png, Clipboard_2021-09-09-12-47-46.png, Clipboard_2021-09-09-12-54-01.png, Clipboard_2021-09-09-13-01-57.png, Clipboard_2021-09-09-13-03-20.png, Clipboard_2021-09-09-13-05-56.png, Clipboard_2021-09-09-13-08-32.png, Clipboard_2021-09-09-13-09-28.png, Clipboard_2021-09-09-13-10-15.png, Clipboard_2021-09-09-13-11-16.png, Clipboard_2021-09-09-13-13-02.png, Clipboard_2021-09-09-13-15-01.png, Clipboard_2021-09-09-13-21-12.png, Clipboard_2021-09-09-13-23-28.png, Clipboard_2021-09-09-13-23-45.png, Clipboard_2021-09-09-13-25-27.png, Clipboard_2021-09-09-13-25-48.png, Clipboard_2021-09-09-13-26-11.png, Clipboard_2021-09-09-13-26-33.png, Clipboard_2021-09-09-13-26-50.png, Clipboard_2021-09-09-13-27-11.png, Clipboard_2021-09-09-13-29-23.png, Clipboard_2021-09-09-13-38-27.png, Clipboard_2021-09-09-13-38-40.png, Clipboard_2021-09-09-13-38-53.png, Clipboard_2021-09-09-13-39-20.png, Clipboard_2021-09-09-13-39-31.png, Clipboard_2021-09-10-14-42-17.png, Clipboard_2021-09-10-14-43-14.png, Clipboard_2021-09-10-14-45-35.png, Clipboard_2021-09-10-14-46-47.png, Clipboard_2021-09-10-17-55-54.png, Clipboard_2021-09-10-17-57-25.png, Clipboard_2021-09-10-17-58-52.png, Clipboard_2021-09-10-18-00-52.png, Clipboard_2021-09-10-18-08-35.png, Clipboard_2021-09-10-18-08-42.png, Clipboard_2021-09-10-21-01-32.png, Clipboard_2021-09-10-21-04-51.png, Clipboard_2021-09-10-21-05-27.png]
tags: [Big Data Platforms]
title: Lesson 2 - 09/09
created: '2021-09-09T09:09:12.790Z'
modified: '2021-09-10T18:07:06.934Z'
---

# Lesson 2 - 09/09

## Apache Spark

- Distributed programming framework for Big Data processing
- Based on functional programming, and expands Google MapReduce framework
- Implements distributed Scala collections like interfaces for Scala, Java, Python and R
- Implemented on top of the Akka framework

### Google MapReduce
- Scalable batch processing framework developed at Google (for compuiting the web index)
- Still in production use but nobody writes new things
- When dealing with Big Data the only viable option is to use servers and hard disks in parallel (storage space and computational resources)
- Some of the challenges for storage is coming from Web servies to store and distribute pics and videos
- We need a system that can effectively utilize hard disk parallelism and both hide hdds and computer failures from the programmer (and also network failures) -> fault tolerance
- Tailored for batch processing (100k+ machines running in parallel)
- Typical job runtimes ~minutes/hours
- As a bonus we'd like to get increased programmer productivity compared to each programmer developing their own tools for utilizing hdd parallelism
- Takes care of all issues related to parallelization, synchronization, load balancing, fault tolerance -> hidden from the programmer, it thinks on itself
- The system needs to be **linearly scalable** to thousands of computers working in parallel -> very restricted programming model where the comminication between computer is really controlled
- **Apache Hadoop** is an open source MapReduce implementation (used by Yahoo!, Facebook and Twitter)

#### Google MapReduce and functional programming
Google MapReduce (and MapReduce method in general) is based on the functional programming language in the large:
- User is only allowed to write functions **without side-effect**. These function can be either Map or Reduce functions (it's up the user to not write side-effect functions)
- Re-execution is used for fault tolerance. If a computer that's executing Map or Reduce tasks fails to produce a result (due to hw failure), the task will be re-executed on another computer...side-effets in functions would make this **impossible**, as one could not re-create the environment in which the original task executed
- One just need fault tolerant storage of task inputs. MapReduce uses a fault tolerant distributed filesystem to store compute job I/O
- The map and reduce functions are written in a standard imperative programming language (Java, Python, Scala...)

##### Still on side-effect: why not?
Side-effect free programs will produce the same output irregardless the number of computers used by MapReduce (i.e. running the code on one machine produces the same results as running it in parallel).
Be careful! It's actually easy to introduce side-effect to MapReduce programs as the framework does not enforce a strict programming methodology -> Should be avoided!

### Map and reduce idea

You divide your data in different lists for different nodes, and then you have (key, value) pairs in which key is the name of the list and value is a list of values to compute.
1. **Mapping a list**
  Mapping a list applies the mapping function to each list element (in parallel) and outputs the list of mapped list elements.
  ![](@attachment/Clipboard_2021-09-10-14-42-17.png)

2. **Reducing a list**
  Reducing a list iterates over a list and produces an output created by the reduce function.
  ![](@attachment/Clipboard_2021-09-10-14-43-14.png)

3. **Grouping Map output by key to Reducer**
  In MapReduce the map function outputs ```(key,value)``` pairs. The MapReduce framework groups map outputs by key, and gives each reduce function instance ```(key,(...,list of values,...))``` pair as input. 
  N.B. each list of values having the same key can be processed in parallel.
  ![](@attachment/Clipboard_2021-09-10-14-45-35.png) 

#### MapReduce Data Flow (in practice)
Practical MapReduce systems split input data into large (like 64MB or more) blocks fed to user defined map functions.
![](@attachment/Clipboard_2021-09-10-14-46-47.png)

#### Recap
- The framework only allows a user to write two functions: a **Map** function and a **Reduce** function
- The map function is fed blocks of data (~64-128MB), and it produces `(key,value)` pairs
- The framework groups all values with the same key to a `(key,(...,list of values,...))` format, and there are then fed to the reduce function
- The grouping can be achieved by hashing by the key by the so called **_Shuffle_** phase
- Master process takes care of the scheduling and fault tolerance by re-executing mappers or reducers

### MapReduce diagram

![](@attachment/Clipboard_2021-09-09-12-54-01.png)

### Example: word count
This is like the Hello World for MapReduce. It's in Java but can be obv also done in Python or Scala.

```java
import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class WordCount {

  public static class TokenizerMapper
       extends Mapper<Object, Text, Text, IntWritable>{

    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();

    public void map(Object key, Text value, Context context
                    ) throws IOException, InterruptedException {
      StringTokenizer itr = new StringTokenizer(value.toString());
      while (itr.hasMoreTokens()) {
        word.set(itr.nextToken());
        context.write(word, one);
      }
    }
  }

  public static class IntSumReducer
       extends Reducer<Text,IntWritable,Text,IntWritable> {
    private IntWritable result = new IntWritable();

    public void reduce(Text key, Iterable<IntWritable> values,
                       Context context
                       ) throws IOException, InterruptedException {
      int sum = 0;
      for (IntWritable val : values) {
        sum += val.get();
      }
      result.set(sum);
      context.write(key, result);
    }
  }

  public static void main(String[] args) throws Exception {
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf, "word count");
    job.setJarByClass(WordCount.class);
    job.setMapperClass(TokenizerMapper.class);
    job.setCombinerClass(IntSumReducer.class);
    job.setReducerClass(IntSumReducer.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);
    FileInputFormat.addInputPath(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));
    System.exit(job.waitForCompletion(true) ? 0 : 1);
  }
}
```

Now consider doing a word count of the following using MapReduce:
```
Hello World Bye World
Hello Hadoop Goodbye Hadoop
```
Consider a Map function that reads in words on a time, and outputs `(word, 1)` for each parsed input word. The Map function output is:
```
(Hello, 1)
(World, 1)
(Bye, 1)
(World, 1)
(Hello, 1)
(Hadoop, 1)
(Goodbye, 1)
(Hadoop, 1)
```

The shuffle phase between Map and Reduce phases creates a list of values associated with each key a.k.a. it creates the following input for the Reduce phase:
```
(Bye, (1))
(Goodbye, (1))
(Hadoop, (1, 1))
(Hello, (1, 1))
(World, (1, 1))
```

Consider now a reduce function that sums the numbers in the list for each key and outputs `(word, count)` pairs. The final output is:
```
(Bye, 1)
(Goodbye, 1)
(Hadoop, 2)
(Hello, 2)
(World, 2)
```

## Going back to Apache Spark

### Why Apache Spark?
The MapReduce framework is still a widely used Big Data processing framework. Apache Spark is a newer framework that extends MapReduce with many additional features; MapReduce has several problems that Spark is able to address:
- Reading and storing data from main memory instead of hard disks (main memory is much faster)
- Running iterative algorithms much faster. Escpecially important for many machine learning algorithms

### Spark Benchmarking
- The original papers claim Spark to be up to 100x faster when data fits into RAM vs MapReduce, or up to 10x faster when data is on disks -> large speedups observed in the RAM vs HDD case
- However. independent benchmarks show when both use HDDs, Spark is up to 2.5-5x faster for CPU bound workloads, however MapReduce could still sort faster with hard drives.

### RDD a.k.a. Resilient Distributed Datasets
RDDs are distributed data structures that allow processed data to be distributed over several computers.
The framework stores the RDDs in data partitions; each computer can have several data partitions, and one thread can process each partition on its own.
To implement fault tolerance, the RDD partitions record **lineage**, a recipe to recompute the RDD partition based on the parent RDDs and the operation used to generate the partition
RDDs are immutable: once created the RDD data contents don't change.

#### Creating RDDs
- Local data structures or local files
- From other RDDs using RDD transformations
- Usually with Big Data, files are stored in distributed storage systems like HDFS (Hadoop Distributed FileSystem), Amazon S3, HBase, etc...

#### RDD partitions: performance tuning
If you need to start to performance tune applications:
- When an RDD is created, it is initially split to a number of partitions, which should be large enough (e.g. at least 2-10 times the number of total number of cores in the used computers) to allow for efficient load balancing between cores
- Each partition should still be large enough to take more than 100ms to process, so not to waste too much time in starting and finishing the task processing a partition

### PySpark Wordcount example
PySpark is the Python interface to the Apache Spark framework. Let's create a wordcount application with PySpark and RDDs.
1. **Reading input**
  First of all we create an RDD form local filesystem using `textFile` function, which returns an RDD where each line is its own data item. Then `take(10)` function returns an array that contains the 10 first data items in the RDD (that's only for debugging purposes).
  ![](@attachment/Clipboard_2021-09-10-17-55-54.png)

2. **Splitting lines**
  Then each line is split into words by running the `flatMap` routine, which maps wach element to potentially several output elements. Here the Python `split` function is by `flatMap` for each line, generate as its output the sequence of words inside each line.
  ![](@attachment/Clipboard_2021-09-10-17-57-25.png)

3. **Mapping words**
  We call the `map` function which creates an RDD that has pairs with the word instances as keys and the constant 1 as value.
  N.B. the `map` function, as well as the other RDD transformations (e.g.`flatMap`) can be run in parallel for different partitions of the RDD.
  ![](@attachment/Clipboard_2021-09-10-17-58-52.png)

4. **Calculating totals**
  Here `reduceByKey` function is used to reduce the list of values associated with each key to a single value using the function given as parameter. The reduce function takes as input two values and combines them into one value.
  **!!!** The function must be assotiative and commutative!
  ![](@attachment/Clipboard_2021-09-10-18-00-52.png)

#### Important: on associativity and commutativity
Associativity and commutativity (e.g. the + operator in the previous image) allows the `reduceByKey` to freely schedule the reductions, and to even do parts of the reductions in parallel on different machines. This is actually happening inside Spark: each worker first computes local words counts inside each partition, and then sends only the sum of the partiion word counts to the final reduce routine computed on potentially a different machine, saving a lof of network bandwith; then the final reduce routine sums up the local word counts from each partition.
This ONLY works because of associativity and commutativity!

### RDD Transformations and Actions
RDD transformations build a DAG (directed acyclic graph I guess) of dependencies between RDD partitions but do not yet start processing. The actual data processing is done **lazily** -> the RDD Actions are needed to start the computation.

#### Transformations
Many different types of transformations, just look at the Apache Spark [documentation](https://spark.apache.org/docs/latest/rdd-programming-guide.html). These are the most famous ones:

| Transformation | Meaning |
|---|---|
|  map(func)  |  Return a new distributed dataset formed by passing each element of the source through a function func.  |
|  filter(func)  |  Return a new dataset formed by selecting those elements of the source on which func returns true.  |
|  flatMap(func)  |  Similar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item).  |
|  mapPartitions(func)   |  Similar to map, but runs separately on each partition (block) of the RDD, so func must be of type     Iterator<T> => Iterator<U> when running on an RDD of type T.  |
|  mapPartitionsWithIndex(func)  |  Similar to mapPartitions, but also provides func with an integer value representing the index of   the partition, so func must be of type (Int, Iterator<T>) => Iterator<U> when running on an RDD of type T.    |
|  sample(withReplacement, fraction, seed)  |  Sample a fraction fraction of the data, with or without replacement, using a given random number generator seed.  |
|  union(otherDataset)  |  Return a new dataset that contains the union of the elements in the source dataset and the argument.  |
|  intersection(otherDataset)  |  Return a new RDD that contains the intersection of elements in the source dataset and the argument.  |
|  distinct([numPartitions]))  |  Return a new dataset that contains the distinct elements of the source dataset. |
|  groupByKey([numPartitions])   |  When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable<V>) pairs.       Note: If you are grouping in order to perform an aggregation (such as a sum or       average) over each key, using reduceByKey or aggregateByKey will yield much better       performance.           Note: By default, the level of parallelism in the output depends on the number of partitions of the parent RDD.       You can pass an optional numPartitions argument to set a different number of tasks.    |
|  reduceByKey(func, [numPartitions])   |  When called on a dataset of (K, V) pairs, returns a dataset of  (K, V) pairs where the values for each key are aggregated using the  given reduce function func, which must be of type (V,V) => V. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument.  |
|  aggregateByKey(zeroValue)(seqOp, combOp, [numPartitions])   |  When called on a dataset of (K, V) pairs, returns a dataset of  (K, U) pairs where the values for each key are aggregated using the  given combine functions and a neutral "zero" value. Allows an aggregated  value type that is different than the input value type, while avoiding  unnecessary allocations. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument.  |
|  sortByKey([ascending], [numPartitions])   |  When called on a dataset of (K, V) pairs where K implements  Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending  or descending order, as specified in the boolean ascending argument. |
|  join(otherDataset, [numPartitions])   |  When called on datasets of type (K, V) and (K, W), returns a  dataset of (K, (V, W)) pairs with all pairs of elements for each key.     Outer joins are supported through leftOuterJoin, rightOuterJoin, and fullOuterJoin.    |
|  cogroup(otherDataset, [numPartitions])   |  When called on datasets of type (K, V) and (K, W), returns a  dataset of (K, (Iterable<V>, Iterable<W>)) tuples. This  operation is also called groupWith.  |
|  cartesian(otherDataset)  |  When called on datasets of types T and U, returns a dataset of (T, U) pairs (all pairs of elements).  |
|  pipe(command, [envVars])  |  Pipe each partition of the RDD through a shell command, e.g. a Perl or bash script. RDD elements are written to the     process's stdin and lines output to its stdout are returned as an RDD of strings.  |
|  coalesce(numPartitions)   |  Decrease the number of partitions in the RDD to numPartitions. Useful for running operations more efficiently     after filtering down a large dataset.  |
|  repartition(numPartitions)  |  Reshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them.     This always shuffles all data over the network.  |
|  repartitionAndSortWithinPartitions(partitioner)  |  Repartition the RDD according to the given partitioner and, within each resulting partition,   sort records by their keys. This is more efficient than calling repartition and then sorting within   each partition because it can push the sorting down into the shuffle machinery.  |

#### Actions
And also many different RDD Actions, still watch at the documentation, these are the most common ones:
| Action | Meaning |
|---|---|
|  reduce(func)  |  Aggregate the elements of the dataset using a function func  (which takes two arguments and returns one). The function should be  commutative and associative so that it can be computed correctly in  parallel.  |
|  collect()  |  Return all the elements of the dataset as an array at the driver  program. This is usually useful after a filter or other operation that  returns a sufficiently small subset of the data.  |
|  count()  |  Return the number of elements in the dataset.  |
|  first()  |  Return the first element of the dataset (similar to take(1)).  |
|  take(n)  |  Return an array with the first n elements of the dataset.  |
|  takeSample(withReplacement, num, [seed])  |  Return an array with a random sample of num elements of the dataset, with or without replacement, optionally pre-specifying a random number generator seed. |
|  takeOrdered(n, [ordering])  |  Return the first n elements of the RDD using either their natural order or a custom comparator.  |
|  saveAsTextFile(path)  |  Write the elements of the dataset as a text file (or set of text  files) in a given directory in the local filesystem, HDFS or any other  Hadoop-supported file system. Spark will call toString on each element  to convert it to a line of text in the file.  |
|  saveAsSequenceFile(path)   (Java and Scala)  |  Write the elements of the dataset as a Hadoop SequenceFile in a  given path in the local filesystem, HDFS or any other Hadoop-supported  file system. This is available on RDDs of key-value pairs that implement  Hadoop's Writable interface. In Scala, it is also    available on types that are implicitly convertible to Writable (Spark  includes conversions for basic types like Int, Double, String, etc).  |
|  saveAsObjectFile(path)   (Java and Scala)  |  Write the elements of the dataset in a simple format using Java serialization, which can then be loaded using     SparkContext.objectFile().  |
|  countByKey()   |  Only available on RDDs of type (K, V). Returns a hashmap of (K, Int) pairs with the count of each key.  |
|  foreach(func)  |  Run a function func on each element of the dataset. This is usually done for side effects such as updating an Accumulator or interacting with external storage systems.    Note: modifying variables other than Accumulators outside of the foreach() may result in undefined behavior. See Understanding closures  for more details. |

### RDD Operations and Actions
Many well known functional programming constructs such as `map` or `flatMap` and `reduce` are available as operations. One can implement relational database-like functionality easy on top of RDD operations, id needed (this is how Spark SQL, a Big Data analytics framework, was originally implemented).
Many operations take a user function to be called back as argument.
Warning: freely mixing user code with RDD operations limits the optimization capabilities of Spark RDDs.

#### Implementing RDD Operations and Actions
As the RDD operations are run in several computers, there is no shared memory to use to share state between operations. The functions run in all of the distributed computers need to copy all of the data they need to all of the Spark workers using so called **closures**.
To minimize the amount of data that needs to be copied to all computers, the functions should refer to as little data as possible to keep the closures small.

### Lineage
Lineage can contain both narrow and wide dependencies.
![](@attachment/Clipboard_2021-09-10-21-01-32.png)

#### Narrow and wide dependencies
- In narrow dependencies, the data partition from an input RDD can be scheduled on the same physical machines as the out RDD. This is called **pipelining**; thus RDD operations with only narrow dependencies can scheduled to be done without any network traffic
- Wide dependencies require all RDD partitions at the previous level of the lineage to be inputs to computing an RDD partition; this requires sending the RDD data over the network. This operations is called **shuffle**; they are unavoidable for applications needing (e.g. global sorting of the output data)

#### DAG scheduling
DAG scheduler is used to schedule RDD operations.
![](@attachment/Clipboard_2021-09-10-21-04-51.png)

Scheduler pipelines work into stages separated by wide dependencies.

![](@attachment/Clipboard_2021-09-10-21-05-27.png)

#### Spark extensions
There are many Spark extensions, e.g.:
- MLlib: distributed ML library
- Spark SQL: distributed analytics SQL database; can be tightly integrated with Apache Hive Data Warehouse, uses HQL (Hive SQL variant)
- Spark Streaming and structured streaming: streaming data processing frameworks
- GraphX: a graph processing system
- etc...







