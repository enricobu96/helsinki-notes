---
attachments: [Clipboard_2021-09-28-11-00-14.png, Clipboard_2021-09-28-11-03-22.png, Clipboard_2021-09-28-11-04-58.png, Clipboard_2021-09-28-11-23-47.png, Clipboard_2021-09-28-11-24-02.png, Clipboard_2021-09-28-11-33-04.png, Clipboard_2021-09-28-11-33-24.png, Clipboard_2021-09-28-11-33-32.png, Clipboard_2021-09-28-11-34-17.png, Clipboard_2021-09-28-11-36-16.png, Clipboard_2021-09-28-11-39-25.png, Clipboard_2021-09-28-11-39-42.png, Clipboard_2021-09-28-11-41-44.png]
tags: [Big Data Platforms]
title: Lesson 6 - 28/09
created: '2021-09-28T07:13:19.235Z'
modified: '2021-09-28T08:42:20.404Z'
---

# Lesson 6 - 28/09

## Database ACID guarantees

A traditional (centralized) database can guarantee its client ACID properties:
- Atomicity: database modifications must follow an "all or nothing" rule. Each transaction is said to be atomic; if one part of the transaction fails, the entire transaction fails and the database state is left unchanged
- Consistency: any transaction the database performs will take it from one consistent state to another
- Isolation: no transaction should be able to interfere with another transaction at all
- Durability: one a transaction has been commited, it will remain so

## Distributed databases - CAP Theorem

In cloud computing we have to implement distributed databases in order to scale to a large number of users sharing common system state. We define the three general properties of distributed systems (Eric Brewer):
- Consistency: all nodes have a consistent view of the contents of the (distributed) database
- Availability: a guarantee that every database request eventually receives a respone about whether it was successful or whether it failed
- Partition Tolerance: the system continues to operate despite arbitrary message loss

It's impossible to have all of three! You have to choose between two:
- CA: non-distributed (centralized) database system
- CP: distributed database that can not be modified when network splits into partitions
- AP: distributed database that can become inconsistent when network splits into partitions

### Example CA Systems

- Single-site (non-distributed) databases
- LDAP: lightweight directory access protocol (for user authentication)
- NFS: network file system
- Centralized version control - SVN
- HDFS namenode

There systems are often based on two-phase commit algorithms, or cache invalidation algorithms

### Example CP Systems

- Distributed databases, e.g. Google Bigtable, Apache HBase
- Distributed coordination systems, e.g. Google Chubby, Apache Zookeeper

The systems often use a master copy location for data modifications combined with pessimistic locking or majority algorithms such as Paxos by Leslie Lamport

### Example AP Systems

- Filesystems allowing updates while disconnected from the main server such as AFS and Coda
- Web caching
- DNS - Domain Name System
- Distributed VCS like Git
- "Eventually consistent" datastores - Amazon Dynamo, Apache Cassandra

The employed mechanism include cache expiration times and leases. Sometimes intelligent merge mechanisms exists for automatically merging independent updates

### System tradeoffs

#### CA systems
- Limited scalability - use when expected system load is low
- Easiest to implement
- Easiest to program against
- High latency if clients are not geographically close

#### CP systems
- Good scalability
- Relatively easy to program against
- Data consistency achieved in a scalable way
- High latency of updates
- Quorum algorithms can make the system available if a majority of the system components are connected to each other
- Will eventually result in user visible unavailability for updates after network partition

#### AP systems
- Extremely good scalability
- Very difficult to program against: there is no single algorithms to merge inconsistent updates
- Data consistency sactificed - can sometimes be tolerated, e.g. web page caching
- Low latency updates possible, always update the "closest copy" and let the system to eventually propagate changes in the background to other data copies
- Will eventually result in user visible inconsistency of data after network partition

##### Why AP systems?
- Much of the internet infrastructure is AP (web caching, dns...)
- Works best with data that does not change, or change infrequently
- Often much simpler to implement in a scalable fashion as CP systems, e.g. just cache data with a timeout
- Needs application specific code to handle inconistent updates, no generic way to handle this!
- Needed for low latency applications

#### Some examples

![](@attachment/Clipboard_2021-09-28-11-00-14.png)

![](@attachment/Clipboard_2021-09-28-11-03-22.png)

![](@attachment/Clipboard_2021-09-28-11-04-58.png)

#### Combinations of AP and CP systems

- AP system usually used for user interaction due to its better scalability (cheaper to run) and lower latency. Requires application specific code to merge inconsistent updates
- CP system used to store the "master copy" of data, has large latency that needs to be hidden from the user
- Also CA systems used - often the simplest implementation, needs careful architecture design not to become the bottleneck

### Transactions in Distributed Databases

- The most widely used protocol is the "two phase commit"; in a two phase commit algorithm you have two kinds of server: a transaction "coordinator" and database servers called "cohorts"
- The protocol consists of two main phases:
  1. A voting phase, where the coordinator asks all cohorts to prepare for a transaction
  2. A completion phase, where the coordinator asks all cohort to either commit or rollback the transaction (depending on if all cohorts voted for the transaction to proceed)
- If a cohort crashes during transaction, the system will automatically either rollback or commit the transaction that was in progress for that cohort
- If a coorddinator crashes permanently in the middle of the completion phase, some cohorts might be blockerd forever waiting for either a commit or a rollback to arrive
- The problem can not be easily solved without changing the used protocol; thus two phace commit can block indefinitely: is not fault tolerant!

#### The asynchronous consensus problem

Classic problem in distributed systems, can be used e.g. to decide wheter to commit a transaction or not
- System as `N` processes
- Each process `i` starts with an initial vote `v_i in {0,1}`
- Asynchronous (no upper bound on message delivery) but reliable network (all messages are delivered). N.B. in particular there is no access to a common block
- At most one process fails by halting (becoming permanently unresponsive)
- The protocol should terminate in finite time by deciding 0 or 1, and the decided balue should be an initial vote of one of the processes

##### The FLP theorem

- Impossibility result: there is no algorithm that will solve the asynchronous consensus problem
- Key intuition: in an asynchronous model it is impossible to distinguish between a failed and a slow process
- Thus a consensus protocol will have very long (actually infinite) runs where it is assuming some dead process is just slow, making no progress in achieving consensus
- The problem becomes decidable if messages always have an upper bound transmission delay

###### Proof
- A detailed look at the proof: FLP proves that any sound fault-tolerant protocol for consensus in an asynchronous setting has runs that never terminate in consensus
- However, in practice these runs consist of an infinite sequence of cases where many different process are
repeatedly assumed to have failed because their messages are delayed

###### FLP and practical approaches to Consensus
- In a practical computer networks the probability of infinite runs where the correctly functioning processes repeatedly keep suspecting each other to be faulty due to excessive message delays is diminishingly small
- Thus a practical consensus algorithm can be made that with very high probability quickly solves the asynchronous consensus problem

##### Paxos algorithm

- Designed to be a sound algorithm for the asynchronous consensus problem
- The guarantee is: if the Paxos algorithm terminates, the its output is a correct outcome of a consensus algorithm
- Does not always terminate, FLP theorem still stands! But instead it terminates in practive w.h.p.
- One can use Paxos to implement a highly fault tolerant database
- Uses `2f+1` servers to tolerate `f` concurrent failures, and still make progress
- Key intuition: if modifications are saved on at least `f+1` servers (a majority/quorum), at least one ot them will survive f failures
- For efficiency reasons, the algorithm can use a centralized leader through which all modifications are done
- If the leader is expected to have failed, a new leader can be quickly elected, and modifications sent to it instead

### Distributed coordination systems

- Consensus under failures protocols are extremely subtle, and should not be done in normal applications code
- In cloud based systems usually the protocols needed to deal with consensus are hidden inside a "distributed coordination system"

#### Google Chubby

![](@attachment/Clipboard_2021-09-28-11-33-04.png)

#### Apache Zookeeper

![](@attachment/Clipboard_2021-09-28-11-33-24.png)
![](@attachment/Clipboard_2021-09-28-11-33-32.png)

##### Zookeeper namespace

![](@attachment/Clipboard_2021-09-28-11-34-17.png)

##### Zookeeper Service

![](@attachment/Clipboard_2021-09-28-11-36-16.png)

##### Zookeeper characteristics

- Reads are served from the cache of the Server the client is connected to. The reads can return slightly out-of-date (max tens of seconds) data.
- There is a sync() call that forces a Server to catch up with the Master before returning data to the client
- The Leader will acknowledge a write if a majority of the servers are able to persist it in their logs
- Thus is a minority of Servers fail, there is at least one server with the most uptodate commits
- If the Leader fails, Zookeeper selects a new Leader

##### Zookepeer components

![](@attachment/Clipboard_2021-09-28-11-39-25.png)

##### Zookeper performance

![](@attachment/Clipboard_2021-09-28-11-39-42.png)

- Note that for workloads that are mostly reads, adding more servers will improve the Zookeper performance
- However, if more than ~30% of the operations are writes, the minimum three server configuration is the best performing configuration
- Thus Zookeeper's Zab and other Paxos-like algorithms have bad performance for write heavy workloads

#### Raft

![](@attachment/Clipboard_2021-09-28-11-41-44.png)
