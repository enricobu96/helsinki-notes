---
attachments: [Clipboard_2022-01-20-19-05-34.png, Clipboard_2022-01-20-19-06-37.png, Clipboard_2022-01-20-19-06-48.png, Clipboard_2022-01-20-19-07-54.png, Clipboard_2022-01-20-19-08-38.png, Clipboard_2022-01-20-19-08-59.png, Clipboard_2022-01-20-19-10-50.png, Clipboard_2022-01-20-19-11-15.png, Clipboard_2022-01-20-19-11-25.png, Clipboard_2022-01-20-19-11-37.png]
tags: [Introduction to Deep Learning]
title: Lesson 1 - 19/01
created: '2022-01-20T17:01:36.730Z'
modified: '2022-01-21T10:19:18.554Z'
---

# Lesson 1 - 19/01

## Introduction

**Deep learning**: artificial neural networks; these basically map vectors into other vectors. As long as you can represent your input and output with fixed width vectors, you can try a deep learning system.

![](@attachment/Clipboard_2022-01-20-19-05-34.png)

Three real numbers **or** a new vector, of course!

### How to find/get/represent the input vectors?

It depends...

#### Images

![](@attachment/Clipboard_2022-01-20-19-06-37.png)

#### Text data

![](@attachment/Clipboard_2022-01-20-19-06-48.png)

But...that's not such a great idea, it's really difficult to define relations => words as points in geometric space:

![](@attachment/Clipboard_2022-01-20-19-07-54.png)

#### Sentences

Different methods

**BOW**: bag of words ![](@attachment/Clipboard_2022-01-20-19-08-38.png)

**CBOW**: continuous BOW ![](@attachment/Clipboard_2022-01-20-19-08-59.png)

## Recap on linear models

Linear regression: always the same. Concepts to remind:
- Measuring the Fit
- Loss function
- We can consider LR as a single layer neural network

Logistic regression: always the same.

### Cross-entropy

Important: How can we compare two probability distributions? -> Cross-entropy ![](@attachment/Clipboard_2022-01-20-19-10-50.png)

![](@attachment/Clipboard_2022-01-20-19-11-15.png)

![](@attachment/Clipboard_2022-01-20-19-11-25.png)

![](@attachment/Clipboard_2022-01-20-19-11-37.png)
