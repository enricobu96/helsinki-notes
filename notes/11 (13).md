---
tags: [Introduction to Machine Learning]
title: Lesson 4 - 12/11
created: '2021-11-13T13:32:21.727Z'
modified: '2021-11-13T13:35:16.723Z'
---

# Lesson 4 - 12/11

## How good is my model? Which model to choose?

Apply the supervised learning model to the training data:
- Simpler model may not fit data perfectly
- More flexible model typically fits the data better. In particular, _nested_ model such as polynomial of increasing order

And now...how can I estimate the loss on new data/ What is the correct model flexibility? Is there a way to regulate the flexibility of the model and/or insert prior knowledge (regularization)?

## Statistical learning model

![](@attachment/Clipboard_2021-11-13-12-55-26.png)

The problem could be very hard to solve, but it would not be a statistical problem; since F is unknown, learning comes into picture

### Empirical loss

If we have training data frawn from F we can infer something of the properties of F. In particular, based on the law of large numbers, the average loss is with high probability close to the expected loss: 
![](@attachment/Clipboard_2021-11-13-12-56-55.png)
But...what if there are many possible models?

Well, the bigger set of models to choose from, the worse it gets

### Overfitting

Aka creating models that follow too closely the specifics of training data, resulting in poor performance on unseen data; often results from using too flexible models with too little data -> choosing the right model flexibility is a difficult problem for which there are many methods (e.g. cross validation)

### Model flexibility

- For parametric models the number of parameters can be used to obtain a measure of complexity
- Some non-parametric models also have intuitive complexity measures
- There are also less obvious parameters that can be used to control overfitting

Loss vs flexibility:

![](@attachment/Clipboard_2021-11-13-13-02-24.png)

### Bias-variance tradeoff

Bias: how far the average model is from the real optimal classifier.
Variance: how far a model tends to be from the average model.
The goal is to have both low, but:
- High model complexity: low bias and high variance
- Low model complixity: high bias and low variance

#### For regression

![](@attachment/Clipboard_2021-11-13-13-04-10.png)

#### How to deal with this in practice

- Split the data in random, e.g. training set 50%, validation set 25%, test set 25%
- Train the model on different complexities on training set
- Pick a model complexity that gives smallest validation set loss
- Train the model on combined training and validation set
- Report test set loss

### Validation

- What is the correct model complexity? -> divide the data into training and validation sets and do as above
- What is the generalization error? -> divide data into training and test sets, the generalization error is approximately the error on the test set

There are more efficient methods tho, such as cross-validation

#### Cross-validation

![](@attachment/Clipboard_2021-11-13-13-07-29.png)

![](@attachment/Clipboard_2021-11-13-13-07-38.png)

## Estimating model parameters

![](@attachment/Clipboard_2021-11-13-13-08-08.png)

### Consistent estimators

Estimator is consistent if it produces a true value at the limit of infinite data. Many of the estimates we would like to use in machine learning are consistent; this means that if we have very large data then the "statistical learning" will just be "learning"

### Estimating mean and empirical loss of the mean estimate

![](@attachment/Clipboard_2021-11-13-13-09-45.png)

### Parametric estimation of confidence interval

The most important special case is given by t statistics; the t-statistics is given by:

![](@attachment/Clipboard_2021-11-13-15-21-17.png)

![](@attachment/Clipboard_2021-11-13-15-21-25.png)

!!! Be careful in interpreting parameter estimates if the underlying assumptions are not obeyed

### [Extra] Non-parametric bootstrap estimation

Often we do not want or cannot estimate confidence interval parametrically, then we use the boostrap.

![](@attachment/Clipboard_2021-11-13-15-24-12.png)

#### Bootstrap of standard deviation of estimate of mean

![](@attachment/Clipboard_2021-11-13-15-24-19.png)

#### Bootstrap confidence intervals

![](@attachment/Clipboard_2021-11-13-15-25-18.png)

## How to adjust model complexity in (linear) regression

- Feature selection
- Regularization (shrinkage):
  - Ridge
  - Lasso

### Feature selection

- Less features = less flexible model
- Naive algorithm is: given number of features k, find a subset of features of size k, train a regressor for those features, compute cross-validation losso, choose the smallest one
- Problem: what when there are too many subsets? -> running time $O(p^k)$, not good, it's NP-hard:
  - Forward selection: greedy heuristics, running time $O(pk)$ -> choose feature that gives the smallest cross-validation loss, add a feature which gives smallest cross-validation loss, iterate untile you have k features
  - Backward selection: start from all feature, drop them one by one

![](@attachment/Clipboard_2021-11-13-15-29-08.png)

![](@attachment/Clipboard_2021-11-13-15-29-15.png)

#### Regularization: Ridge and Lasso

Feature subset selection is one way to control the complexity of the model, but we can also softly punish more complex models.
Observation: badly overfitting linear models often have large regression coefficients -> Idea: we can contrain allowed models "softly" by punishing large regression coefficients. This increases bias and decreases variance.

![](@attachment/Clipboard_2021-11-13-15-32-02.png)

#### Regularization: Bayesian regularization

![](@attachment/Clipboard_2021-11-13-15-31-29.png)

![](@attachment/Clipboard_2021-11-13-15-31-40.png)

