---
attachments: [Clipboard_2022-01-21-12-26-46.png, Clipboard_2022-01-21-12-27-41.png, Clipboard_2022-01-21-12-48-50.png, Clipboard_2022-01-21-12-49-20.png, Clipboard_2022-01-21-12-49-42.png, Clipboard_2022-01-21-12-49-54.png, Clipboard_2022-01-21-12-52-15.png, Clipboard_2022-01-21-12-54-29.png, Clipboard_2022-01-21-12-54-40.png, Clipboard_2022-01-21-13-03-18.png, Clipboard_2022-01-21-13-04-27.png, Clipboard_2022-01-21-13-15-13.png, Clipboard_2022-01-21-13-20-53.png, Clipboard_2022-01-21-13-40-17.png, Clipboard_2022-01-21-13-40-31.png, Clipboard_2022-01-21-13-41-00.png, Clipboard_2022-01-21-13-41-54.png, Clipboard_2022-01-21-13-42-42.png]
tags: [Introduction to Deep Learning]
title: Lesson 2 - 21/01
created: '2022-01-21T10:14:28.935Z'
modified: '2022-01-21T11:46:59.446Z'
---

# Lesson 2 - 21/01

## Training neural networks

![](@attachment/Clipboard_2022-01-21-12-26-46.png)

We can start randomly and then adjust the loss...gradient descent!

## Gradient Descent

![](@attachment/Clipboard_2022-01-21-12-27-41.png)

### Computing the gradient

![](@attachment/Clipboard_2022-01-21-12-48-50.png)

**Partial derivative cheat sheet**
![](@attachment/Clipboard_2022-01-21-12-49-20.png)
![](@attachment/Clipboard_2022-01-21-12-49-42.png)
![](@attachment/Clipboard_2022-01-21-12-49-54.png)
Chain rule: ![](@attachment/Clipboard_2022-01-21-12-52-15.png)

**Back on computing the gradient**

![](@attachment/Clipboard_2022-01-21-12-54-29.png)

Optimize them both = derivate both _m_ and _c_

![](@attachment/Clipboard_2022-01-21-12-54-40.png)

### Stochastic Gradient Descent

![](@attachment/Clipboard_2022-01-21-13-03-18.png)

Q: See each training example only once? Does this guarantee we can find the correct solution? -> Yes, only is we had infinite samples. In practive: we make sure multipla passes over same training points (**epochs**)

![](@attachment/Clipboard_2022-01-21-13-04-27.png)

## Non-linear models

Linear vs non-linear...
Linear: ![](@attachment/Clipboard_2022-01-21-13-15-13.png)

If I want to use a linear model I have to decide (/find out) if the data is linearly separable -> can you draw a hyperplane which has all the positive examples in one side and all the negative samples in the other side? BUT that'rather rare and too good to be true. Two cases:
- Noisy data: there is no linear decision boundary but we can still say that shit happens and use a linear model
- There is simply no way: the data is not linearly separable ![](@attachment/Clipboard_2022-01-21-13-20-53.png)

Sometimes we can make the data linearly separable: 

![](@attachment/Clipboard_2022-01-21-13-40-17.png) -> ![](@attachment/Clipboard_2022-01-21-13-40-31.png)

### XOR

![](@attachment/Clipboard_2022-01-21-13-41-00.png)

Not linearly separable BUT if I add another dimension...

![](@attachment/Clipboard_2022-01-21-13-41-54.png)

These are called **kernel methods**: can define non-linear decision boundaries in the original space

![](@attachment/Clipboard_2022-01-21-13-42-42.png)
