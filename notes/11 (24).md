---
attachments: [Clipboard_2021-11-26-10-21-39.png, Clipboard_2021-11-26-10-29-25.png, Clipboard_2021-11-26-10-39-56.png, Clipboard_2021-11-26-10-40-16.png, Clipboard_2021-11-26-10-40-38.png, Clipboard_2021-11-26-10-41-19.png, Clipboard_2021-11-26-10-41-31.png, Clipboard_2021-11-26-10-41-47.png, Clipboard_2021-11-26-10-42-01.png, Clipboard_2021-11-26-11-05-14.png, Clipboard_2021-11-26-11-05-26.png, Clipboard_2021-11-26-11-05-37.png, Clipboard_2021-11-26-11-24-19.png, Clipboard_2021-11-26-11-24-32.png, Clipboard_2021-11-26-11-24-50.png, Clipboard_2021-11-26-11-25-22.png, Clipboard_2021-11-26-11-34-47.png, Clipboard_2021-11-26-11-34-55.png, Clipboard_2021-11-26-11-35-07.png, Clipboard_2021-11-26-11-35-15.png, Clipboard_2021-11-26-11-35-28.png, Clipboard_2021-11-26-11-35-39.png, Clipboard_2021-11-26-11-53-45.png]
tags: [Introduction to Machine Learning]
title: Lesson 8 - 26/11
created: '2021-11-26T08:17:07.545Z'
modified: '2021-11-26T09:53:49.815Z'
---

# Lesson 8 - 26/11

## Properties of decision trees

Nonparametric approach:
- If the tree size is unlimited, can in approximate any decision boundary to arbitrary precision (k-NN)
- With infinite data could in principle always learn the optimal classifier
- But with finite training data, need to avoid overfitting (bias-variance tradeoff)

- Local, greedy learning to find a reasonale solution in reasonable time
- Relatively easy to interpret
- Classification generally really fast
- Usually comptetitive only when combined with ensemble methods: bagging, random forests, boosting

## Ensemble methods

![](@attachment/Clipboard_2021-11-26-10-21-39.png)

### Bagging

![](@attachment/Clipboard_2021-11-26-10-29-25.png)

### Random forest

Bagging tends to improve somewhat, but the trees are highly dependent in cases where the splits that maximize tha gain are clearly better than the 2nd best splits.
- The trees can be forced to use different features by only allowing splits based on a random sample of the features
- Trees constructed in bagging or random forests are usually not pruned since averaging a large number of trees reduces over-fitting
- Random forests are one of the most powerful family of classifiers available

## SVM

### Some refreshers

#### Linear models

![](@attachment/Clipboard_2021-11-26-10-39-56.png)

#### Multivariate linear regression

![](@attachment/Clipboard_2021-11-26-10-40-16.png)

#### Feature transformations

![](@attachment/Clipboard_2021-11-26-10-40-38.png)

#### Dummy variables

![](@attachment/Clipboard_2021-11-26-10-41-19.png)

![](@attachment/Clipboard_2021-11-26-10-41-31.png)

#### Linear classification via regression

![](@attachment/Clipboard_2021-11-26-10-41-47.png)

![](@attachment/Clipboard_2021-11-26-10-42-01.png)

### Perceptron algorithm

![](@attachment/Clipboard_2021-11-26-11-05-14.png)

![](@attachment/Clipboard_2021-11-26-11-05-26.png)

![](@attachment/Clipboard_2021-11-26-11-05-37.png)

![](@attachment/Clipboard_2021-11-26-11-24-19.png)

### Margin

![](@attachment/Clipboard_2021-11-26-11-24-32.png)

![](@attachment/Clipboard_2021-11-26-11-24-50.png)

### Margin & SVM: terminology

![](@attachment/Clipboard_2021-11-26-11-25-22.png)

### Overvations on max margin classifiers

![](@attachment/Clipboard_2021-11-26-11-34-47.png)

![](@attachment/Clipboard_2021-11-26-11-34-55.png)

![](@attachment/Clipboard_2021-11-26-11-35-07.png)

![](@attachment/Clipboard_2021-11-26-11-35-15.png)

### Kernel trick and kernels

![](@attachment/Clipboard_2021-11-26-11-35-28.png)

![](@attachment/Clipboard_2021-11-26-11-35-39.png)

### SVM: properties

![](@attachment/Clipboard_2021-11-26-11-53-45.png)
