---
attachments: [Clipboard_2022-01-25-16-22-21.png, Clipboard_2022-01-25-16-28-01.png, Clipboard_2022-01-25-16-28-40.png, Clipboard_2022-01-25-16-28-54.png, Clipboard_2022-01-25-16-29-01.png, Clipboard_2022-01-25-16-34-46.png, Clipboard_2022-01-25-16-35-01.png, Clipboard_2022-01-25-16-36-21.png, Clipboard_2022-01-25-16-46-26.png, Clipboard_2022-01-25-16-48-40.png, Clipboard_2022-01-25-16-51-10.png, Clipboard_2022-01-25-16-54-21.png, Clipboard_2022-01-25-17-04-17.png, Clipboard_2022-01-25-17-06-34.png, Clipboard_2022-01-25-17-06-55.png, Clipboard_2022-01-25-17-08-09.png, Clipboard_2022-01-25-17-08-20.png, Clipboard_2022-01-25-17-12-36.png, Clipboard_2022-01-25-17-17-18.png, Clipboard_2022-01-25-17-25-07.png, Clipboard_2022-01-25-17-25-20.png, Clipboard_2022-01-25-17-26-17.png, Clipboard_2022-01-25-17-26-40.png]
tags: [Probabilistic Graphical Models]
title: Lesson 3 - 25/01
created: '2022-01-25T14:17:12.854Z'
modified: '2022-01-25T15:26:40.231Z'
---

# Lesson 3 - 25/01

## Bayesian inference

Three steps of BDA:
1. Set up a full probability model - a joint probability distribution of all observable and unobservable variables that are relevant to the problem
2. Conditioning on observed data: compute the **posterior distribution** of the unobserved variables of ultimate interest, given the observed data
3. Evaluate the fit of the model and the implications of the resulting prosterior distribution

In more detail: ![](@attachment/Clipboard_2022-01-25-16-22-21.png)

In less detail: ![](@attachment/Clipboard_2022-01-25-16-28-01.png)

## Simple bayesian analysis

Step 1: specify models
![](@attachment/Clipboard_2022-01-25-16-28-40.png)

Step 2: assign priors
![](@attachment/Clipboard_2022-01-25-16-29-01.png)

Step 3: collect data

Step 4: calculate likelihoods
![](@attachment/Clipboard_2022-01-25-16-34-46.png)

Step 5: the posterior
![](@attachment/Clipboard_2022-01-25-16-35-01.png)

Step 6: draw inferences
![](@attachment/Clipboard_2022-01-25-16-36-21.png)

In some cases we could use MAP (maximum a posteriori model) for prediction

## More advanced bayesian analysis

- A richer set of models allows more precise proportion estimates, but comes with a cost: the amount of calulations necessary increase proportionally
![](@attachment/Clipboard_2022-01-25-16-46-26.png)

### What about maximum likelihood?

![](@attachment/Clipboard_2022-01-25-16-48-40.png)

ML-parameters for the Bernoulli model

![](@attachment/Clipboard_2022-01-25-16-51-10.png)

What are the problems? ![](@attachment/Clipboard_2022-01-25-16-54-21.png) -> we need **regularization**
Be bayesian:
- Parameters are exactly the things you do not know -> they should be considered as random variables with (prior and posterior) distributions
- Remember that posterior distribution is the goal of Bayesian analysis

### Predicting with posterior distribution

![](@attachment/Clipboard_2022-01-25-17-06-34.png)

#### Prior for Bernoulli parameter
![](@attachment/Clipboard_2022-01-25-17-08-09.png)

#### Beta distribution
![](@attachment/Clipboard_2022-01-25-17-08-20.png)

#### Posterior for the Bernoulli model
![](@attachment/Clipboard_2022-01-25-17-12-36.png)

#### Prediction
![](@attachment/Clipboard_2022-01-25-17-17-18.png)

#### Laplace smoothing
![](@attachment/Clipboard_2022-01-25-17-25-07.png)

#### Point estimates
![](@attachment/Clipboard_2022-01-25-17-25-20.png)

#### Sequential bayesian updating
![](@attachment/Clipboard_2022-01-25-17-26-17.png)

#### Bayesian model selection
![](@attachment/Clipboard_2022-01-25-17-26-40.png)























