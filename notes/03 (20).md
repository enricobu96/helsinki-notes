---
attachments: [Clipboard_2022-03-31-12-21-46.png, Clipboard_2022-03-31-12-23-40.png, Clipboard_2022-03-31-12-26-13.png, Clipboard_2022-03-31-12-29-31.png, Clipboard_2022-03-31-12-29-41.png, Clipboard_2022-03-31-12-32-57.png, Clipboard_2022-03-31-12-34-42.png, Clipboard_2022-03-31-12-35-41.png, Clipboard_2022-03-31-12-40-35.png, Clipboard_2022-03-31-12-46-31.png, Clipboard_2022-03-31-12-48-37.png, Clipboard_2022-03-31-12-52-46.png, Clipboard_2022-03-31-13-12-17.png, Clipboard_2022-03-31-13-12-31.png, Clipboard_2022-03-31-13-17-10.png, Clipboard_2022-03-31-13-29-50.png, Clipboard_2022-03-31-13-46-28.png, Clipboard_2022-03-31-13-46-38.png, Clipboard_2022-03-31-13-52-02.png, Clipboard_2022-03-31-13-52-21.png]
tags: [Advanced Machine Learning]
title: Lesson 6 - 31/03
created: '2022-03-31T09:17:50.669Z'
modified: '2022-03-31T11:01:42.640Z'
---

# Lesson 6 - 31/03

## Nonlinear supervised learning

![](@attachment/Clipboard_2022-03-31-12-21-46.png)

### Non-linear classifiers

![](@attachment/Clipboard_2022-03-31-12-23-40.png)

**By preprocessing the inputs**
![](@attachment/Clipboard_2022-03-31-12-26-13.png)

#### Non-linear SVM

**Linear SVM**
![](@attachment/Clipboard_2022-03-31-12-29-31.png)
![](@attachment/Clipboard_2022-03-31-12-29-41.png)

-> Non linear
![](@attachment/Clipboard_2022-03-31-12-32-57.png)

**Non-linear SVM and kernel trick**
![](@attachment/Clipboard_2022-03-31-12-34-42.png)
You don't even need the phi, you just need the kernel function

#### Kernel methods

![](@attachment/Clipboard_2022-03-31-12-35-41.png)

Other, advanced kernels
![](@attachment/Clipboard_2022-03-31-12-40-35.png)

**Representer theorem**
![](@attachment/Clipboard_2022-03-31-12-46-31.png)

Example: Kernelized ridge regression
![](@attachment/Clipboard_2022-03-31-12-48-37.png)
Why would we do that? -> exactly, because we can replace XXT with a kernel matrix

**Computing with kernels**
![](@attachment/Clipboard_2022-03-31-12-52-46.png)

## Ensemble methods

![](@attachment/Clipboard_2022-03-31-13-12-17.png)

### Bagging

![](@attachment/Clipboard_2022-03-31-13-12-31.png)

### Random forests

![](@attachment/Clipboard_2022-03-31-13-17-10.png)

### Boosting

AdaBoost
![](@attachment/Clipboard_2022-03-31-13-29-50.png)
![](@attachment/Clipboard_2022-03-31-13-46-28.png)
![](@attachment/Clipboard_2022-03-31-13-46-38.png)
Solving for f_m(x)
![](@attachment/Clipboard_2022-03-31-13-52-02.png)
Soving for beta_m
![](@attachment/Clipboard_2022-03-31-13-52-21.png)
