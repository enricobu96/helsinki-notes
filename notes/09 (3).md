---
attachments: [Clipboard_2021-09-07-16-27-41.png, Clipboard_2021-09-07-16-34-23.png, Clipboard_2021-09-07-16-41-48.png, Clipboard_2021-09-07-16-47-48.png, Clipboard_2021-09-07-16-55-37.png, Clipboard_2021-09-07-16-56-53.png, Clipboard_2021-09-07-16-56-59.png, Clipboard_2021-09-07-17-20-58.png, Clipboard_2021-09-07-17-22-17.png, Clipboard_2021-09-07-17-28-15.png, Clipboard_2021-09-07-17-31-39.png, Clipboard_2021-09-07-17-37-42.png, Clipboard_2021-09-07-17-44-23.png, Clipboard_2021-09-07-17-44-51.png, Clipboard_2021-09-07-17-46-42.png, Clipboard_2021-09-07-17-48-44.png, Clipboard_2021-09-07-17-49-46.png, Clipboard_2021-09-08-16-56-32.png, Clipboard_2021-09-08-17-10-44.png, Clipboard_2021-09-08-17-14-44.png, Clipboard_2021-09-08-17-18-06.png, Clipboard_2021-09-08-17-21-08.png, Clipboard_2021-09-08-17-23-18.png, Clipboard_2021-09-08-17-24-09.png, Clipboard_2021-09-08-17-26-23.png]
tags: [Introduction to Data Science]
title: Lesson 2 - 07/09
created: '2021-09-07T13:17:29.468Z'
modified: '2021-09-08T14:26:59.579Z'
---

# Lesson 2 - 07/09

## What is data science?

Many definitions:
- Collecting, manipulating and analysing data in order to extracting value from it
- Extraction of knowledge from data, which is a continuation of the field of data mining and predictive analytics
- The empirical synthesis of actionable knowledge from raw data through the complete data lifecycle process

- A data scientist should be able to:
  - Understand the background domain
  - Design solutions that produce added value
  - Implement the solutions efficiently
  - Communicate the findings clearly

- A data scientist is a pratictioner with sufficient expertise in software engineering, statistics/machine learning AND the application domain.

## Data

### Real vs synthetic data

#### Real data
Data collected/recorded in real world settings. Real data describe real-world attributes or matters.

#### Synthetic data
Artificially manufactured (usually produced algorithmically). Subsitute for real data; useful when real data are hard to acquire/collect (e.g. deep learning)

#### Examples

Sales records - real
Sampling from a normal distribution - synthetic
Open healthcare data - it's open so it's synthetic (anonimization)
Data from a scientific experiment - real

### Data collection

**Think of the added value you want to achieve**: think what data you should gather based on the purpose of the project. It's better to be smart in your data collection process than try to be smart later in your analysis.
And so...
#### Sampling plan
Before data collection it's goot to draft a sampling plan. The essential decisions for the collection process are, like:
- Desiderd sample size
- Variables that should be measured
- Variability of the population
- Timefram of study
- Potential need for handling sensitive data
- Etc...

#### Selection bias
Be careful with the selection bias! It can occur when the collected sample is no representative of the intended population of the study -> it can result in systematic error (e.g. plane with red dots).

#### Data collection methods
Different methods, e.g.:
- Observation
- Experiment
- Secondary data collection
- Survey
- Archival research
- Etc...

![](@attachment/Clipboard_2021-09-08-17-21-08.png)

### Kinds of data
| Structured data | Unstructured data |
|-----------------|-------------------|
|Lists|Text|
|n x m arrays|Images|
|Hierarchies|Video|
|Networks|Sound|

### Structured data formats
- CSV (comma separated values)
```
lenght,width,number
18,12,3
9,3,19
```
- Hierarchies, e.g. Newick tree format ```(A,B,(C,D)E)F```
- Networks, e.g. GraphViz
```
digraph graphname {
  a -> b -> c;
  b -> d;
}
```
- JSON (that's also easy to parse in python with ```csv``` library)

#### Parsing
Given a known grammar, unstructured text data can be parsed to discover its inherent structure.
Similarly, also images can be segmented into parts (image segmentation).

#### Array data
The most important operations on arrays include:
- Construction arrays (reading from input or initializing them)
- Reshaping
- Slicing ( == subsets of rows and/or columns)
- Numerical operations (element-wise or aggregate)

!!! Especially for large arrays, different implementations have huge efficency differences, e.g. Python list vs Numpy array.
RoT: avoid for-loops like the plague!

#### Data frames

Like, you know, pandas dataframe. Use this, it's cool.

#### Transformations

Nothing, you can do transformations between data formats, probably there's a library for everything :)))
Different types of transformations can be the following.

##### Scaling
Transforming features so that they are in a specific range, for instance [0,1].
The most common is the min-max scaler:
![](@attachment/Clipboard_2021-09-08-17-23-18.png)

##### Z-score normalization a.k.a. standardization
Transforming features to a standard normal distribution (mu = 0, sigma = 1).
![](@attachment/Clipboard_2021-09-08-17-24-09.png)

#### Filtering

Many different filtering operations, most important is the subsetting a.k.a. slicing (taking only some rows and/or columns). Many filtering operations can be done easily and efficently using CLI tools suck as grep, cut, awk, sed.

![](@attachment/Clipboard_2021-09-07-17-44-51.png)

#### And when you're missing data? Imputation!

When you're missing data it's kinda a problem: missing values can be a show-stopper for many analysis methods.
One way is to filter out all records with missing value (e.g. in python you can just ```.dropna()```); but this can loase a lot of important data.
What can you do then? -> **Imputation** a.k.a. enter fake data in the place of the missing entries:
- For continuous variables you can use mean or median
- For categorical variables you can use mode (most typical value in column)
- Also possible to use Machine Learning to try to infere the missing entries based on the other features

![](@attachment/Clipboard_2021-09-08-17-10-44.png)

Anyway, be flexible! The usefulness/safety of different ways of dealing with missing data depend on the circumstances. e.g. if the data is "missing completely at random" then it's safe to delete rows with missing elements (although this means a smaller amount of data).

If, however, missingness depends on a variable of interest, then deleting rows may **bias** the conslusions.

#### Data reduction
Different techniques for reducing the dimension of the data. Could include an initial selection of the features available and/or different data aggregation techniques. e.g. leave out of the analysis any of the feature we are not interested in including.

#### Data discretization
Breaking down continuou variables into a catergorical variable -> less granularity, lower resolution of the information.
![](@attachment/Clipboard_2021-09-08-17-26-23.png)

#### Data sampling
It mainly involves sampling subsets from larger datasets. This method can be particularly helpful when working with big data.


