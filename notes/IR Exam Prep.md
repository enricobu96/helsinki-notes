---
attachments: [Clipboard_2022-03-06-10-55-33.png, Clipboard_2022-03-06-10-56-01.png, Clipboard_2022-03-06-10-56-23.png, Clipboard_2022-03-06-10-56-35.png, Clipboard_2022-03-06-10-56-45.png, Clipboard_2022-03-06-10-57-15.png, Clipboard_2022-03-06-10-58-38.png, Clipboard_2022-03-06-11-01-29.png, Clipboard_2022-03-06-11-04-00.png, Clipboard_2022-03-06-11-04-42.png, Clipboard_2022-03-06-11-05-06.png, Clipboard_2022-03-06-11-06-25.png, Clipboard_2022-03-06-11-33-07.png, Clipboard_2022-03-06-11-33-54.png, Clipboard_2022-03-06-11-34-13.png, Clipboard_2022-03-06-11-39-44.png, Clipboard_2022-03-06-11-42-26.png, Clipboard_2022-03-06-11-42-42.png, Clipboard_2022-03-06-11-51-09.png, Clipboard_2022-03-06-11-52-06.png, Clipboard_2022-03-06-11-52-46.png, Clipboard_2022-03-06-11-53-14.png, Clipboard_2022-03-06-11-54-47.png, Clipboard_2022-03-06-11-58-24.png, Clipboard_2022-03-06-11-59-06.png, Clipboard_2022-03-06-11-59-32.png, Clipboard_2022-03-06-12-00-26.png, Clipboard_2022-03-06-12-04-43.png, Clipboard_2022-03-06-12-05-09.png, Clipboard_2022-03-06-12-05-38.png, Clipboard_2022-03-06-12-06-53.png, Clipboard_2022-03-06-12-08-32.png, Clipboard_2022-03-06-12-08-42.png, Clipboard_2022-03-06-12-10-05.png, Clipboard_2022-03-06-12-11-08.png, Clipboard_2022-03-06-12-11-19.png, Clipboard_2022-03-06-12-11-58.png, Clipboard_2022-03-06-12-12-11.png, Clipboard_2022-03-06-12-12-26.png, Clipboard_2022-03-06-12-12-46.png, Clipboard_2022-03-06-12-13-35.png, Clipboard_2022-03-06-12-16-46.png, Clipboard_2022-03-06-12-18-48.png, Clipboard_2022-03-06-12-18-57.png, Clipboard_2022-03-06-12-19-54.png, Clipboard_2022-03-06-13-05-55.png, Clipboard_2022-03-06-13-07-33.png, Clipboard_2022-03-06-13-08-04.png, Clipboard_2022-03-06-13-08-30.png, Clipboard_2022-03-06-13-09-01.png, Clipboard_2022-03-06-13-09-26.png, Clipboard_2022-03-06-13-10-28.png, Clipboard_2022-03-06-13-10-58.png, Clipboard_2022-03-06-13-11-57.png, Clipboard_2022-03-06-13-12-13.png, Clipboard_2022-03-06-13-12-45.png, Clipboard_2022-03-06-13-13-32.png, Clipboard_2022-03-06-13-13-42.png, Clipboard_2022-03-06-13-14-06.png, Clipboard_2022-03-06-13-14-19.png, Clipboard_2022-03-06-13-16-18.png, Clipboard_2022-03-06-15-02-11.png, Clipboard_2022-03-06-15-02-31.png, Clipboard_2022-03-06-15-03-39.png, Clipboard_2022-03-06-15-04-19.png, Clipboard_2022-03-06-15-05-03.png, Clipboard_2022-03-06-15-05-16.png, Clipboard_2022-03-06-15-06-09.png, Clipboard_2022-03-06-15-06-57.png, Clipboard_2022-03-06-15-08-27.png, Clipboard_2022-03-06-15-08-58.png, Clipboard_2022-03-06-15-09-21.png, Clipboard_2022-03-06-15-11-01.png, Clipboard_2022-03-06-15-13-19.png, Clipboard_2022-03-06-15-14-27.png, Clipboard_2022-03-06-15-16-44.png, Clipboard_2022-03-06-15-17-01.png, Clipboard_2022-03-06-15-51-56.png, Clipboard_2022-03-06-15-52-26.png, Clipboard_2022-03-06-15-54-07.png, Clipboard_2022-03-06-15-54-41.png, Clipboard_2022-03-06-16-14-01.png, Clipboard_2022-03-06-16-17-50.png, Clipboard_2022-03-06-16-18-40.png, Clipboard_2022-03-06-16-18-59.png, Clipboard_2022-03-06-16-21-32.png, Clipboard_2022-03-06-16-22-23.png, Clipboard_2022-03-06-16-23-27.png, Clipboard_2022-03-06-16-23-41.png, Clipboard_2022-03-06-16-33-08.png, Clipboard_2022-03-06-16-33-16.png, Clipboard_2022-03-06-16-33-30.png, Clipboard_2022-03-06-16-33-39.png, Clipboard_2022-03-06-16-35-37.png, Clipboard_2022-03-06-16-35-46.png, Clipboard_2022-03-06-16-35-59.png, Clipboard_2022-03-06-16-36-34.png, Clipboard_2022-03-06-16-36-44.png, Clipboard_2022-03-06-16-38-33.png, Clipboard_2022-03-06-16-39-13.png, Clipboard_2022-03-06-16-39-24.png, Clipboard_2022-03-06-16-40-22.png, Clipboard_2022-03-06-16-40-33.png, Clipboard_2022-03-06-16-41-40.png, Clipboard_2022-03-06-16-42-11.png, Clipboard_2022-03-06-16-52-33.png, Clipboard_2022-03-06-16-54-05.png, Clipboard_2022-03-06-16-54-22.png, Clipboard_2022-03-06-16-54-37.png, Clipboard_2022-03-06-16-54-55.png, Clipboard_2022-03-06-16-55-24.png, Clipboard_2022-03-06-16-55-35.png, Clipboard_2022-03-06-16-55-51.png, Clipboard_2022-03-06-16-57-00.png, Clipboard_2022-03-06-16-59-23.png, Clipboard_2022-03-06-16-59-51.png, Clipboard_2022-03-06-17-00-56.png, Clipboard_2022-03-06-17-01-38.png, Clipboard_2022-03-06-18-40-24.png, Clipboard_2022-03-06-18-41-13.png, Clipboard_2022-03-06-18-41-33.png, Clipboard_2022-03-06-18-42-04.png, Clipboard_2022-03-06-18-43-03.png, Clipboard_2022-03-06-18-44-33.png, Clipboard_2022-03-06-18-49-11.png, Clipboard_2022-03-06-18-49-42.png, Clipboard_2022-03-06-18-50-33.png, Clipboard_2022-03-06-18-51-23.png, Clipboard_2022-03-06-18-51-44.png, Clipboard_2022-03-06-18-52-22.png, Clipboard_2022-03-06-18-52-35.png, Clipboard_2022-03-06-18-53-02.png, Clipboard_2022-03-06-18-53-17.png, Clipboard_2022-03-06-18-53-54.png, Clipboard_2022-03-06-18-54-24.png, Clipboard_2022-03-06-18-54-45.png, Clipboard_2022-03-06-18-55-32.png, Clipboard_2022-03-06-18-56-23.png, Clipboard_2022-03-06-18-57-55.png, Clipboard_2022-03-06-18-58-44.png, Clipboard_2022-03-06-18-59-17.png, Clipboard_2022-03-06-18-59-35.png, Clipboard_2022-03-07-16-10-26.png, Clipboard_2022-03-07-16-26-52.png, Clipboard_2022-03-07-16-28-56.png, Clipboard_2022-03-07-16-29-12.png, Clipboard_2022-03-07-16-30-09.png, Clipboard_2022-03-07-16-30-24.png, Clipboard_2022-03-07-16-30-49.png, Clipboard_2022-03-07-16-30-58.png, Clipboard_2022-03-07-16-31-14.png, Clipboard_2022-03-07-16-32-07.png, Clipboard_2022-03-07-16-34-21.png, Clipboard_2022-03-07-16-34-41.png, Clipboard_2022-03-07-18-15-07.png]
tags: [Information Retrieval]
title: IR Exam Prep
created: '2022-03-06T08:50:03.812Z'
modified: '2022-03-07T16:15:20.434Z'
---

# IR Exam Prep

## Inverted index

We could use an incident matrix and then bitwise and to search sth, BUT sparse data -> lot of space -> no -> only record the position of the 1s

![](@attachment/Clipboard_2022-03-06-10-55-33.png)

Inverted index construction: ![](@attachment/Clipboard_2022-03-06-10-56-01.png)
-> ![](@attachment/Clipboard_2022-03-06-10-56-23.png)
-> ![](@attachment/Clipboard_2022-03-06-10-56-35.png)
-> ![](@attachment/Clipboard_2022-03-06-10-56-45.png)
-> ![](@attachment/Clipboard_2022-03-06-10-57-15.png)

### Processing boolean queries

Boolean queries:
- AND, OR, NOT
- Each document is a set of terms
- Precise

**Simple Conjunctive Query with two terms** (Butus AND Calpurnia). O(p+q)
![](@attachment/Clipboard_2022-03-06-10-58-38.png)

**Optimization** (Brutus AND Calpurnia AND Caesar)
- Process in order of increasing frequency: start with the two shortest, and so on
![](@attachment/Clipboard_2022-03-06-11-01-29.png)

### The Lexicon

- Data structure for storing the term vocabulary
- For each term t we need document frequency and pointer to postings list for t

Fast lexicons: hash tables and tries, both allow lookup of a term in O(|t|) and both assumes that the lexicon fits in RAM

#### Hash table

- Array of fixed size
- Hash function: maps a term t to an integer

![](@attachment/Clipboard_2022-03-06-11-04-00.png)

**Collisions?**
- Chaining ![](@attachment/Clipboard_2022-03-06-11-04-42.png)
- Linear probing ![](@attachment/Clipboard_2022-03-06-11-05-06.png)
- Perfect hashing -> perfect hash function that has no collisions. Works only for a specific set of terms, which must be given in advance, and takes some time to construct (and must be reconstructed if the set of terms changes)

#### Tries

![](@attachment/Clipboard_2022-03-06-11-06-25.png)

Nice Tries: tries support prefix matching much more naturally than hash tables do. No need to rehash.

## Data compression

We can compress both components of an inverted index; lists are much bigger (factor 10 at least) than lexicon.
Key idea: observer that elements of L are monotonically increasing -> we can just take de differences. Integers are then smaller and it's easy to "degap".

### Variable Length Encoding

Use few bits for small gpas, many for large gaps (like arachnid and other rare terms -> 20, the and other very frequent terms -> 1)

#### Integers representation

Minimal binary codes alone are not good bc we need to be able to decompress as well -> we need a self-delimining coding scheme

**Unary code**
Integer n as n zeros and a final 1

**Elias gamma codes**
![](@attachment/Clipboard_2022-03-06-11-33-07.png)
Also the most significant bit of the minimal binary code is always 1 -> we don't need to store it
![](@attachment/Clipboard_2022-03-06-11-33-54.png)
And to decode a gamma code?
![](@attachment/Clipboard_2022-03-06-11-34-13.png)
Length of the entire code is 2logn+1 bits, just over twice the size of the minimal binary code

**Vbyte codes**
![](@attachment/Clipboard_2022-03-06-11-39-44.png)
Much faster than Elias gamma codes to decode

**Elias-fano representation**
![](@attachment/Clipboard_2022-03-06-11-42-26.png)
![](@attachment/Clipboard_2022-03-06-11-42-42.png)
Size: overall less than 2n+n*ceil(log(z/n)) bits

## Web crawling

A web crawler (or (ro)bot or spider) is the subsystem of the search engine that downloads pages from the Web for storage in the web repository.
![](@attachment/Clipboard_2022-03-06-11-51-09.png)

![](@attachment/Clipboard_2022-03-06-11-52-06.png)

![](@attachment/Clipboard_2022-03-06-11-52-46.png)

![](@attachment/Clipboard_2022-03-06-11-53-14.png)

![](@attachment/Clipboard_2022-03-06-11-54-47.png)
And also ensures that the robots exclusion protocol is obeyed by every crawler

### Extending the web repository

Crawlers rely on hyperlinks to discover new URLs

Discovery process:
![](@attachment/Clipboard_2022-03-06-11-58-24.png)

![](@attachment/Clipboard_2022-03-06-11-59-06.png)

![](@attachment/Clipboard_2022-03-06-11-59-32.png)

We need efficient data structures!
![](@attachment/Clipboard_2022-03-06-12-00-26.png)

And...order in which pages are downloaded?
- Random order?
- Breadth-first order is common

Also some pages are more important than others -> URL prioritization strategies! And how to measure page quality?
- Exploit the connectivity of pages -> page rank is based on this idea
- Measure potential impact the page would make on the search result quality to the user -> page popularity statistics, advertiser preferences, etc...

### Refreshing the web repository

Not trivial bc only way to determine if page's content has changed is to download it.
Also, in what order should we refresh pages?
![](@attachment/Clipboard_2022-03-06-12-04-43.png)

![](@attachment/Clipboard_2022-03-06-12-05-09.png)

![](@attachment/Clipboard_2022-03-06-12-05-38.png)

### Distributed web crawling

Faster is better (the crawler)

![](@attachment/Clipboard_2022-03-06-12-06-53.png)

But...network bandwidth is the main bottleneck -> large-scale web crawlers use different clusters (few hundred nods) in order to exploit network bandwidth.

But...redundancy! To solve: firewall mode
![](@attachment/Clipboard_2022-03-06-12-08-32.png)
![](@attachment/Clipboard_2022-03-06-12-08-42.png)

But...coordination! Lack of coordination in firewall mode means that links whose source and destination pages are assigned to different nodes cannot be followed by any node -> loss of coverage bc some pages will never be discovered ![](@attachment/Clipboard_2022-03-06-12-10-05.png)
To solve: follow a link (only one hop) even if its destination URL is assigned to another node (cross-over mode)
BUT! AGAIN! REDUNDANCY! -> exchange mode
![](@attachment/Clipboard_2022-03-06-12-11-08.png)
![](@attachment/Clipboard_2022-03-06-12-11-19.png)

### Factors affecting crawling performance

- Delay attack: ![](@attachment/Clipboard_2022-03-06-12-11-58.png)
- Cloaking: ![](@attachment/Clipboard_2022-03-06-12-12-11.png)
- Spider traps: ![](@attachment/Clipboard_2022-03-06-12-12-26.png)
- Link farms: ![](@attachment/Clipboard_2022-03-06-12-12-46.png)

### Again on indexing

![](@attachment/Clipboard_2022-03-06-12-13-35.png)

We want efficient way to construct inverted index bc it's done often
![](@attachment/Clipboard_2022-03-06-12-18-48.png)
![](@attachment/Clipboard_2022-03-06-12-18-57.png)
On a large scale: MapReduce
And to update?
![](@attachment/Clipboard_2022-03-06-12-19-54.png)

#### Terms, parsing, indexing

Some problems:
- One word or two? (Hewlett-Packard)
- Numbers/dates
- Chinese: no whitespace, ambiguous segmentation
- Compounds in Dutch and German (and others)
- Arabic: bidirectionality

So, now:
**Normalization**
Canonizing tokens so that matches occur despice sueprficial differences (U.S.A. vs USA)

- Case folding
- Stop words
- Lemmatization: ![](@attachment/Clipboard_2022-03-06-12-16-46.png)
- Stemming: lemmatization is hard to do well, so we use this heuristic process. Also increases effectiveness for some query and decreases for others
  - Porter algorithm (just search on internet if you want to apply it)

## Rank and other stuff

Boolean retrieval models are good only for some specific application, usually tho feast or famine problem -> ranked retrieval models:
- Only top documents
- Free text queries
- No feast or famine

Basis: scoring. We wish to return in order the documents most likely to be useful. How can we rank-order the documents?

**Jaccard coefficient**
![](@attachment/Clipboard_2022-03-06-13-05-55.png)
But:
- It doesn't consider the term frequency (how many times a term occurs in a document)
- Rare terms in a collection are more informative than frequent terms, and Jaccard doesn't consider this information

On bag of words model:
![](@attachment/Clipboard_2022-03-06-13-07-33.png)

**Term frequency tf**
![](@attachment/Clipboard_2022-03-06-13-08-04.png)
-> Log-frequenct weighting
![](@attachment/Clipboard_2022-03-06-13-08-30.png)

On collection vs document frequency:
![](@attachment/Clipboard_2022-03-06-13-09-01.png)

**idf weigth**
![](@attachment/Clipboard_2022-03-06-13-09-26.png)
- Has no effect on ranking one term queries, affects the ranking of documents for queries with at least two terms

**tf-idf weighting**
![](@attachment/Clipboard_2022-03-06-13-10-28.png)
- Increases with the number of occurrences within a document
- Increases with the rarity of the term in the collection

-> Score of a document given a query
![](@attachment/Clipboard_2022-03-06-13-10-58.png)
Many variants:
- How tf is computed (with/without logs)
- Wether the terms in the query are also weighted
- ...

### Documents as vectors

![](@attachment/Clipboard_2022-03-06-13-11-57.png)
![](@attachment/Clipboard_2022-03-06-13-12-13.png)

How to calculate distance?
Euclidean distance is a bad idea ![](@attachment/Clipboard_2022-03-06-13-12-45.png)
-> Use angle instead of distance: cosine similarity
Length normalization: ![](@attachment/Clipboard_2022-03-06-13-13-32.png)
![](@attachment/Clipboard_2022-03-06-13-13-42.png)
For length-normalized vectors, cosine similarity is simply the dot product:
![](@attachment/Clipboard_2022-03-06-13-14-06.png)
![](@attachment/Clipboard_2022-03-06-13-14-19.png)

## Relevance feedback

![](@attachment/Clipboard_2022-03-06-13-16-18.png)

Two ways to improve recall:
- Query expansion/reformulation
- Relevance feedback

### Query reformulation
Need many formulations of queries for good retrieval. There's the "word mismatch" problem, i.e. some unretrieved relevant documents that are indexed by a different set of terms. Idea = when documents are initally retrieved they should be examined for relevance information and then we can improve the query for retrieving additional relevant documents -> expanding original query with new terms and reweighing of the terms in the expanded query.

**Term re-weighing without query expansion**
![](@attachment/Clipboard_2022-03-06-15-02-11.png)
![](@attachment/Clipboard_2022-03-06-15-02-31.png)

Three approaches (for query reformulation):
- Relevance feedback
- Local analysis: based on information derived from the set of initially retrieved documents
- Global analysis: based on global information derived from the document collection

![](@attachment/Clipboard_2022-03-06-15-03-39.png)
![](@attachment/Clipboard_2022-03-06-15-04-19.png)

Vector space example:
![](@attachment/Clipboard_2022-03-06-15-05-03.png)
![](@attachment/Clipboard_2022-03-06-15-05-16.png)

On a key concept - centroid:
![](@attachment/Clipboard_2022-03-06-15-06-09.png)

**Rocchio algorithm** - to separate relevan/nonrelevant documents
Incorporates relevance feedback information into the vector space model.
![](@attachment/Clipboard_2022-03-06-15-06-57.png)
There's a good illustration of the algorithm on the slides.

**Rocchio algorithm (SMART)**
![](@attachment/Clipboard_2022-03-06-15-08-27.png)
![](@attachment/Clipboard_2022-03-06-15-08-58.png)
In practice: ![](@attachment/Clipboard_2022-03-06-15-09-21.png)

**Relevance feedback - Ide**
Three strategies extending Rocchio's approach:
- Basic formula minus normalization fo the number of relevant and non-relevant documents
- Allowed only feedback from relevant documents
- Allowed limited negative feedback from only the highest ranked non-relevant document
![](@attachment/Clipboard_2022-03-06-15-11-01.png)

### Other on relevance feedback

**Issues**:
- Increased burden on the user: they don't like providing constant feedback -> increased cognitive load
- Often users are not reliable or they just don't make relevance assessments
- Partial relevance assessment: this information is missing
- Why is a document relevant? -> it is not always clear why positive/negative feedback was provided

**Relevance feedback methods**
- ![](@attachment/Clipboard_2022-03-06-15-13-19.png)
- Eye gaze: ![](@attachment/Clipboard_2022-03-06-15-14-27.png)

And in search engine? -> query logs!
![](@attachment/Clipboard_2022-03-06-15-54-07.png)

### Local analysis

![](@attachment/Clipboard_2022-03-06-15-16-44.png)

Examine documents retrieved for query to determine query expansion with no user assistance.
Two strategies (to add terms to the query):
- Local clustering: terms that are synonyms, stemming variations
- Local contex analysis: terms close to query terms in text

Issues:
- Query drift: if top document is not that relevant -> reformulated query may not reflect the user information need
- Computation cost high (must be done at retrieval time, on-line)

### Global analysis

![](@attachment/Clipboard_2022-03-06-15-17-01.png)

**Dictionary-based query expansion**
For each term t in the query, expand the query with words the thesaurus lists as semantically related (e.g. aircraft -> plane).
Generally increases recall and may significantly decrease precision (particularly with ambiguous terms).
Actually widely used in specialised search engines, e.g. science.
Expensive to create and maintain a thesaurus.
![](@attachment/Clipboard_2022-03-06-15-51-56.png)

-> Automatic thesauri construction
![](@attachment/Clipboard_2022-03-06-15-52-26.png)

-> Automatic theaurus generation
Two words are similar if they co-occur with similar words and in a given relation with the same words.
Problems:
- False positives
- False negatives

## Evaluation

What can we evaluate?
![](@attachment/Clipboard_2022-03-06-15-54-41.png)

But it's hard:
- Large variation in human information needs and queries
- The precise contributions of each component are hard to entangle:
  - Collection coverage
  - Document indexing
  - Query formulation
  - Matching algorithm

### Test collections

We compare retrieval performance using a test collection:
- Document collection: documents themselves, can be pdf, html...
- Queries/requests
- Relevance judgements, stating for a query the relevant documents

To compare the performance of two techniques:
- Each used to evaluate test queries
- Reults compared using some performance measure (most common: precision and recall)
- Usually use multiple measures -> different views of performance
- Usually test with multiple collections: performance is collection dependent!

**Creating test collections**
- Documents: must be representative of the documents we expect to see in reality
- Queries: must be representative blah blah
- Human relevance assessments: we need to hire assessors for these tasks, and they must be representative of the users. ! Expensive and time consuming

![](@attachment/Clipboard_2022-03-06-16-17-50.png)
_Note the large test part!_
![](@attachment/Clipboard_2022-03-06-16-18-40.png)

### Standard relevance benchmark

**Cranfield**
Standard relevance benchmark, first allowing quantitative measures, but too small and too untypical for today.
Methodology:
![](@attachment/Clipboard_2022-03-06-16-14-01.png)

**TREC**
Large test collection. Actually a set of several different relevance benchmarks. No exhaustive relevance judgements; available only for the documents that were among the top k returned

**Others**
![](@attachment/Clipboard_2022-03-06-16-18-59.png)

### Relevance judgement

Problem: relevance is situational -> need extensive sampling to counteract natural variation: large population of users and information.
Guidelines given to assessors, in order to define relevance as a reasonably objective property of the document-query pair. Relevance defined to be irrespective of information contained in other documents

**TREC**
![](@attachment/Clipboard_2022-03-06-16-21-32.png)

**Pooling** - put together things!
![](@attachment/Clipboard_2022-03-06-16-22-23.png)

**Validity of relevance assessments**
Relevance assessments must be consistent, unless they are useless! How can we measure this consistency?
![](@attachment/Clipboard_2022-03-06-16-23-27.png)
![](@attachment/Clipboard_2022-03-06-16-23-41.png)

### System oriented evaluation

Measuring effectiveness (with precision and recall). System oriented evaluation = looking at precision and recall

Precision/recall:
![](@attachment/Clipboard_2022-03-06-16-33-08.png)
![](@attachment/Clipboard_2022-03-06-16-33-16.png)
F-score:
![](@attachment/Clipboard_2022-03-06-16-33-30.png)
E-score:
![](@attachment/Clipboard_2022-03-06-16-33-39.png)
Usually need a compromise, but some times one is preferred over the other.
Comparison of systems -> MAP: Mean Average Precision
![](@attachment/Clipboard_2022-03-06-16-35-37.png)
![](@attachment/Clipboard_2022-03-06-16-35-46.png)

Complete methodology:
![](@attachment/Clipboard_2022-03-06-16-35-59.png)

!!! Also important to retrieve relevant documents before the non-relevant ones: **rank based measures**
**Binary relevance** - @K
![](@attachment/Clipboard_2022-03-06-16-36-34.png)
![](@attachment/Clipboard_2022-03-06-16-36-44.png)
MAP:
- If a relevant document never gets retrieved -> precision corresponding to that particular one is zero
- MAP is macro-averaging: each query counts equally
- Assumes user is interested in finding many relevant documents for each query
- Requires many relevance judgements in text collections

**Discounted Cumulative Gain**
![](@attachment/Clipboard_2022-03-06-16-38-33.png)
![](@attachment/Clipboard_2022-03-06-16-39-13.png)
![](@attachment/Clipboard_2022-03-06-16-39-24.png)

**Normalized DCG**
Using DCG to compare two queries may lead to misleading conclusions: queries with more results will have higher DCG -> normalize at rank p
![](@attachment/Clipboard_2022-03-06-16-40-22.png)
![](@attachment/Clipboard_2022-03-06-16-40-33.png)
Limitations:
![](@attachment/Clipboard_2022-03-06-16-41-40.png)

And if the users is interested in only one specific document?
![](@attachment/Clipboard_2022-03-06-16-42-11.png)

**Mean Reciprocal Rank**
Evaluates systems that produce a list of ranked items for queries
![](@attachment/Clipboard_2022-03-06-16-52-33.png)

### Large search engine evaluation

Usually stuff like Precision@K but also:
- Clickthrough on first result
- Analysing search logs
- Studies of user behaviour in the lab
- A/B testing

**A/B testing**
![](@attachment/Clipboard_2022-03-06-16-54-05.png)
![](@attachment/Clipboard_2022-03-06-16-54-55.png)

**Behaviour-based measures...**
![](@attachment/Clipboard_2022-03-06-16-54-22.png)
**...and metrics**
![](@attachment/Clipboard_2022-03-06-16-54-37.png)

**Interleaved ranking**
![](@attachment/Clipboard_2022-03-06-16-55-24.png)
![](@attachment/Clipboard_2022-03-06-16-55-35.png)
Pro/cons:
![](@attachment/Clipboard_2022-03-06-16-55-51.png)

## Evaluating Interactive Information Retrieval

![](@attachment/Clipboard_2022-03-06-16-57-00.png)

### On IIR

Goal: study interaction with a search system to learn about the user's search intent and when they envounter relevant documents.
![](@attachment/Clipboard_2022-03-06-16-59-23.png)

Three problem areas:
![](@attachment/Clipboard_2022-03-06-16-59-51.png)

Solves problems with system-oriented experiments:
![](@attachment/Clipboard_2022-03-06-17-00-56.png)

Experimental step-by-step:
![](@attachment/Clipboard_2022-03-06-17-01-38.png)

What is the best rate for the exploration-exploitation tradeoff? -> LinRel (to decide which document let analyze)
![](@attachment/Clipboard_2022-03-06-18-40-24.png)

**2: evaluation methods**
![](@attachment/Clipboard_2022-03-06-18-41-13.png)

**4: plan user testing**
e.g. ![](@attachment/Clipboard_2022-03-06-18-41-33.png)

**7: analyze results**
![](@attachment/Clipboard_2022-03-06-18-42-04.png)

How to measure:
- Outcome: aspectual precision and aspectual recall (_i.e._ precision and recall but "subjective")
- Process: ![](@attachment/Clipboard_2022-03-06-18-43-03.png)

SCIENCE! Identify and describe the problem -> research questions (narrow and specific enough that it can be addressed in a study) -> hypothesis ![](@attachment/Clipboard_2022-03-06-18-44-33.png)

**Variables and measurements**
Variables represent concepts. To investigate concepts, researchers must engage in two processes:
- Conceptualization
- Operationalization
![](@attachment/Clipboard_2022-03-07-18-15-07.png)

## Statistics basics + Online controlled experiments (aka sum all up)

### Stats 101

Study design: ![](@attachment/Clipboard_2022-03-07-16-10-26.png)

Same old things on null and alternative hypothesis.

We use a **test statistic** to tell whether the alternative hypothesis is more likely to be true than the null hypothesis.

**Permutation test**
Compare the p-value with a significance level a (usually 0.5): if P \<= A the result is considered statistically significant.
![](@attachment/Clipboard_2022-03-07-16-26-52.png)
Pros:
- Non-parametric -> very few assumptions about the data
- Methodologically simple, widely-applicable
Cons:
- Can be computionally intensive
- Lose statistical power
!!! If we make some mathematical assumptions, P-values can be calculated analyticall without permutations!

#### Between vs Within subject study

**Within**
![](@attachment/Clipboard_2022-03-07-16-29-12.png)
How can we say if the mean difference is non-zero? -> Student's t-test
![](@attachment/Clipboard_2022-03-07-16-30-09.png)
![](@attachment/Clipboard_2022-03-07-16-30-24.png)
-> t-statistic ![](@attachment/Clipboard_2022-03-07-16-30-49.png)
-> t-test ![](@attachment/Clipboard_2022-03-07-16-30-58.png)
And paired t-test: ![](@attachment/Clipboard_2022-03-07-16-31-14.png)

**Between**
![](@attachment/Clipboard_2022-03-07-16-28-56.png)
Two sample t-test ![](@attachment/Clipboard_2022-03-07-16-32-07.png)

Issues with t-test:
- Non-normality of the sampling distribution, e.g. if there can only be positive results, ordinal results, etc...
- Outliers can distort results

**Rank transformation**
What if we replace observations with their rank? We lose our data, but we keep the relative positions -> no more outliers
Wilcoxon rank-sum test: ![](@attachment/Clipboard_2022-03-07-16-34-21.png)
Wilcoxon signed-rank test: ![](@attachment/Clipboard_2022-03-07-16-34-41.png)

**Which test to use**?
- In general, prefer non-parametric tests
- If there are outliers: use a non-parametric test
- If data cannot be normally distributed: use a non-parametric test
- If possible, design experiment with paired obervations and make sure to randomize systems

### OCE - A/B testing
![](@attachment/Clipboard_2022-03-06-18-49-11.png)
![](@attachment/Clipboard_2022-03-06-18-49-42.png)
![](@attachment/Clipboard_2022-03-06-18-50-33.png)

Procedure:
![](@attachment/Clipboard_2022-03-06-18-51-23.png)
![](@attachment/Clipboard_2022-03-06-18-51-44.png)
![](@attachment/Clipboard_2022-03-06-18-52-22.png)
![](@attachment/Clipboard_2022-03-06-18-52-35.png) ![](@attachment/Clipboard_2022-03-06-18-53-02.png)
![](@attachment/Clipboard_2022-03-06-18-53-17.png)
![](@attachment/Clipboard_2022-03-06-18-53-54.png)
![](@attachment/Clipboard_2022-03-06-18-54-24.png)
![](@attachment/Clipboard_2022-03-06-18-54-45.png)
![](@attachment/Clipboard_2022-03-06-18-55-32.png)
![](@attachment/Clipboard_2022-03-06-18-56-23.png)

**At a large scale**
- Trustworthiness
- Massive data logged
- Carryover effect: shuffle users all the time! (segments of users exposed to a bad experience will carry over to the next experiment)

**Strategies in practice**
![](@attachment/Clipboard_2022-03-06-18-57-55.png)

**Issues**
![](@attachment/Clipboard_2022-03-06-18-58-44.png)

**Other techniques vs A/B testing**
![](@attachment/Clipboard_2022-03-06-18-59-17.png)
![](@attachment/Clipboard_2022-03-06-18-59-35.png)






























