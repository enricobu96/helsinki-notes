---
attachments: [Clipboard_2021-09-30-22-08-16.png, Clipboard_2021-09-30-22-12-07.png, Clipboard_2021-09-30-22-17-21.png, Clipboard_2021-09-30-22-18-44.png, Clipboard_2021-09-30-22-19-05.png, Clipboard_2021-09-30-22-20-01.png, Clipboard_2021-09-30-22-24-33.png, Clipboard_2021-09-30-22-24-56.png, Clipboard_2021-09-30-22-26-37.png, Clipboard_2021-09-30-22-27-47.png, Clipboard_2021-09-30-22-30-12.png, Clipboard_2021-09-30-22-31-18.png, Clipboard_2021-09-30-22-32-15.png, Clipboard_2021-09-30-22-36-56.png, Clipboard_2021-09-30-22-38-48.png, Clipboard_2021-09-30-22-39-16.png, Clipboard_2021-09-30-22-39-48.png, Clipboard_2021-09-30-22-41-07.png, Clipboard_2021-09-30-22-41-30.png, Clipboard_2021-09-30-22-42-44.png, Clipboard_2021-09-30-22-44-31.png, Clipboard_2021-09-30-22-45-02.png, Clipboard_2021-09-30-22-46-55.png, Clipboard_2021-09-30-22-48-31.png]
tags: [Introduction to Data Science]
title: Lesson 8 - 28/09
created: '2021-09-30T19:03:19.553Z'
modified: '2021-09-30T19:51:16.113Z'
---

# Lesson 8 - 28/09

## Computer Vision

Giving computers the ability to understand visual information

### From pixels to understanding

The camera image needs to be digitalized for computer processing, turning it into millions of discrete picutre elements, or pixels

![](@attachment/Clipboard_2021-09-30-22-08-16.png)

## Deep Learning

Before:
- Hand-crafted features, e.g. colour distributions, edge histograms
- Complicated feature selection mechanism
- "Classical" ML, e.g. kernel methods like SVM

About 10 years ago: Deep Learning
- Increased in processing power and data leads to feasible neural networks
- End-to-end learning, i.e. the network itself learns the features
- Each layer typically learns a higher level of representation
- However: entirely data-driven, features can be hard to interpret

### DL = Neural Networks

![](@attachment/Clipboard_2021-09-30-22-12-07.png)

#### Learning in neural networks

- **Forward pass**: input is passed through the network
- Each node interacts with every previous node via the weights

![](@attachment/Clipboard_2021-09-30-22-17-21.png)

- Compare output to true answer: loss function ![](@attachment/Clipboard_2021-09-30-22-18-44.png)
- Learning with stochastic gradient descent, we seek gradients ![](@attachment/Clipboard_2021-09-30-22-19-05.png)
- Gradients for each weight are obtained with **backpropagation** ![](@attachment/Clipboard_2021-09-30-22-20-01.png) and so on until the first layer

### Deep Learning for Vision

While we don't hand-craft features anymore, in practice we still apply some "domain knowledge" to make learning feasible:
- Neighbouring pixels are probably related (convolutions)
- There are common image features which can appear anywhere such as edges, corners, etc (weight sharing)
- Often the exact location of a feature isn't important (max pooling)

#### Convolutional Neural Networks

![](@attachment/Clipboard_2021-09-30-22-24-33.png)

![](@attachment/Clipboard_2021-09-30-22-24-56.png)

So no more every node connected with every other node in the next layer, but only a few

##### Convolution in 2D

We arrange the input and output neurons in 2D:
![](@attachment/Clipboard_2021-09-30-22-26-37.png)

The output is the result of a weighted sum of a small local area in the previous layer, and this is a **convolution**
![](@attachment/Clipboard_2021-09-30-22-27-47.png)

The weights `K(m,n)` is what is learned

##### Learning in layers

The convolutional layer learns several sets of weights, each a kind of feature detector

![](@attachment/Clipboard_2021-09-30-22-30-12.png)

And these are built up in layers

![](@attachment/Clipboard_2021-09-30-22-31-18.png)

Until we get out end result, e.g. an object detector

![](@attachment/Clipboard_2021-09-30-22-32-15.png)

#### Real convolutional neural nets

What we call CNNs actually also contain a other types of operations/layers: fully connected layers, non-linearities

![](@attachment/Clipboard_2021-09-30-22-36-56.png)

##### Examples of real CNNs

###### AlexNet

![](@attachment/Clipboard_2021-09-30-22-38-48.png)

###### Inception V3

![](@attachment/Clipboard_2021-09-30-22-39-16.png)

###### ResNet-152

![](@attachment/Clipboard_2021-09-30-22-39-48.png)

## Object Recognition Challenge

![](@attachment/Clipboard_2021-09-30-22-41-07.png)

### Accuracy vs model complexity

(circle size = number of parameters)

![](@attachment/Clipboard_2021-09-30-22-41-30.png)

## Computer vision applications

### Semantic Segmentation

![](@attachment/Clipboard_2021-09-30-22-42-44.png)

With this you can do object detection and localisation

![](@attachment/Clipboard_2021-09-30-22-44-31.png)

### Describing an image

![](@attachment/Clipboard_2021-09-30-22-45-02.png)

### Generative Adversarial Networks (GANs)

- We have two networks: generator and discriminator
- The generator produces samples, while the discriminator tries to distinguish between real data items and the generated samples
- The discriminator tries to learn to classify correctly, while the generator in turn tries to learn to fool the discriminator

![](@attachment/Clipboard_2021-09-30-22-46-55.png)

## AI vs humans?

On ImageNet, where do humans stand?

![](@attachment/Clipboard_2021-09-30-22-48-31.png)

But don't confuse classification accuracy with understanding! Humans can understand the context, machines don't!
