---
attachments: [Clipboard_2021-09-20-10-52-36.png, Clipboard_2021-09-20-11-40-51.png, Clipboard_2021-09-21-09-54-04.png, Clipboard_2021-09-21-09-54-14.png, Clipboard_2021-09-21-09-54-25.png, Clipboard_2021-09-21-09-56-05.png, Clipboard_2021-09-21-10-02-24.png, Clipboard_2021-09-21-10-03-01.png, Clipboard_2021-09-21-10-05-39.png]
tags: [Introduction to Data Science]
title: Lesson 5 - 20/09
created: '2021-09-20T07:20:01.190Z'
modified: '2021-09-21T07:07:51.742Z'
---

# Lesson 5 - 20/09

## What is Machine Learning?

Definition:
- Machine = computer, computer program
- Learning = improving performance on a **given** task, based on experience/examples

In other words, instead of the programmer writing explicit rules for how to solve a given problem, the programmer instructs the computer how to learn from examples. In many cases the computer program can even become better at the task than the programmer is.

### Example: spam filter

- Method 1: programmer writes rules: "if it contains 'lottery' then it is spam". Difficult, not user-adaptive
- Method 2: the user marks which mails are spam, which are legit, and a ML algorithm is used to construct a classifier

### Machine learning setting

One definition of ML is "a computer program improves its performance on a given task with experience (i.e. examples, data)".
So we need to separate:
- Task: what is the problem that the program is solving?
- Performance measure: how is the performance of the program (when solving the given task) evaluated?
- Experience: what is the data (examples) that the program is using to improve its performance?

### Neighboring disciplines

- Artificial Intelligence (AI): machine learning can be seen as one approach towards implementing "intelligent" machines, but there are other approaches too (e.g. static rule-based systems)
- Neural networks, deep learning: inspired by and trying to mimic the function of biological brains, in order to make computers that learn from experience. Modern ML really grew out of the neural networks boom in the 1980s and 1990s
- Statistics: more like hypotesis testing and some other basic problems such as linear regression. Plenty of "cross fertilization" between research in ML and statistics

![](@attachment/Clipboard_2021-09-20-10-52-36.png)

### Kinds of Machine Learning

We'll mostly focus on supervised ML; our goal is to learn to identify a ML problem and choose a workable approach, instead of learning the details.

#### Supervised ML

Task is to predict the correct (or good) response `y` given an input `x`, e.g.:
- Classify emails as spam or legit
- Classify handwritten numbers or other symbols
- Predict movie profits based on directors, actors...
- Generate text description of images

#### Unsupervised ML

Task is to create models or summaries of the input `x` (no `y`), e.g.:
- Clustering (users, products, text documents by topic, ...)
- Building dependency graphs (Bayesian networks, ...)
- Reducing dimensionality to the essentials
- Visualization (dimension reduction to 2D/3D)

#### Semi-supervised learning

Supervised learning task but only some training data is labeled

#### Reinforcement learning

Supervised learning but no direct feedback about the goodnes of individual choices; instead delayed reward/penalty

### Some examples

![](@attachment/Clipboard_2021-09-21-09-54-04.png)
![](@attachment/Clipboard_2021-09-21-09-54-14.png)
![](@attachment/Clipboard_2021-09-21-09-54-25.png)


### Loss functions

The key problem in supervides learning (classification and regression) is to maximize the **predictive performance**; for big data scenarios, computational complexity may become critical as well.
The performance is measured using a **loss function**. Tipically:
![](@attachment/Clipboard_2021-09-21-09-56-05.png)

- **Training loss**: average `L(f(x),y)` over `(x,y)` in _training_ dataset
- **Test loss**: average `L(f(x),y)` over `(x,y)` in _test_ dataset

Example loss functions:
![](@attachment/Clipboard_2021-09-20-11-40-51.png)

The real "cost" or utility in the practical application is minimizing one thing can be far from optimal in terms of another.

### Overfitting
_Thinking you're smarter than you are_

Training loss can be low because:
- The problem is simple and good predictions easy to find
- We have tried many different predictors and some of them just happen to fit the **training** data

The last one is called **overfitting** -> training error is small, but **test error** is big.

General rule: the smaller the training set, the easier is to overfit; you may think you're doing better than you are because of small training set error.

- The overfitting problem is closely related to the complexity of the models being fitted
- There are fewer simple models than complex models; therefore, fitting a simple model leads to a lower risk of overfitting than fitting a complex model.

Example:
![](@attachment/Clipboard_2021-09-21-10-02-24.png)

#### Validation

A separate validation data set can be used to reduce the risk of overfitting:
![](@attachment/Clipboard_2021-09-21-10-03-01.png)

Fit models with varying complexity on training data, e.g.:
- Regression with different covariate subsets (feature selection)
- Decision trees with variable number of nodes
- Support Vector Machines with different regularization parameters

!!! Try to choose the subset/number of nodes/regularization based on performance on the validation set

#### Cross-validation

To get more reliable statistics than a single split provides, use K-fold cross-validation:
1. Divide the data into K equal-sized subsets
  ![](@attachment/Clipboard_2021-09-21-10-05-39.png)
2. For j from 1 to K:
  2.1. Train the model(s) using all data except that of subset j
  2.2. Compute the resulting validation error on the subset j
3. Average the K results

When K = N (i.e. each datapoint is a separate subset) this is known as leave-one-out cross-validation.


















