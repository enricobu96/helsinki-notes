---
attachments: [Clipboard_2022-03-01-15-52-33.png, Clipboard_2022-03-01-15-53-08.png, Clipboard_2022-03-01-15-53-33.png, Clipboard_2022-03-01-15-53-44.png, Clipboard_2022-03-01-15-53-54.png, Clipboard_2022-03-01-15-54-38.png, Clipboard_2022-03-01-15-54-49.png, Clipboard_2022-03-01-15-56-07.png, Clipboard_2022-03-01-15-57-16.png, Clipboard_2022-03-01-15-57-43.png, Clipboard_2022-03-01-15-57-58.png, Clipboard_2022-03-01-15-58-07.png, Clipboard_2022-03-01-15-58-29.png, Clipboard_2022-03-01-15-58-53.png, Clipboard_2022-03-01-15-59-02.png, Clipboard_2022-03-01-15-59-54.png, Clipboard_2022-03-01-16-01-20.png, Clipboard_2022-03-01-16-01-35.png, Clipboard_2022-03-01-16-03-47.png, Clipboard_2022-03-02-14-56-16.png, Clipboard_2022-03-02-15-00-50.png, Clipboard_2022-03-02-15-01-41.png, Clipboard_2022-03-02-15-04-46.png, Clipboard_2022-03-02-15-05-09.png, Clipboard_2022-03-02-15-05-49.png, Clipboard_2022-03-02-15-08-07.png, Clipboard_2022-03-02-15-08-42.png, Clipboard_2022-03-02-15-09-13.png, Clipboard_2022-03-02-15-09-25.png, Clipboard_2022-03-02-15-09-38.png, Clipboard_2022-03-02-15-10-05.png, Clipboard_2022-03-02-15-13-17.png, Clipboard_2022-03-02-15-14-26.png, Clipboard_2022-03-02-15-14-36.png, Clipboard_2022-03-02-15-15-21.png, Clipboard_2022-03-02-15-31-50.png, Clipboard_2022-03-02-16-15-33.png, Clipboard_2022-03-02-16-17-30.png, Clipboard_2022-03-02-16-17-43.png, Clipboard_2022-03-02-16-19-09.png, Clipboard_2022-03-02-16-19-48.png, Clipboard_2022-03-02-16-20-35.png, Clipboard_2022-03-02-16-20-57.png, Clipboard_2022-03-02-16-21-09.png, Clipboard_2022-03-02-16-21-43.png, Clipboard_2022-03-02-16-23-29.png, Clipboard_2022-03-02-16-24-19.png, Clipboard_2022-03-02-16-25-41.png, Clipboard_2022-03-02-16-26-39.png, Clipboard_2022-03-02-16-50-36.png, Clipboard_2022-03-02-17-25-55.png, Clipboard_2022-03-02-17-26-06.png, Clipboard_2022-03-02-17-27-34.png, Clipboard_2022-03-02-17-27-53.png, Clipboard_2022-03-02-17-28-02.png, Clipboard_2022-03-02-17-28-11.png, Clipboard_2022-03-02-17-28-18.png, Clipboard_2022-03-02-17-39-08.png, Clipboard_2022-03-02-17-39-45.png, Clipboard_2022-03-02-17-40-15.png, Clipboard_2022-03-02-17-40-30.png, Clipboard_2022-03-02-17-40-46.png, Clipboard_2022-03-02-17-41-00.png, Clipboard_2022-03-02-17-41-27.png, Clipboard_2022-03-02-17-41-49.png, Clipboard_2022-03-02-17-42-00.png, Clipboard_2022-03-02-17-42-11.png, Clipboard_2022-03-02-17-42-22.png, Clipboard_2022-03-02-17-42-41.png, Clipboard_2022-03-02-17-45-11.png, Clipboard_2022-03-02-17-45-32.png, Clipboard_2022-03-02-17-46-06.png, Clipboard_2022-03-02-17-47-50.png, Clipboard_2022-03-03-11-45-49.png, Clipboard_2022-03-03-11-50-04.png, Clipboard_2022-03-03-12-09-32.png, Clipboard_2022-03-03-12-10-46.png, Clipboard_2022-03-03-12-10-55.png, Clipboard_2022-03-03-12-11-16.png, Clipboard_2022-03-03-12-14-21.png, Clipboard_2022-03-03-12-14-31.png, Clipboard_2022-03-03-12-16-32.png, Clipboard_2022-03-03-14-33-15.png, Clipboard_2022-03-03-14-35-35.png, Clipboard_2022-03-03-14-37-20.png, Clipboard_2022-03-03-14-38-56.png, Clipboard_2022-03-03-14-39-59.png, Clipboard_2022-03-03-14-41-08.png, Clipboard_2022-03-03-14-41-16.png, Clipboard_2022-03-03-14-43-28.png, Clipboard_2022-03-03-14-44-43.png, Clipboard_2022-03-03-14-45-34.png, Clipboard_2022-03-03-14-47-43.png, Clipboard_2022-03-03-15-18-17.png, Clipboard_2022-03-03-16-39-00.png, Clipboard_2022-03-03-16-41-36.png, Clipboard_2022-03-03-17-43-49.png, Clipboard_2022-03-03-17-57-52.png, Clipboard_2022-03-03-18-01-47.png, Clipboard_2022-03-03-18-02-14.png, Clipboard_2022-03-03-18-04-11.png, Clipboard_2022-03-04-11-36-57.png, Clipboard_2022-03-04-11-38-57.png, Clipboard_2022-03-04-11-40-08.png, Clipboard_2022-03-04-11-40-31.png, Clipboard_2022-03-04-11-44-44.png, Clipboard_2022-03-04-11-50-34.png, Clipboard_2022-03-04-11-51-04.png, Clipboard_2022-03-04-11-52-54.png]
favorited: true
tags: [Introduction to Deep Learning]
title: Exam prep
created: '2022-03-01T13:48:34.199Z'
modified: '2022-03-04T10:22:12.192Z'
---

# Exam prep

# Intro

## Loss function related stuff

![](@attachment/Clipboard_2022-03-02-15-08-07.png)

This will overfit -> regularization, but will see later
![](@attachment/Clipboard_2022-03-02-15-08-42.png)

Common loss functions:
- hinge: ![](@attachment/Clipboard_2022-03-02-15-09-13.png), ![](@attachment/Clipboard_2022-03-02-15-09-25.png)
- log-loss: ![](@attachment/Clipboard_2022-03-02-15-09-38.png)
- cross-entropy (negative-log likelihood): ![](@attachment/Clipboard_2022-03-02-15-10-05.png)
- ranking losses

## Cross Entropy related stuff

![](@attachment/Clipboard_2022-03-01-15-53-33.png)
![](@attachment/Clipboard_2022-03-01-15-53-44.png)
![](@attachment/Clipboard_2022-03-01-15-53-54.png)

For instance:
![](@attachment/Clipboard_2022-03-01-15-54-38.png)
![](@attachment/Clipboard_2022-03-01-15-54-49.png)

## Pytorch related stuff
![](@attachment/Clipboard_2022-03-02-16-15-33.png)
![](@attachment/Clipboard_2022-03-03-12-16-32.png)

## ML basics to not forget

![](@attachment/Clipboard_2022-03-01-16-03-47.png)

Generally, you use the three-way split:
![](@attachment/Clipboard_2022-03-02-14-56-16.png)

sign function to classify, but to get the confidence?
![](@attachment/Clipboard_2022-03-02-15-00-50.png)

And multiclassification?
![](@attachment/Clipboard_2022-03-02-15-01-41.png)

Model capacity
![](@attachment/Clipboard_2022-03-02-15-31-50.png)

## Representation related stuff

![](@attachment/Clipboard_2022-03-02-15-04-46.png)
![](@attachment/Clipboard_2022-03-02-15-05-09.png)

And confidence for multiclassification?
![](@attachment/Clipboard_2022-03-02-15-05-49.png)

## GD/SGD related stuff

Intro on **gradient descent**:
![](@attachment/Clipboard_2022-03-01-15-56-07.png)

**Brief excursus on partial derivatives**
![](@attachment/Clipboard_2022-03-01-15-57-43.png)
![](@attachment/Clipboard_2022-03-01-15-57-58.png)
![](@attachment/Clipboard_2022-03-01-15-58-07.png)
Chain rule:![](@attachment/Clipboard_2022-03-01-15-58-29.png)

Computing the gradient (let's use mean squared loss as loss function) [on linear regression]:
![](@attachment/Clipboard_2022-03-01-15-57-16.png)
![](@attachment/Clipboard_2022-03-01-15-58-53.png)
![](@attachment/Clipboard_2022-03-01-15-59-02.png)

And then **stochastic gradient descent**:
![](@attachment/Clipboard_2022-03-01-15-59-54.png)
(notes: "draw one input-output pair..." -> in practice we make multiple passes over same training points, these are colled **epochs**)

So SGD for Linear regression (still, that's a simple example)
![](@attachment/Clipboard_2022-03-01-16-01-35.png)

![](@attachment/Clipboard_2022-03-02-15-13-17.png)

**SGD**
![](@attachment/Clipboard_2022-03-02-15-14-26.png)
![](@attachment/Clipboard_2022-03-02-15-14-36.png)
![](@attachment/Clipboard_2022-03-02-15-15-21.png)

# FFNN

## Non-linear models -> FFNN

We have non-linear data. And now?
![](@attachment/Clipboard_2022-03-02-16-17-30.png)

![](@attachment/Clipboard_2022-03-02-16-17-43.png)
But great danger of overfitting due to high dimensionality -> maybe...we can let the model select the combinations that matter itself: key feature and break-through of NNs
![](@attachment/Clipboard_2022-03-02-16-19-09.png)

## FFNN/MLP

### General/Intro

Ok, now, **MLP** (Multi Layer Perceptron)
![](@attachment/Clipboard_2022-03-02-16-19-48.png)
![](@attachment/Clipboard_2022-03-03-11-50-04.png)
![](@attachment/Clipboard_2022-03-03-12-09-32.png)
![](@attachment/Clipboard_2022-03-03-12-10-55.png)
![](@attachment/Clipboard_2022-03-03-12-11-16.png)

Defining a NN using matrices
![](@attachment/Clipboard_2022-03-02-16-20-35.png)

And then a second layer comes...
![](@attachment/Clipboard_2022-03-02-16-20-57.png)
![](@attachment/Clipboard_2022-03-02-16-21-09.png)

Hey! Watch out! If you combine more linear layers...you will have a linear layer. We need non-linearity:
![](@attachment/Clipboard_2022-03-02-16-23-29.png)

(the famous XOR problem example)
![](@attachment/Clipboard_2022-03-02-16-24-19.png)
Still XOR -> ![](@attachment/Clipboard_2022-03-03-11-45-49.png)
And this fucking equation describes the MLP!

Ok, so, MLP, we define the network structure, i.e.
![](@attachment/Clipboard_2022-03-02-16-25-41.png)
The output layer is the final prediction (and we use like softmax in classification)
Hidden layers woirk as feature learning (hopefully) ![](@attachment/Clipboard_2022-03-02-16-26-39.png)
!!! The features are progressively higher-level features

Some common nonlinearities:
![](@attachment/Clipboard_2022-03-03-12-14-21.png)
![](@attachment/Clipboard_2022-03-03-12-14-31.png)

### Training - SGD, backprop

How do we train? Using SGD
![](@attachment/Clipboard_2022-03-02-16-21-43.png)
Actually we use backpropagation (which is dynamic programming, tmyk) bc SGD alone is too inefficient (we calculate same derivatives over and over again)

![](@attachment/Clipboard_2022-03-02-16-50-36.png)

**On sizes** (MLP, Layer 2 has 16 neurons and Layer 3 has 10 neurons, which is the shape of the gradient of the loss wrt:

- activations h() at layer 2: 16x1 column vector
- pre-activations a() ath layer 2: 16x1 column vector
- pre-activations a() at layer 3: 10x1 column vector
- weight matrix W between layer 2 and 3: 10x16 matrix
![](@attachment/Clipboard_2022-03-02-17-27-34.png)
![](@attachment/Clipboard_2022-03-02-17-27-53.png)
![](@attachment/Clipboard_2022-03-02-17-28-02.png)
![](@attachment/Clipboard_2022-03-02-17-28-11.png)
![](@attachment/Clipboard_2022-03-02-17-28-18.png)

# CNNs

## Intro and structure

Using convolutional filters, we can look at a small section of the image at a time. Convolution operation = applying a convolutional filter

![](@attachment/Clipboard_2022-03-03-14-33-15.png)

Is convolution a linear operator or not? Yes -> in fact then you have to put some non-linearity (e.g. relu) on top of the convolutional layer.

![](@attachment/Clipboard_2022-03-03-14-35-35.png)
![](@attachment/Clipboard_2022-03-03-14-37-20.png)
![](@attachment/Clipboard_2022-03-03-14-47-43.png)

Differences between FFNNs and CNNs?
1. Locality -> nearby pixels are more strongly related than distant ones; objects are built up out of smaller parts
2. Parameter sharing ![](@attachment/Clipboard_2022-03-03-16-41-36.png)
3. FFNNs are "not translation invariant". Invariant means that if we translate the input by a small amount, the values of most of the pooled outputs do not change
4. There's **equivariance**, i.e. if the input changes, the output changes the same way.

Filters learned by CNNs: image -> low-level feature -> mid-level feature -> high level feature -> trainable classifier -> classification (output)

## Practical facts

**Convolution changes the dimension**
![](@attachment/Clipboard_2022-03-03-14-38-56.png)
-> Padding: we add zeros at the boundaries (before convolution! so like 10x10->12x12->10x10 again)

**With colors**
![](@attachment/Clipboard_2022-03-03-14-39-59.png)

**Stride**, i.e. "how many pixels to skip". Improves efficiency but decreases resolution

## Convolutional layer(s)

**What's a convolution**
![](@attachment/Clipboard_2022-03-03-16-39-00.png)
- First argument (x) is the input, second argument (w) is the kernel, output is the feature map.
- Commutative operation

**Convolutional layer example**
![](@attachment/Clipboard_2022-03-03-14-41-08.png)
![](@attachment/Clipboard_2022-03-03-14-41-16.png)

**To fully specify a convolutional layer** we need:
1. The input dimension
2. The filter size
3. The number of filters
4. The padding size
5. The stride
![](@attachment/Clipboard_2022-03-03-14-43-28.png)

![](@attachment/Clipboard_2022-03-03-15-18-17.png)

### Pooling layers

Pooling layers are used to reduce the dimensions of the feature maps. Thus, it reduces the number of parameters to learn and the amount of computation performed in the network. The pooling layer summarises the features present in a region of the feature map generated by a convolution layer.

![](@attachment/Clipboard_2022-03-03-14-44-43.png)

**Max pooling**: take the max element in the considered set of pixels
![](@attachment/Clipboard_2022-03-03-14-45-34.png)

- We trade some spacial resolution for efficiency
- Can improve results actually, increasing spatial tolerance -> for image recognition is often not necessary to know the exact position of an object
- Brings long-distance items close, allowing greater "view fields"

**Average pooling**: same of max pooling but taking the average instead of the max

### Training a CNN

- Using backprop
- Convolutional filters are initialized randomly
- GPU typically used due to high number of dot products
- Quite large memory footprint bc the intermediate results of each convolution operation need to be stored for backprop

# Regularization and optimization

## Some intro

**Some remainder on variance and bias**
![](@attachment/Clipboard_2022-03-03-17-43-49.png)
Too simple model with very few parameters: high bias, low variance, and vice versa
![](@attachment/Clipboard_2022-03-03-17-57-52.png)
Avoidable bias:

- Train more powerful model (e.g. nonlinear instead of linear)
- Tune NN architecture and hyperparameters
- Train longer
- Optimization (see later)

Variance:

- Find more data
- Tune NN and hyperparameters
- Train shorter
- Regularize (see later)

Regularization controls variance without compromising bias too much

First approach is to divide data good:
1. Dev and test set shoud come from the "same" distribution
2. Be as noise-free as possible
3. For training set, some noise may even be useful

**Scenarios**:
1. Error on training set = 18%, error on developement set = 19% -> underfitting: high bias, low variance, no real point in collecting more data
2. Error on training set = 1%, error on developement set = 19% -> overfitting: low bias, high variance, more data (=more diversity) or better techniques to overfit less should be useful
3. Error on training set = 15%, error on developement set = 27% -> horrible model, high bias, high variance

## Regularization

**Approach 0**
Keep model simple. But in practice works better having a large and regularized model.

### Approach 1
Keep parameters under control. 
**L2-norm**
![](@attachment/Clipboard_2022-03-03-18-02-14.png)
Works bc now the model has to worry about the L2-norm of its parameters too, it cannot just grow its parameters until it matches the training data 100% bc their L2 norm contributes personally to the loss. !!! Do not use for sparsity, use L1 instead

**L1-norm**
![](@attachment/Clipboard_2022-03-03-18-04-11.png)
Every non-zero parameter is gonna contribute to its loss. Best not to use a parameter at all, unless it really needs it. Works in sparsity, !!! do not use for moderate weight control without sparsity as your real goal. Quite strong

### Approach 2

**Ensemble learning**
Idea is to train several models (different) separately and then have all of them vote on test input.
Combine: different learning algorithms, same algorithms trained in different ways and also same algorithms trained the same way. Combine -> majority vote, confidence-weighted vote, learning hot to combine.
Why tf does this work? -> Different models usually make different errors, if we combine enough models we can out-weight the erroneous model every time.
Assumptions: classifiers independent in predictions and with accuracy >50%
Two important words:

- Bagging: varies data set every time, k different datasets each sampled with replacement from the original set
- Boosting: weight your data in the meanwhile and build one model incrementally. Wrongly classified data takes more weight

**Dropout**
This is a way of practical ensemble learning for NNs. It's an expensive approximation to training and evaluating an ensemble of exponentially-many networks. Basically we drop randomly in training time; at every minibatch, we apply an independent filter, and this prevents the neurons from placing "too much trust" in one single input or neuron activation.
We don´t know why it works but it does

### Approach 3

Don't train too enthusiastically

**Multi-task learning**
I don't want my model to memorize the perfect solution on the training data -> I'll just confuse it by making it have to solve many problems at once. How it works: by having one basic set of parameters (shared between tasks) and another set fine-tuned to each task. Model has to reuse shared params, so they better be generic ones; also this way there's also more data for training (by pooling in frm all tasks).

### Approach 4

**Inject noise**

### Approach 5

**Adversarial training**
By optimizing for an input x' very close to input x but for which the model output is different, and then train on these adversarially perturbed examples, for example by training a second adversarial network.

### Approach 6

Find more data.

**Data augmentation**
For instance invariant transformations of your image/speech/etc... (like translation of an image)
Injecting noise in your data to create noisy forms of the original examples; small random noise should be both solvable and beneficial.

**Just find more data haha is it real world even real haha**

## Optimization

Problems:

- We have these saddle points that are a problem (local minima) because sometimes the found one is not the global minima
- Ill-conditioning
- Cliff and exploding gradients
- Long term dependencies

### Approach 1

Choose your non-linearity wisely.

### Approach 2

"Make your signals behave well"

**Batch normalization**
![](@attachment/Clipboard_2022-03-04-11-36-57.png)
In a neural network, batch normalization is achieved through a normalization step that fixes the means and variances of each layer's inputs. Ideally, the normalization would be conducted over the entire training set, but to use this step jointly with stochastic optimization methods, it is impractical to use the global information. Thus, normalization is restrained to each mini-batch in the training process. 
Helps prevent covariate shift and vanishing gradients, allows higher learning rates, may smooth-out the error surface and slightly regularize the model.
Problems: fails for small batch sizes, recurrent architectures is a problem, different training and test set calculations

To facilitate learning, we typically normalize the initial values of our parameters by initializing them with zero mean and unit variance. As training progresses and we update parameters to different extents, we lose this normalization, which slows down training and amplifies changes as the network becomes deeper.

Batch normalization [30] reestablishes these normalizations for every mini-batch and changes are back-propagated through the operation as well. By making normalization part of the model architecture, we are able to use higher learning rates and pay less attention to the initialization parameters. Batch normalization additionally acts as a regularizer, reducing (and sometimes even eliminating) the need for Dropout.

**Layer normalization**(or other normalizations)
![](@attachment/Clipboard_2022-03-04-11-38-57.png)
Layer Normalization directly estimates the normalization statistics from the summed inputs to the neurons within a hidden layer so the normalization does not introduce any new dependencies between training cases.

### Approach 3

**Momentum**
![](@attachment/Clipboard_2022-03-04-11-40-08.png)
You want to use it if you have ill-conditioning
![](@attachment/Clipboard_2022-03-04-11-40-31.png)

### Approach 4

Adaptive learning rate
Learning rate is difficult to tune; the cost may be super sensitive to some directions in the parameter space, and insensitive to others -> how about setting a separate learning rate for each parameter?

**AdaGrad**
Individually adapt the LR for each parameter ![](@attachment/Clipboard_2022-03-04-11-44-44.png)
Idea: if one parameter is changing too rapidly, be careful in this direction and go slowly, if another is changing slowly you can go faster to save time.

**RMSProp** (root-mean-square)
Modification of AdaGrad but instead of the sum of all its history of gradient use a weighted moving average ![](@attachment/Clipboard_2022-03-04-11-50-34.png)

**Adam** (adaptive moments) = RMSProp+Momentum
![](@/Clipboard_2022-03-04-11-51-04.png)
Actually works worse than SGD but it's faster

**Adam with weight decay (AdamW)**
Adam doesn't generalize all that well: requires more regularization, but L2 is not effective in combination with Adam -> weigth decay ![](@attachment/Clipboard_2022-03-04-11-52-54.png)

### Approach 5

"Newton"

**Approximate 2nd order methods**
Check not only the gradient, but also the gradient of the gradient (aka 2nd derivative aka Hessian). Pain in the ass to comupte -> that's why "approximate"
Ideal for ill-conditioning, but bums out for saddle points

### Approach 6

Start well:
Gotta break symmetry: if all initial weigths are the same, they will ALL grow the same too, so we need to initialize each a bit differently -> draw them from a normal uniform distribution or wathever, set all the biases to 0.

# Word embeddings stuff





































