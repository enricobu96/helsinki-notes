---
attachments: [Clipboard_2022-04-05-12-19-20.png, Clipboard_2022-04-05-12-27-02.png, Clipboard_2022-04-05-12-27-21.png, Clipboard_2022-04-05-12-34-20.png, Clipboard_2022-04-05-12-36-46.png, Clipboard_2022-04-05-12-39-30.png, Clipboard_2022-04-05-12-43-38.png, Clipboard_2022-04-05-12-44-30.png, Clipboard_2022-04-05-12-45-17.png, Clipboard_2022-04-05-12-45-36.png, Clipboard_2022-04-05-12-45-47.png, Clipboard_2022-04-05-13-17-53.png, Clipboard_2022-04-05-13-18-04.png, Clipboard_2022-04-05-13-21-20.png, Clipboard_2022-04-05-13-21-57.png, Clipboard_2022-04-05-13-25-10.png, Clipboard_2022-04-05-13-26-07.png, Clipboard_2022-04-05-13-31-21.png, Clipboard_2022-04-05-13-31-51.png, Clipboard_2022-04-05-13-35-45.png, Clipboard_2022-04-05-13-39-50.png, Clipboard_2022-04-05-13-40-08.png, Clipboard_2022-04-05-13-52-12.png, Clipboard_2022-04-05-13-54-36.png, Clipboard_2022-04-05-13-55-46.png, Clipboard_2022-04-05-13-58-30.png, Clipboard_2022-04-05-14-00-28.png, Clipboard_2022-04-05-14-00-42.png, Clipboard_2022-04-05-14-00-52.png]
tags: [Advanced Machine Learning]
title: Lesson 7 - 05/04
created: '2022-04-05T09:18:49.686Z'
modified: '2022-04-05T11:07:54.674Z'
---

# Lesson 7 - 05/04

## Neural networks

![](@attachment/Clipboard_2022-04-05-12-19-20.png)

### Neuron

![](@attachment/Clipboard_2022-04-05-12-27-02.png)

![](@attachment/Clipboard_2022-04-05-12-27-21.png)

### MLP

Same things we saw in DL -> input layer, hidden layers, output layer

**Why non-linearity?**
![](@attachment/Clipboard_2022-04-05-12-34-20.png)
-> bring flexibility:
![](@attachment/Clipboard_2022-04-05-12-36-46.png)

#### Activation functions

![](@attachment/Clipboard_2022-04-05-12-39-30.png)

ReLU behaves more nicely because the gradients do not saturate

#### Learning MLPs

![](@attachment/Clipboard_2022-04-05-12-43-38.png)

-> **Backpropagation**
  Some notation: ![](@attachment/Clipboard_2022-04-05-12-44-30.png)
  Algorithm:
  ![](@attachment/Clipboard_2022-04-05-12-45-17.png)
  ![](@attachment/Clipboard_2022-04-05-12-45-36.png)
  ![](@attachment/Clipboard_2022-04-05-12-45-47.png)

-> Training MLPs

#### Gradients of MLP

![](@attachment/Clipboard_2022-04-05-13-17-53.png)

![](@attachment/Clipboard_2022-04-05-13-18-04.png)

![](@attachment/Clipboard_2022-04-05-13-21-20.png)

Two problems: vanishing gradients and exploding gradients
![](@attachment/Clipboard_2022-04-05-13-21-57.png)

![](@attachment/Clipboard_2022-04-05-13-25-10.png)
- Inititialization: what distribution to use? ![](@attachment/Clipboard_2022-04-05-13-26-07.png)
- Batch normalization ![](@attachment/Clipboard_2022-04-05-13-31-21.png)
- Regularization ![](@attachment/Clipboard_2022-04-05-13-31-51.png)
- Parameter sharing ![](@attachment/Clipboard_2022-04-05-13-35-45.png)
- Convolution ![](@attachment/Clipboard_2022-04-05-13-39-50.png) ![](@attachment/Clipboard_2022-04-05-13-40-08.png)

### RNNs

![](@attachment/Clipboard_2022-04-05-13-52-12.png)

![](@attachment/Clipboard_2022-04-05-13-54-36.png)

-> gradients
![](@attachment/Clipboard_2022-04-05-13-55-46.png)

**LSTM**
![](@attachment/Clipboard_2022-04-05-13-58-30.png)

### NN properties

**Rich outputs**
![](@attachment/Clipboard_2022-04-05-14-00-28.png)

**Autoencoders**
![](@attachment/Clipboard_2022-04-05-14-00-42.png)
![](@attachment/Clipboard_2022-04-05-14-00-52.png)

- Adversarial example
- GAN
