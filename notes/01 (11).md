---
attachments: [Clipboard_2022-01-27-15-37-43.png, Clipboard_2022-01-27-15-41-06.png, Clipboard_2022-01-27-15-42-18.png, Clipboard_2022-01-27-17-25-08.png, Clipboard_2022-01-27-17-26-23.png, Clipboard_2022-01-27-17-27-24.png, Clipboard_2022-01-27-17-28-33.png, Clipboard_2022-01-27-17-28-51.png, Clipboard_2022-01-27-17-29-31.png, Clipboard_2022-01-27-17-31-24.png, Clipboard_2022-01-27-17-31-34.png, Clipboard_2022-01-27-17-34-20.png, Clipboard_2022-01-27-17-35-12.png, Clipboard_2022-01-27-17-37-49.png, Clipboard_2022-01-27-17-38-52.png, Clipboard_2022-01-27-17-39-33.png, Clipboard_2022-01-27-17-39-35.png, Clipboard_2022-01-27-17-40-10.png, Clipboard_2022-01-27-17-44-24.png, Clipboard_2022-01-27-17-44-42.png, Clipboard_2022-01-27-17-45-02.png, Clipboard_2022-01-27-17-45-33.png, Clipboard_2022-01-27-17-49-23.png, Clipboard_2022-01-27-17-49-45.png, Clipboard_2022-01-27-17-51-47.png, Clipboard_2022-01-27-17-52-42.png, Clipboard_2022-01-27-18-27-10.png, Clipboard_2022-01-27-18-27-17.png, Clipboard_2022-01-27-18-29-44.png]
tags: [Introduction to Deep Learning]
title: Lesson 3 - 26/01
created: '2022-01-27T13:29:16.904Z'
modified: '2022-01-27T16:39:06.833Z'
---

# Lesson 3 - 26/01

We were talking about kernel methods...so, what's the problem then?

![](@attachment/Clipboard_2022-01-27-15-37-43.png)

Long story short: overfitting because of high dimensionality. A (too) easy solution is CV and prune unnecessary features.
But then maybe...we don't need all these combinations -> can we let the model select the combinations that matter itself? Yes, this was a key feature and break-through of NNs.

![](@attachment/Clipboard_2022-01-27-15-41-06.png)

## NNs as a feature extractor + classifier

![](@attachment/Clipboard_2022-01-27-15-42-18.png)

## Feed-forward Neural Networks (MLP)

![](@attachment/Clipboard_2022-01-27-17-25-08.png)

Feed-forward means information is moving only forward through the layers

![](@attachment/Clipboard_2022-01-27-17-26-23.png)

But here we have three layers all of which do the same thing -> let's not write all the equations over and over for all of them, we have linear algebra!

![](@attachment/Clipboard_2022-01-27-17-27-24.png)

(the ws are the weights vectors for the layers)

### Second layer (hidden layer)

![](@attachment/Clipboard_2022-01-27-17-28-33.png)

![](@attachment/Clipboard_2022-01-27-17-28-51.png)

### How do we train?

Using SGD:
![](@attachment/Clipboard_2022-01-27-17-29-31.png)

But SGD is very inefficient -> use a Dynamic Programming algorithm, that is **backpropagation** (we'll see it soon)

## Activaction function

![](@attachment/Clipboard_2022-01-27-17-31-34.png)

Combinin linear models -> you get a new linear model, nothing more! (and so it doesn't discover anything new)

But if you add some non-linearity we expand the model capacity. For instance, if I add only one hidden non-linear layer I get ![](@attachment/Clipboard_2022-01-27-17-34-20.png)

i.e. you can approximate every non linear function.

However, this doesn't work (works only in theory).

![](@attachment/Clipboard_2022-01-27-17-35-12.png)

In theory, a single non-linear layer is enough BUT it doesn't work -> idea of Deep Neural Networks.

![](@attachment/Clipboard_2022-01-27-17-37-49.png)

### Let's go back to XOR

Using the ugly (reLU):
![](@attachment/Clipboard_2022-01-27-17-38-52.png)
And combine them:
![](@attachment/Clipboard_2022-01-27-17-39-35.png)

In matrices:
![](@attachment/Clipboard_2022-01-27-17-40-10.png)

## Running example: digit classification

![](@attachment/Clipboard_2022-01-27-17-44-24.png)
![](@attachment/Clipboard_2022-01-27-17-44-42.png)
![](@attachment/Clipboard_2022-01-27-17-45-02.png)
784 is the number of features, in this case; this is why we need fixed-size vectors as input and output
![](@attachment/Clipboard_2022-01-27-17-45-33.png)

Why do we need hidden layers? They discover some important features ![](@attachment/Clipboard_2022-01-27-17-49-23.png)
![](@attachment/Clipboard_2022-01-27-17-49-45.png)

Like in the first layer it learns some types of lines, in the second some others, etc...

![](@attachment/Clipboard_2022-01-27-17-51-47.png) -> too many parameters: need gradients for all: we need backpropagation
![](@attachment/Clipboard_2022-01-27-17-52-42.png)

Learn to perform a task:
![](@attachment/Clipboard_2022-01-27-18-27-10.png)

### Back-propagation

![](@attachment/Clipboard_2022-01-27-18-29-44.png)
