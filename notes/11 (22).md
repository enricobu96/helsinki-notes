---
attachments: [Clipboard_2021-11-24-12-43-26.png, Clipboard_2021-11-24-12-43-46.png, Clipboard_2021-11-24-12-46-24.png, Clipboard_2021-11-24-12-54-21.png, Clipboard_2021-11-24-12-54-31.png, Clipboard_2021-11-24-13-13-54.png, Clipboard_2021-11-24-13-16-11.png, Clipboard_2021-11-24-13-21-27.png, Clipboard_2021-11-24-13-22-05.png, Clipboard_2021-11-24-13-22-53.png, Clipboard_2021-11-24-13-25-26.png, Clipboard_2021-11-24-13-37-16.png, Clipboard_2021-11-24-13-39-02.png, Clipboard_2021-11-24-13-46-18.png, Clipboard_2021-11-24-13-46-37.png, Clipboard_2021-11-24-13-46-45.png, Clipboard_2021-11-24-13-51-56.png]
tags: [Introduction to Machine Learning]
title: Lesson 7 - 24/11
created: '2021-11-24T10:14:12.416Z'
modified: '2021-11-24T11:58:07.707Z'
---

# Lesson 7 - 24/11

## Similarity/dissimilarity

**Similarity**:
- Numerical measure of the degree to which two objects are alike
- High -> objects are alike
- Typically between 0 and 1

**Dissimilarity**:
- Inverse of similarity

## Nearest neighbor classifier

- Simple geometric model based on distances:
  - Store all the training data
  - To classify a new data point, find the closest one in the training set and use its class
- More generally, it finds k nearest points in the training set and uses the majority class
- Different notions of distance can be used, but usually euclidean

![](@attachment/Clipboard_2021-11-24-12-43-26.png)

### Bayes risk

![](@attachment/Clipboard_2021-11-24-12-43-46.png)

### k-NN theory

![](@attachment/Clipboard_2021-11-24-12-46-24.png)

![](@attachment/Clipboard_2021-11-24-12-54-21.png)

![](@attachment/Clipboard_2021-11-24-12-54-31.png)

### Pros and cons
- Simple
- Nice theorietical proterties
- Quite powerful
- Large time and memory complexity (e.g. you must store the training data)
- There is no model (sometimes the model parameters tell something of the data)

## Decision trees

Idea: ask a sequence of questions to infer the class.

![](@attachment/Clipboard_2021-11-24-13-13-54.png)

![](@attachment/Clipboard_2021-11-24-13-21-27.png)

### Regression trees

![](@attachment/Clipboard_2021-11-24-13-16-11.png)

### Structure

Structure of the tree:
- A single root node with no incoming edges, and zero or more outgoing edges (where edges go downwards)
- Internal nodes, each of which has exactly one incoming edge and two or more outgoing edges
- Leaf or terminal nodes, each of which has exactly one incoming edge and no outgoing edges
Node contents:
- Each terminal node is assigned a prediction (here, for simplicity: a definite class label)
- Each non-terminal node defines a test, with the outgoing edges representing the various possible results of the test (here, for simplicity: a test only involves a single feature)

### Learning a decision tree

Simple idea: recursively divide up the space into pieces which are as pure as possible:

![](@attachment/Clipboard_2021-11-24-13-22-05.png)

![](@attachment/Clipboard_2021-11-24-13-22-53.png)

And so on, recursively

### Decision tree vs linear classifier (e.g. LDA)

![](@attachment/Clipboard_2021-11-24-13-25-26.png)

### Again, regression trees

- The idea is to predict a continuous outcome Y by a representative (usually, average) outcome y withing each leaf
- Otherwise, the idea is the same

### Feature test conditions

![](@attachment/Clipboard_2021-11-24-13-37-16.png)

### Impurity measures

Key part is choosing test for a node is purity of a subset D of training data.

![](@attachment/Clipboard_2021-11-24-13-39-02.png)

### Selecting the best split

![](@attachment/Clipboard_2021-11-24-13-46-18.png)

#### Binary Gini

![](@attachment/Clipboard_2021-11-24-13-46-37.png)

![](@attachment/Clipboard_2021-11-24-13-46-45.png)

### Avoid overfitting

- The tree building process that splits until nodes have size nmax (e.g., 5), will overfit
- It would be possible to stop recursion once the subset D is ”pure enough”. This is known as pre-pruning but generally notrecommended 
- In contrast, post-pruning (called simply pruning in the textbook) is an additional step to simplify the tree after it has first been fully grown until |D|≤nmax

### Cost-complexity pruning

![](@attachment/Clipboard_2021-11-24-13-51-56.png)
