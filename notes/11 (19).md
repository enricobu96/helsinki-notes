---
attachments: [Clipboard_2021-11-22-10-17-48.png, Clipboard_2021-11-22-10-18-28.png, Clipboard_2021-11-22-10-19-04.png, Clipboard_2021-11-22-10-20-00.png, Clipboard_2021-11-22-10-21-35.png, Clipboard_2021-11-22-10-28-19.png, Clipboard_2021-11-22-10-28-57.png, Clipboard_2021-11-22-10-33-41.png, Clipboard_2021-11-22-10-34-07.png, Clipboard_2021-11-22-10-44-38.png, Clipboard_2021-11-22-10-51-22.png]
tags: [Trustworthy Machine Learning]
title: Lesson 7 - 22/11
created: '2021-11-22T08:14:31.235Z'
modified: '2021-11-22T09:47:19.359Z'
---

# Lesson 7 - 22/11

## Privacy-preserving data sharing

### Motivation

- Sharing data is desirable for various reasons -> open science, faster scientific information sharing...
- BUT data often contains sensitive information
![](@attachment/Clipboard_2021-11-22-10-17-48.png)
- Getting rid of this sensitive information is the only wat to true anonimity: ![](@attachment/Clipboard_2021-11-22-10-18-28.png)
But we don't want to get rid of too much information ![](@attachment/Clipboard_2021-11-22-10-19-04.png)

### Privacy-preserving ML

![](@attachment/Clipboard_2021-11-22-10-20-00.png)

This protects the privacy of the individuals from being exposed from the **outcome** of an analysis.

### Releasing sensitive data

We want to release the actual data, not only an outcome.
- Naive approach: input perturbation, e.g. Gaussian mechanism ![](@attachment/Clipboard_2021-11-22-10-21-35.png)
  Here $\Delta_{X}$ denotes the sensitivity, i.e. the largest change, in the input X.
- But...what if:
  - data has no bounded sensitivity?
  - X is binary? Does it make sense to add Gaussian noise?

### Parenthesis on generative models

- Goal: learn a model that generates samples similar to training data
- e.g. GANs, Variational Autoencoders (VAEs) ![](@attachment/Clipboard_2021-11-22-10-28-19.png)

#### GANs

![](@attachment/Clipboard_2021-11-22-10-28-57.png)

### Synthetic data: a solution for sharing sensitive data?

![](@attachment/Clipboard_2021-11-22-10-33-41.png)

Well, these models can produce real high-quality synthetic data; and data is synthetic, so that must be anonymous...right?
Well, no, the trained model can leak information.

![](@attachment/Clipboard_2021-11-22-10-34-07.png)

For example, GPT-2 leaks sensitive data (from training data).

Then...

### Privacy-preserving synthetic data

![](@attachment/Clipboard_2021-11-22-10-44-38.png)

- Learning the generative model while controlling the statistical disclosure with DP
- Generate synthetic data:
  - Same DP guarantees through post-processing immunity
  - Can be used in arbitrary downstream tasks

#### Examples of data release techniques

- Solution based on DP-SGD:
  - DP-GANs, SPRINT-gan
  - Mixture of VAEs
  - Data sharing through probabilistic models
- Other methods
  - PrivBayes
  - Adaptive projection method

### Privacy-preserving synthetic data through probabilistic modelling

![](@attachment/Clipboard_2021-11-22-10-51-22.png)
