---
attachments: [Clipboard_2021-11-15-10-31-21.png, Clipboard_2021-11-15-10-40-01.png, Clipboard_2021-11-15-10-40-12.png, Clipboard_2021-11-15-10-42-06.png, Clipboard_2021-11-15-10-48-16.png, Clipboard_2021-11-15-10-49-06.png, Clipboard_2021-11-15-10-55-13.png, Clipboard_2021-11-15-11-31-35.png, Clipboard_2021-11-15-11-50-53.png, Clipboard_2021-11-15-11-56-50.png]
tags: [Trustworthy Machine Learning]
title: Lesson 5 - 15/11
created: '2021-11-15T08:14:13.544Z'
modified: '2021-11-15T09:56:50.339Z'
---

# Lesson 5 - 15/11

## Federated learning

Strategy for improving communication efficinecy of learning on decentralised data. The basic idea is to accelerate distributed gradient-based optimisation by having each client run multiple steps before communicationg to average the updates.
Your device computes the gradient (in local) and then send it to the server.

![](@attachment/Clipboard_2021-11-15-10-31-21.png)

Many clients with local data (model training locally) -> single server (few steps of learning and send back) -> back to the client and to testers and rest of the world.

From a privacy perspective:
- The final model is the same
- Individual clients are sharing updates, there might be information on local data in any case

### Use cases

**Cross-device** federated learning:
- Each party has a data for a single user
- Must tolerate clients coming and going
- Clients are plentiful, some applications assume each client partecipates once
- E.g. mobile devices

![](@attachment/Clipboard_2021-11-15-10-40-12.png)

**Cross-silo** federated learning:
- Each party has data for many users
- Stable connectivity and repeated client participation are assumed
- E.g. hospitals unable to share the data (or other big organisations)

![](@attachment/Clipboard_2021-11-15-10-40-01.png)

### Privacy of vanilla federated learning

It's not a privacy technology! It can help improve privacy, but they're only (some) limits (possibly small).
In fact, updates shared in non-DP federated learning contain a lot of information that can be used for a reconstruction attack, e.g.

![](@attachment/Clipboard_2021-11-15-10-42-06.png)

## Privacy-preserving federated/distributed learning

### DP for FL

DP can protect against different threats:
- Centralised DP protects against outside adversaries. The output of DP mechanism is the final released model; often needs a trusted aggreagtor to perform the learning
- Local DP provides protection against parties of the protocol. The output of the DP mechanism are individual updates shared by a client; need much more noise, leading to lower utility
- As a rule of thumb, for $n$ users the error in central DP converges at rate $O(\frac{1}{2})$, local DP at $O(\frac{1}{\sqrt{n}})$

### Secure aggregation

Many protocols, including DP-SGD, only need to use the sum $\Sigma z_j$ of the individual client contributions $z_i$.
Secure aggregation uses secret sharing or cryptographic techniques to securely compute the sum.
![](@attachment/Clipboard_2021-11-15-10-48-16.png)

Some limitations tho:
- Fragile protocol: mistake in even single message completely destroys the result
- Secure only when servers do not collude

![](@attachment/Clipboard_2021-11-15-10-49-06.png)

### Secure shuffling

- Anonymity can provide significant amplification for LDP
- Main idea: employ shuffler to prevent linking high-$\epsilon$ LDP reports to users
- Provides central DP amplification of order $O(\frac{1}{\sqrt{n}})$ over LDP

![](@attachment/Clipboard_2021-11-15-10-55-13.png)

Pros:
- More robust to corrupted messages than secure aggregation

Cons:
- Secure only when aggregator and shuffler do not collude
- Secure implementation can be non-obvious
- Potentially weaker utility at equivalent DP guardanteethan central DP or secure aggregation (but still much stronger than LDP)

### Trust

- Trust assumptions make important differences between approaches
- Typical trus models:
  - Honest but curious parties follow the protocol, snoop what they can
  - Malicious parties may break the rules to get their way

Malicious parties are often handled via cryptographic zero-knowledge proofs to validate their contributions.

TMYK: privacy properties can sometimes depend on others following the protocol

## Privacy attacks against machine learning

### Reconstruction attack

Aka infer private data using a model and public data.
Risk exist whenever users can interact with a system making use of private information. E.g. recommender systems based on collaborative filtering, facebook and microtargeting.

![](@attachment/Clipboard_2021-11-15-11-31-35.png)

Mitigations and challenges:
- Adding limits on number of target users in microtargeting does not help if the attacker can create fake profiles
- Adding more randomness to processes might haelp, but is atodds with business goals

### Model inversion attack

Aka extracting training data from a published model.
E.g. dynamical models (complete known stems), deep learning classification (extract the average input for a class), model inversion for regression model.

### Membership inference attack

Aka identifying which samples were used to train the model and possibly learning the target class from input.
Given a set of 2n samples (x,y) -> predict which n were used to train the model.
Easy without DP, as flexible models such as DL behave very differently on training samples vs other samples.
Often unrealistic, but used as a simple prototype of an attack by a strong adversary.

![](@attachment/Clipboard_2021-11-15-11-50-53.png)

### Auditing DP-SGD

Basic idea:
1. Construct a model poisoning attack to inject a backdoor into the model (e.g. images with a specific pixel pattern will be classified in a specific way)
2. Evaluate the success rate of incorporating the backdoor in T trials, translate into an estimate of $\epsilon$

### Adversary instantiation

![](@attachment/Clipboard_2021-11-15-11-56-50.png)









































