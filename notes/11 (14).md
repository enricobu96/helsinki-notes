---
attachments: [Clipboard_2021-11-15-10-31-21.png, Clipboard_2021-11-15-10-40-01.png, Clipboard_2021-11-15-10-40-12.png, Clipboard_2021-11-15-10-42-06.png, Clipboard_2021-11-15-10-48-16.png, Clipboard_2021-11-15-10-49-06.png, Clipboard_2021-11-15-10-55-13.png]
tags: [Trustworthy Machine Learning]
title: Lesson 5 - 15/11
created: '2021-11-15T08:14:13.544Z'
modified: '2021-11-15T08:59:43.770Z'
---

# Lesson 5 - 15/11

## Federated learning

Strategy for improving communication efficinecy of learning on decentralised data. The basic idea is to accelerate distributed gradient-based optimisation by having each client run multiple steps before communicationg to average the updates.
Your device computes the gradient (in local) and then send it to the server.

![](@attachment/Clipboard_2021-11-15-10-31-21.png)

Many clients with local data (model training locally) -> single server (few steps of learning and send back) -> back to the client and to testers and rest of the world.

From a privacy perspective:
- The final model is the same
- Individual clients are sharing updates, there might be information on local data in any case

### Use cases

**Cross-device** federated learning:
- Each party has a data for a single user
- Must tolerate clients coming and going
- Clients are plentiful, some applications assume each client partecipates once
- E.g. mobile devices

![](@attachment/Clipboard_2021-11-15-10-40-12.png)

**Cross-silo** federated learning:
- Each party has data for many users
- Stable connectivity and repeated client participation are assumed
- E.g. hospitals unable to share the data (or other big organisations)

![](@attachment/Clipboard_2021-11-15-10-40-01.png)

### Privacy of vanilla federated learning

It's not a privacy technology! It can help improve privacy, but they're only (some) limits (possibly small).
In fact, updates shared in non-DP federated learning contain a lot of information that can be used for a reconstruction attack, e.g.

![](@attachment/Clipboard_2021-11-15-10-42-06.png)

## Privacy-preserving federated/distributed learning

### DP for FL

DP can protect against different threats:
- Centralised DP protects against outside adversaries. The output of DP mechanism is the final released model; often needs a trusted aggreagtor to perform the learning
- Local DP provides protection against parties of the protocol. The output of the DP mechanism are individual updates shared by a client; need much more noise, leading to lower utility
- As a rule of thumb, for $n$ users the error in central DP converges at rate $O(\frac{1}{2})$, local DP at $O(\frac{1}{\sqrt{n}})$

### Secure aggregation

Many protocols, including DP-SGD, only need to use the sum $\Sigma z_j$ of the individual client contributions $z_i$.
Secure aggregation uses secret sharing or cryptographic techniques to securely compute the sum.
![](@attachment/Clipboard_2021-11-15-10-48-16.png)

Some limitations tho:
- Fragile protocol: mistake in even single message completely destroys the result
- Secure only when servers do not collude

![](@attachment/Clipboard_2021-11-15-10-49-06.png)

### Secure shuffling

- Anonymity can provide significant amplification for LDP
- Main idea: employ shuffler to prevent linking high-$\epsilon$ LDP reports to users
- Provides central DP amplification of order $O(\frac{1}{\sqrt{n}})$ over LDP

![](@attachment/Clipboard_2021-11-15-10-55-13.png)

Pros:
- More robust to corrupted messages than secure aggregation

Cons:
- Secure only when aggregator and shuffler do not collude
- Secure implementation can be non-obvious
- Potentially weaker utility at equivalent DP guardanteethan central DP or secure aggregation (but still much stronger than LDP)










































