---
attachments: [Clipboard_2021-11-11-10-36-44.png, Clipboard_2021-11-11-10-42-10.png, Clipboard_2021-11-11-10-45-04.png, Clipboard_2021-11-11-10-52-13.png, Clipboard_2021-11-13-11-25-58.png, Clipboard_2021-11-13-11-26-08.png, Clipboard_2021-11-13-11-27-57.png, Clipboard_2021-11-13-12-17-33.png, Clipboard_2021-11-13-12-23-42.png]
tags: [Trustworthy Machine Learning]
title: Lesson 4 - 11/11
created: '2021-11-11T08:14:37.199Z'
modified: '2021-11-13T10:23:42.765Z'
---

# Lesson 4 - 11/11

## DP data analytics

### Counting queries and report noisy max

Assume we have a set of $m$ queries that count the number of $n$ individuals satisfying each of $m$ properties; the sensitivity of such counting query is obv 1.
Assume we wish to find the most common out of the $m$ properties; we can app Laplace(0,1/$\epsilon$) noise to each count and pick the largest.
A naive analysis would apply m-fold composition and postprocessing to pick the largest, implying the algorithm is m$\epsilon$-DP; more careful analysis shows that only releasing the highest index and noisy count is $\epsilon$-DP (at least for add/remove neighbours)

### Exponential mechanism

![](@attachment/Clipboard_2021-11-11-10-42-10.png)

### Sparse Vector Technique

Assume we have a sequence of queries $q_i(D)$ and we want to pick c first that are above a public treshold T.
SVT allows returning a noisy vector of binary indicators for queries above treshold with cost proportional to c BUT independent on the number of the queries.
![](@attachment/Clipboard_2021-11-11-10-45-04.png)

SVT also works in adaptive setting where all queries are not fixed in advance but can depend on the outcomes of previous queries. In the non-adaotive setting though it's recommended to use exponential mechanism iteratively instead:
1. Pick top element
2. Remove chosen element and repeat until c elements have been chosen
3. Total privacy cost evaluated with optimal composition

![](@attachment/Clipboard_2021-11-11-10-52-13.png)

### Further DP algorithm design techniques

- Subsample and aggregate: uses a divide-and-conquer approach to increase accuracy in problems where solutions in subsets are stable
- Propose-test-release: helps in cases where high sensitivity is unusual: propose a low-sensitivity solution, test that sensitivity is indeed slow; release only if it is
- Smooth sensitivity: methods allow moving from global to local sensitivity in a way that satisfies DP

## DP Machine Learning

Key principles and mechanisms:
- Composition and post-processing invariance
- Laplace and Gaussian mechanisms
- Exponential mechanism, report-noisy-max and spare vector technique
- Privacy amplification from subsampling

Principles for DP learning algorithms:
1. Input perturbation
2. Objective perturbation
3. Output perturbation
4. Noisy gradient-based optimisation
5. Private aggregation

### Input perturbation

Add noise to input to make it DP, treat learning algorithm as post-processing. e.g. local DP, generalized linear models based on sufficient statistics, PCA.
- Pros: easy to analyse, sufficient statistics perturbation is consistent (guaranteed to converge to non-private result when the number of samples increases)
- Cons: low utility in models not amenable to sufficient statistics

### Objective perturbation

Add a random term to optimisation objective to guarantee DP. e.g. mostly models based on convex optimisation, like funcional mechanism for linear and logistic regression.
- Pros: potentially higher utility than input perturbation; can reuse existing learning algorithms to large extent
- Cons: only applicable to specific models, require model-specific analysis

### Output perturbation

Add noise to the output of a learning algorithm. e.g. mainly theoretical model, with few practical applications.
- Pros: if done well, could yield stronger privacy analysis than other alternatives
- Cons: evaluating the sensitivity can be very challenging for practical algorithms

### Noisy gradient-based optimisation (DP-SGD)

DP stochastic gradient descent, evaluating privacy loss via composition. e.g. DP deep learning or any optimisation-based learning.
- Pros: very general algorithm, no problem-specific analysis needed, surprisingly high utility in many problems
- Cons: probably overstimates the privacy loss if only final model is published; other significant loss in utility over non-private models, especially for very large models

#### SGD vs DP-SGD

SGD:

![](@attachment/Clipboard_2021-11-13-11-25-58.png)

DP-SGD:

![](@attachment/Clipboard_2021-11-13-11-26-08.png)

#### Privacy of DP-SGD: Privacy accounting

DP-SGD includes two elements of randomness important for the privacy:
- Minibatch sampling
- Adding noise to gradients (Gaussian mechanism)

These two steps can be repeated 1000s of times. Computing the cumulative privacy loss for the composition is commonly called privacy accounting; moments accountant and RDP have provided significant advances, accounting based on the privacy loss distribution provides a numerical method allowing arbitrary accounting accuracy.

![](@attachment/Clipboard_2021-11-13-12-17-33.png)

### Private aggregation

Train a number of models on different disjoint subsets of data, privately aggregate their predictions. e.g. DP classification and GAN training.
- Pros: Reported higher utility compare to DP-SGD
- Cons: Relies on additional public data

![](@attachment/Clipboard_2021-11-13-11-27-57.png)

### Rule of thumb for DP machine learning and deep learning

![](@attachment/Clipboard_2021-11-13-12-23-42.png)

### DP-SGD implementation and operation

- Minibatch sampling needs to be random to satisfy privacy. Processing samples in order can leak privacy
- Gradient clipping requires access to and manipulation of per-example gradients that are not needed in usual SGD. Early deep learning frameworks do not support this easily
- For production use, a cryptographically secure random number generator should be used

DP-SGD requires additional hyperparameters: clipping threshold C and noise variance $\sigma^2$. Together with number of iterations and minibatch subsampling rate q, these define the cumulative privacy cost.

#### Hyperparameter tuning

- If hyperparameters are computed using sensitive data they can leak information
- In practice, leakage from hyperparamenters is often ignored


































