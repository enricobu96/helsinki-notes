---
attachments: [Clipboard_2022-02-01-10-58-03.png, Clipboard_2022-02-01-10-59-39.png, Clipboard_2022-02-01-11-00-45.png, Clipboard_2022-02-01-11-27-38.png, Clipboard_2022-02-01-11-29-09.png, Clipboard_2022-02-01-11-30-06.png, Clipboard_2022-02-01-11-30-31.png, Clipboard_2022-02-01-11-32-05.png, Clipboard_2022-02-01-11-32-26.png, Clipboard_2022-02-01-11-33-18.png, Clipboard_2022-02-01-11-34-32.png, Clipboard_2022-02-01-11-37-30.png, Clipboard_2022-02-01-11-39-45.png, Clipboard_2022-02-01-11-40-42.png, Clipboard_2022-02-01-12-01-19.png, Clipboard_2022-02-01-12-02-02.png, Clipboard_2022-02-01-12-03-09.png, Clipboard_2022-02-01-12-09-04.png, Clipboard_2022-02-01-12-09-31.png, Clipboard_2022-02-01-12-10-33.png, Clipboard_2022-02-01-12-10-52.png, Clipboard_2022-02-01-12-13-05.png, Clipboard_2022-02-01-12-14-07.png, Clipboard_2022-02-01-12-15-26.png, Clipboard_2022-02-01-12-16-57.png, Clipboard_2022-02-01-12-17-31.png]
tags: [Information Retrieval]
title: Lesson 4 - 01/02
created: '2022-02-01T08:51:39.781Z'
modified: '2022-02-01T10:17:46.428Z'
---

# Lesson 4 - 01/02

No prof today, this is the video lecture he sent.

## Ranked retrieval

Thus far, our queries have all been boolean, but this is not good for the majority of users.

-> Boolean queires often result in either too few or too many results; it take a lot of shill to come up with a query that retrieves the exact result we want -> **Ranked retrieval models**: the system returns an ordering over the top documents in the collection wrt a query

- **Free text queries**: the user's query is just 1+ words in a human language. That's different, but usually both are used together.

**Scoring**: basis of ranked retrieval. How can we rank-order the documents in the collection wrt a query?: assign a score to each document. So now we need a way to assigning a score to a query/document pair.

![](@attachment/Clipboard_2022-02-01-10-58-03.png)

### Different alternatives

#### Jaccard coefficient

![](@attachment/Clipboard_2022-02-01-10-59-39.png)
e.g. ![](@attachment/Clipboard_2022-02-01-11-00-45.png)
  
- Issues:
  - Doesn't consider term frequency (rare terms are usually more informative than frequent terms)
  - We need a more sophisticated way of normalizing for length

#### Term frequency weighing

![](@attachment/Clipboard_2022-02-01-11-27-38.png) -> BOW model, we are not considering the order but the number of occurencies. It has some issues, in a sense this is a step back (A AND B is now B AND A)

![](@attachment/Clipboard_2022-02-01-11-29-09.png)

![](@attachment/Clipboard_2022-02-01-11-30-06.png)

![](@attachment/Clipboard_2022-02-01-11-30-31.png)

(the score is still 0 if no occurencies in the document)

#### Inverse document frequency

Rare terms:
![](@attachment/Clipboard_2022-02-01-11-32-05.png)

Frequent terms:
![](@attachment/Clipboard_2022-02-01-11-32-26.png)

**idf weight**: ![](@attachment/Clipboard_2022-02-01-11-33-18.png)

TMYK: the log base is not important

e.g.
![](@attachment/Clipboard_2022-02-01-11-34-32.png)

Idf has no effect on one term queries! Only 2+ terms

Collection vs document frequency: ![](@attachment/Clipboard_2022-02-01-11-37-30.png)

#### TD-IDF

![](@attachment/Clipboard_2022-02-01-11-39-45.png)

Increases with the number of occurrences within a document AND with the rarity of the term in dhe document

Final ranking of documents for a query: ![](@attachment/Clipboard_2022-02-01-11-40-42.png)

### Vector space model

We saw that we can represent documents as vectors: ![](@attachment/Clipboard_2022-02-01-12-01-19.png)

And queries as vectors: ![](@attachment/Clipboard_2022-02-01-12-02-02.png)

#### Vector space proximity

Euclidean distance is bad, because it's large for distant vectors

![](@attachment/Clipboard_2022-02-01-12-03-09.png)

Idea: we can use angle instead of distance -> we can use cosines

Starting point -> **length normalization**: ![](@attachment/Clipboard_2022-02-01-12-09-04.png)

**cosine(query, document)**: ![](@attachment/Clipboard_2022-02-01-12-09-31.png)

![](@attachment/Clipboard_2022-02-01-12-10-33.png)

Illustrated: ![](@attachment/Clipboard_2022-02-01-12-10-52.png)

### And now...calculate TF-IDF cosine scores

TF-IDF weighing has many variants: ![](@attachment/Clipboard_2022-02-01-12-13-05.png)

![](@attachment/Clipboard_2022-02-01-12-14-07.png)

Example:
![](@attachment/Clipboard_2022-02-01-12-15-26.png)

![](@attachment/Clipboard_2022-02-01-12-16-57.png)

**Algorithm**
![](@attachment/Clipboard_2022-02-01-12-17-31.png)
