---
attachments: [Clipboard_2021-10-07-12-27-46.png, Clipboard_2021-10-07-12-32-08.png, Clipboard_2021-10-07-12-43-49.png, Clipboard_2021-10-07-12-46-50.png, Clipboard_2021-10-07-12-49-26.png, Clipboard_2021-10-07-12-57-02.png, Clipboard_2021-10-07-12-59-39.png, Clipboard_2021-10-07-13-07-56.png, Clipboard_2021-10-07-13-12-44.png]
tags: [Big Data Platforms]
title: Lesson 8 - 07/10
created: '2021-10-07T09:16:45.946Z'
modified: '2021-10-07T10:12:46.021Z'
---

# Lesson 8 - 07/10

## BigTable

- High scalable consistend and partition tolerant datastore
- Implemented on top of the Google Filesystem (GFS)
- GFS provides data persintence by replication but only supports sequential writes
- Does not do random writes, only sequential, by design! Recall: sequential writes are much faster than random ones
- Writes are optimized over reads
- The random reads that miss the DRAM cache used are slower than in traditional RDBMS
- The design also minimizes the number of bytes written by using efficient logging and compression of data. Also good for SSDs then!

### BigTable Data Model

- Sparse, distributed, persistent multi-dimensional sorted map
- BigTable stores data as strings, which can be accessed by three coordinates:
  - `row`: an arbitrary string row key (10-100 bytes typical, max 64KB)
  - `column`: columns key, consisting of a column family and column qualifier (name) with syntax `family:qualifier`
  - `timestamp`: 64-bit integer that can be used to store a timestamp
  Then we have a map with three coordinates: `(row:string, column:string, time:int64) -> string`

### BigTable Data Layout

- Data is stored sorted by the row key, and the data is automatically sharded (split to different servers) in large blocks (~1GB chunks) sorted by the row key, allowing for efficient scans in increasing alphabetical row key order
- Columns are grouped in "Column families", and all columns in a single column family are stored together in compressed form on disk
- Some queries might not access columns in all column families, and this allows data in these column families to be easily skipped for such queries
- The timestamp field, also, allows application to store several copies of the same data together

### Example: Web Table

![](@attachment/Clipboard_2021-10-07-12-27-46.png)

- Notice how row key is the reverse of the Web page URL to group pages from the same site together
- The “contents” column family stores the Web site contents at the time pointed by the timestamp
- The “anchor” column family stores the links pointing to the Web page in question
- Now it is easy to skip for example reading the Web page contents if we are only interested in links between Web pages

### BigTable Atomicity guardantees

- Each row in BigTable can have a different number of columns
- Some rows might have thousands or even more columns
- !!! Updates to a single row are atomic, irregardeless of the number of column families or columns being updated
- No transactions for updating multiple rows atomically are available

### Tablets

- Rows with consecutive row keys are grouped togheter to "Tablets", which are the unit of sitribution and load balancing. Tablets are by default split when they grow over 1Gb in size
- A immutable datastructure (SSTable) is used to store BigTable data files

#### SSTables

- Persistent, ordered immutable map from keys to values, where both keys and values are arbitrary byte strings
- Inside SSTable are compressed 64KB data blocks combined with an index to find a row key inside these data blocks. SSTable can be also directly generated by MapReduce jobs for bulk loading of data in parallel
- Also optional bloom filter can be added to speed access to non-existent row/column pairs
- Immutability of SSTables enables both compression and Bloom filters!
- Doing compression in traditional database design is difficult due to random writes requiring recompression of data. Also bloom filters are hard to implement when data items are deleted

### Chubby

- BigTable relies on a fault tolerant distributed coordination system (Chubby)
- It's a piece of infrastructure other Google services use to implement common tasks such as leader (master) election and for storing the set of servers a BigTable instance consists of for clients
- Used for a lot of things; if Chubby goes down, Google goes down

### BigTable Tablet Lookup

- The tablet lookup in BigTable starts by asking Chubby for the location of the root Tablet
- An additional layer of indexes pointed by the root Tablet will point to the (servers holding the) user data Tablets

![](@attachment/Clipboard_2021-10-07-12-43-49.png)

### BigTable write path

- When BigTable tablet server receives a write, it first commits it ot a commit log on GFS
- After the commit, write is added to memtable
- Once memtable fills up, all written data is sorted in memory, and stored into a SSTable on GFS, and the commit log file can be removed

![](@attachment/Clipboard_2021-10-07-12-46-50.png)

### BigTable read path

- When BigTable Tablet server receives a read, it first checks the memtable if the data exists there
- If not, then the SSTables are scanned one at a time from newest to oldest to find the SSTable holding the latest version of the data item
- Bloom filters can help avoid scanning some SSTables

![](@attachment/Clipboard_2021-10-07-12-49-26.png)

### Compactions

- If data is continuously written, the number of SSTables grows, and read performance will degrade over time
- To keep the number of SSTables under control, a “compaction” is run on the background, which merges several SSTables into a single SSTable using a merge sort like algorithm
- An item is marked as deleted by having special “tombstone” records, that mark that the item has been
deleted
- In a “major compaction” all SSTables are merged into a one file, only at which point tombstone records can be removed by removing the relevant data items

#### Compaction Schedule

![](@attachment/Clipboard_2021-10-07-12-57-02.png)

### BigTable Performance numbers

![](@attachment/Clipboard_2021-10-07-12-59-39.png)

## Apache HBase

- Apache HBase is an open source Google BigTable clone
- It very closely follows the BigTable design but has the following differences
- Instead of GFS, HBase runs on top of HDFS
- Instead of Chubby, HBase uses Apache Zookeeper
- SSTable of BigTable are called in HBase HFile (and HFile V2)
- HBase only partially supports fully memory mapped data

### Why use HBase

- Elasticity: Ability to add new nodes as storage as needed
- High write throughput
- Consistency guarantees given within a single datacenter
- Comparable (but not better) read performance compared to RDBMS
- High Availability and disaster recovery features
- Fault isolation of e.g., hard disk failures (a hard disk fails every 30 mins)
- Atomic read-modify-write primitives (e.g., for atomic counters)
- Efficient range scans: e.g., give last 100 messages of the user

### Typical 100 node HBase Deployment

![](@attachment/Clipboard_2021-10-07-13-07-56.png)

### Challenges for HBase solution

- Each user is served by a single cluster but Facebook has quite a few of 100 node HBase clusters
- HDFS was not designed for realtime but batch processing - large timeouts etc.
- HDFS NameNode single point of failure was addressed by “AvatarNode” - a new hot backup NameNode solution

![](@attachment/Clipboard_2021-10-07-13-12-44.png)












