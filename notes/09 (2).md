---
attachments: [Clipboard_2021-09-07-10-17-12.png, Clipboard_2021-09-07-10-24-33.png, Clipboard_2021-09-07-10-32-16.png, Clipboard_2021-09-07-10-33-40.png, Clipboard_2021-09-07-10-38-20.png, Clipboard_2021-09-07-10-46-29.png, Clipboard_2021-09-07-10-49-36.png, Clipboard_2021-09-07-10-52-45.png, Clipboard_2021-09-07-10-55-40.png, Clipboard_2021-09-07-10-59-09.png, Clipboard_2021-09-07-11-00-15.png, Clipboard_2021-09-07-11-03-51.png, Clipboard_2021-09-07-11-07-25.png, Clipboard_2021-09-07-11-10-08.png, Clipboard_2021-09-07-11-13-40.png, Clipboard_2021-09-07-11-14-06.png, Clipboard_2021-09-07-11-15-10.png, Clipboard_2021-09-07-11-16-06.png, Clipboard_2021-09-07-11-18-46.png, Clipboard_2021-09-07-11-24-38.png, Clipboard_2021-09-07-11-26-19.png, Clipboard_2021-09-07-11-27-42.png, Clipboard_2021-09-08-09-59-20.png, Clipboard_2021-09-08-11-43-34.png]
tags: [Big Data Platforms]
title: Lesson 1 - 07/09
created: '2021-09-06T06:45:31.365Z'
modified: '2021-09-08T08:54:10.774Z'
---

# Lesson 1 - 07/09

## Logistics

![](@attachment/Clipboard_2021-09-07-10-17-12.png)

No exam, only quizzes and Spark programming Home Excercises

![](@attachment/Clipboard_2021-09-07-10-24-33.png)

Every week exercise, to be completed in 2 weeks (but preferable to complete them in a week)

## Introduction - Cloud computing

Cloud computing is a model for enabling convienient, on-demand network access to a shared pool of configurable computing resources that can be rapidly provisioned and released with minimal management effort or service provider interaction. The cloud model promotes availability and is composed of five essential characteristics, three service models and four deployment models.

### Essential characteristics

- On-demand self-service
- Broad network access
- Resource pooling
- Rapid elasticity
- Measured service

### Service models

- Cloud software as a service (SaaS)
- Cloud platform as a service (PaaS)
- Cloud infrastructure as a service (Iaas)

We'll look more at IaaS

### Deployment models

- Private cloud
- Community cloud
- Public cloud
- Hybrid cloud

### Business drivers of Clouds
![](@attachment/Clipboard_2021-09-07-10-33-40.png)

### Technologies

![](@attachment/Clipboard_2021-09-07-10-38-20.png)

We'll cover GFS, HDFS (filesystems for big data); Apache Hadoop and Spark; Dynamo, Cassandra, ...

## Moore's law

Usual things about Moore's law...the end of free lunch. And then:
- The clock speeds of microprocessors are not going to improve much in the future -> only moderate single threaded performance gains
- The number of transistors in a microprocessor is still growing fast, in the sense that we have multi-cores CPUs (more and more cores) and that's why it's increasing

=> programming models need to change to efficently exploit all the available concurrency -> scalability to high number of cores/processors needs to be a major focus.

### Dark silicon problem

The "dark silicon" term alone stands for the amount of "unused silicon", in the sense of unused transistors, in a microprocessor. The dark silicon problem consists in the increasely higher amount of transistors that can't be powered-on on a microprocessor due to its thermal design power (TDP). In other words, the problem lies in the fact that powering on too much transistors at a time would increase the heat so much that the microprocessor can't be used, since no cooling system is capable to dissipate that heat.

In order to better understand this topic we have to introduce two important laws: Moore's law and Dennard scaling.
Moore's law, formulated by Gordon Moore in 1965, states that the number of transistors in the integrated circuits doubles every 18-24 months, and this is due to the transistors becoming smaller and smaller. Even though this law was formulated more than 50 years ago, it has proved correct: one of the first commercial CPUs, Intel 4004 (1971), had 2250 transistors; Intel 8088 (1979) had 29000 transistors; a modern CPU (2021) has a number of transistors in the order of the billions.

Dennard scaling is a scaling law which states that, as transistors get smaller, their power density (i.e. the amount of power per unit volume) remains constant; this means that, even if the amount of transistors on an integrated circuits increases, the power consumption should stay the same.

This last law might seem contraddicting with the dark silicon problem, and indeed it is. The real problem is that the Dennard scaling is no longer valid since around 2006: according to [1], at 8nm litography the amount of unused transistors may reach up to 50-80% (depending of course on the architecture, cooling and type of workload).

The dark silicon problem introduces many challenges for microprocessors manufacturers, because more energy efficient computing is needed and therefore solutions must be found. For instance, GPGPU computing (i.e. general-purpose graphics processing unit, a GPU programmed for purposes beyond graphics processing) is improving the energy efficiency of training deep neural networks, which is a very intensive type of workload.

Beyond that, there are also other solutions that can be adopted to bypass the problem: cache memories are an example, since they improve the overall integrated circuits performance while consuming power only in special situations (e.g. heavy workloads). 
But, most importantly, parallelization might be the safest way to mitigate the problem: several cores running at a low speed performs better that a single core running at high speed; the challenge here is then trying to parallelize most of the jobs (i.e. programs) that a microprocessor can run.

[1] H. Esmaeilzadeh, E. Blem, R. S. Amant, K. Sankaralingam and D. Burger, "Dark silicon and the end of multicore scaling," 2011 38th Annual International Symposium on Computer Architecture (ISCA), 2011, pp. 365-376.

## Amdahl's law

One of the basics of parallelization (there's no free lunch).
In order to keep the ever increasing number of cores busy, the applications must be extremely parallel to enjoy the benefits of increasing transistors counts...BUT Amdahl's law still applies:

![](@attachment/Clipboard_2021-09-08-11-43-34.png)

![](@attachment/Clipboard_2021-09-07-10-55-40.png)

## Scaling (up vs out aka vertical vs horizontal)

### Scaling up
When the need for parallelism arises, a single powerful computer is added with more CPU cores, memory, hard disks.

### Scaling down (Big Data Platforms tries to exploit this)
When the need for parallelism arises, the task is divided between a large number of less powerful machines with relatively slow CPUs, moderate memroy amounts, moderate hard disk counts.


### Pros and cons
- Up is more expensive than out (e.g. an high-end Xeon is more expensive than two i3)
- Out is more challenging for fault tolerance -> solution: software fault tolerance (e.g. Apache Spark does it)
- Out is more challenging for software development: failures, latencies... -> solution: scalable big data platforms

## Warehouse-scale Computing (WSC)

Nice book to read (it's free I guess)

![](@attachment/Clipboard_2021-09-07-11-03-51.png)

![](@attachment/Clipboard_2021-09-07-11-07-25.png)

That's a rack, every line is a computer (like 150 servers in the picture)
The chimneys are cooling systems obv (water cooling system actually)

### Classical datacenter

![](@attachment/Clipboard_2021-09-07-11-10-08.png)

#### Classical cooling system

![](@attachment/Clipboard_2021-09-07-11-13-40.png)

#### Power usage

![](@attachment/Clipboard_2021-09-07-11-14-06.png)

### Storage hierarchy of WSC

![](@attachment/Clipboard_2021-09-07-11-15-10.png)

#### Latency, bandwidth, capacity

![](@attachment/Clipboard_2021-09-07-11-16-06.png)

Rule of thumb numbers (latencies):
![](@attachment/Clipboard_2021-09-07-11-18-46.png)

## Big data

![](@attachment/Clipboard_2021-09-07-11-24-38.png)

For example...

![](@attachment/Clipboard_2021-09-07-11-26-19.png)

...and than: Big Data Platforms!
- For large enough datasets we must use hard disk parallelism to do processing in reasonable time
- MapReduce is a programming paradigm developed at Google for scalable batch processing (initially developed to compute the Web search indexes for Google)
- Apache Spark system is a new framework which is currently replacing MapReduce
- A high performance fault tolerant distributed filesystem is needed to support Spark/MapReduce (e.g. Google GFS, Hadoop Distributed File System (HDFS))
- PySpark = python interface (we will use it)

