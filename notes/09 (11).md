---
attachments: [Clipboard_2021-09-21-16-26-04.png, Clipboard_2021-09-21-16-26-27.png, Clipboard_2021-09-21-16-40-09.png, Clipboard_2021-09-21-16-43-16.png, Clipboard_2021-09-21-16-43-45.png, Clipboard_2021-09-21-16-49-40.png, Clipboard_2021-09-21-16-53-30.png, Clipboard_2021-09-21-16-54-41.png, Clipboard_2021-09-21-16-58-51.png, Clipboard_2021-09-21-17-01-37.png, Clipboard_2021-09-21-17-25-19.png, Clipboard_2021-09-21-17-26-58.png, Clipboard_2021-09-21-17-54-34.png, Clipboard_2021-09-21-17-58-39.png]
tags: [Introduction to Data Science]
title: Lesson 6 - 21/09
created: '2021-09-21T13:19:16.248Z'
modified: '2021-09-21T15:01:15.367Z'
---

# Lesson 6 - 21/09

## Machine Learning - Methods

Regression vs Classification: continuous vs discrete.

### Linear Regression

- It's a **regression** model: predict a continuous outcome `y in R`
- Input is a vector `x in Rp`
- Model: ![](@attachment/Clipboard_2021-09-21-16-26-04.png)
- In the basic case, we assume ![](@attachment/Clipboard_2021-09-21-16-26-27.png)
  - **Least squares solution** is also maximum likelihood
  - A more heavy-tailed error distribution leads to robust (to outliers) regression
-A good choice for the default method
- Well documented assumptions and tests for, e.g., heteroscedasticy (noise different in different regions)
- Powerful tool when combined with data transformations like:
  - Non-linear basis functions (splines, wavelets,...)
  - Log transform for log-normal data

#### Linear Regression on categorical data: one-hot encoding

With categorical variables, map to **dummy variables**:
![](@attachment/Clipboard_2021-09-21-16-40-09.png)

**Explainability**
![](@attachment/Clipboard_2021-09-21-16-43-16.png)

**Interactions**
![](@attachment/Clipboard_2021-09-21-16-43-45.png)

### Logistic regression

- Actually **not totally** a regression: predicts binary (0/1) outcomes **but** output is a probability of outcome `Y=1` given `x`
- Model: ![](@attachment/Clipboard_2021-09-21-16-49-40.png)

### Multi-class classification

- Multinomial: classes mutually exclusive: instance is either a, b or c. e.g. NB, kNN, DT, logistic
  ![](@attachment/Clipboard_2021-09-21-16-53-30.png)
- Binary classification: one-vs-rest (`{a}` vs `{not a}`, etc...); classes may overlap and instance can be in both a and b or none of the classes. e.g. SVM, logistic, perceptron
  ![](@attachment/Clipboard_2021-09-21-16-54-41.png)

### Multinomial Logistic Regression
_The poor man's neural network_

![](@attachment/Clipboard_2021-09-21-16-58-51.png)

### Decision Trees

- Usually for classification problems, but regression trees exist too
- Ask a sequence of questions to infer the class
  ![](@attachment/Clipboard_2021-09-21-17-01-37.png)
- Structure:
  - A root node
  - Internal nodes with 2+ children
  - Leaf (terminal) nodes with no children
- Node:
  - Non-terminal nodes make a test and the children represent the possible results of the test
  - Typicall tests only involve a single feature at a time
  - Each leaf node is assigned a prediction
  - Prediction can either be deterministic or non-deterministic

#### Example

![](@attachment/Clipboard_2021-09-21-17-25-19.png)
![](@attachment/Clipboard_2021-09-21-17-26-58.png)
And so on

#### Pros and Cons

|Pros|Cons|
|----|----|
|Easy to intepret|Choosing tree depth (overfitting)
|Simple to implement|Accuracy usually not the best|
|Fast||

Random forests are among the best out-of-the-box classifiers:
- Consistently good results without (much) tuning
- Lose interpretability points (too many trees in RF)

## Unsupervised learning

All the above have boon examples of supervised machine learning; trying to predict a specific outputs given an input.
-> Unsupervised ML involves no output:
- Learning data distributions (e.g. density estimation)
- Dimension reduction (e.g. principal component analysis)
- Clusting
- ...

### Dimensionality reduction

- When the dimension of the data `p` is large, it is often hard to visualize and process the data
- Classification and regression models easily overfit
- However, in many cases, the data can be approximately summarized by a lower-dimensional representation, e.g.:
  - Questionnaire data can often be reduced to a few dimensions
  - Psychological scalse such as Myers-Briggs, Keirsey...
- Visualization requires 1D, 2D or 3D representations

#### Principal Component Analysis (PCA)

- A common dimensionality reduction technique
- Basic idea: project the data onto a lower dimensional subspace so that as much variance as possible is retained
![](@attachment/Clipboard_2021-09-21-17-54-34.png)

#### Nonlinear dimension reduction

- PCA can only extract linear subspace structures
- Nonlinear variants are often called "mainfold learning"
  - A mainfold is sort of like a nonlinear version of a subspace
  - Think: generalizations of a curved plane (2D embedded in 3D)
- Good example: t-SNE (t-distributed stochastic neighbor embedding)

![](@attachment/Clipboard_2021-09-21-17-58-39.png)
