---
attachments: [Clipboard_2021-09-16-12-20-28.png, Clipboard_2021-09-16-12-24-18.png, Clipboard_2021-09-16-12-25-07.png, Clipboard_2021-09-16-12-27-20.png, Clipboard_2021-09-16-12-30-55.png, Clipboard_2021-09-16-12-40-18.png, Clipboard_2021-09-16-12-42-20.png, Clipboard_2021-09-16-12-46-31.png, Clipboard_2021-09-16-13-03-29.png, Clipboard_2021-09-16-13-03-39.png, Clipboard_2021-09-16-13-03-47.png, Clipboard_2021-09-16-13-04-05.png, Clipboard_2021-09-16-13-04-17.png, Clipboard_2021-09-16-13-06-14.png, Clipboard_2021-09-16-13-07-31.png, Clipboard_2021-09-16-13-07-41.png, Clipboard_2021-09-16-13-08-00.png, Clipboard_2021-09-16-13-09-20.png, Clipboard_2021-09-17-10-50-11.png, Clipboard_2021-09-17-10-52-09.png, Clipboard_2021-09-17-11-01-06.png, Clipboard_2021-09-17-11-09-45.png, Clipboard_2021-09-17-11-14-31.png]
tags: [Big Data Platforms]
title: Lesson 4 - 16/09
created: '2021-09-16T09:17:33.147Z'
modified: '2021-09-17T13:34:45.785Z'
---

# Lesson 4 - 16/09

## Fault tolerance strategies for storage

Hard disks are a common cause of faiulures in large scale systems (field reports say that ~3% avg yearly replacement rates). There are many ways to make more reliable storage out of unreliable hdds; some examples:
- RAID: Redundant Array of Independent Disks. For improve reliability usually used RAID 1, 5, 6, 10. Used inside servers and specialized SAN/NAS hw
- Replication and/or erasure coding of data inside datacenter, e.g. three way default replication in HDFS (on at least two different racks)
- Geographical replication across datacenters, e.g. Amazon S3 storage service

### RAID - Redundant Array of Independent Disks

RAID is the most commonly used fault tolerance mechanism in small scale. There are many different types of RAID, let's see them.
Notation: IOPS = I/O operations per second.

#### RAID 0 - Striping

![](@attachment/Clipboard_2021-09-17-10-50-11.png)

- Stripes data over a large number of disks to **improve sequential+random reads and writes**
- Very bad choice for fault tolerance, to be used only for scratch data that can be regenerated easily

#### RAID 1 - Mirroring

![](@attachment/Clipboard_2021-09-17-10-52-09.png)

- Each data block is written to two hard disks: first one is a master copy, secondly a mirror slave copy is written after the master
- Reads are served by either one of the two hdds
- Loses half the storage space and halves write bandwidth/IOPS compared to using two drives
- Data is available if either hdd is available
- Easy repair: replace the failed hdd and copy all the content on the replacement drive

#### RAID 5 - Block-level striping with distributed parity

![](@attachment/Clipboard_2021-09-17-11-01-06.png)

- Data is stored on `n+1` hdds, where a parity checksum block is stored for each `n` blocks of data. The parity blocks are distributed over all disks
- Tolerate one hdd failure

##### Properties

- Sequential reads and writes are striped over all disks
- Loses only one hdd worth of storage to parity
- Sequendial read and write speeds are good, and so is random read IOPS
- Random sall write requires reading one data block + parity block, requiring 4*IOPS in the worst case (battery backed up caches try to minimize this overhead tho)
- Rebuilding a disk requires reading all of the contents of the other `n` disks to regenerate the missing disk using parity. This is a slooow process
- Slow during rebuild: when one disk has failed, each missing data block read requires reading `n` blocks of data while rebuild is in progress
- Vulnerability to a second hdd failure during array rebuild and long rebuild thime -> most vendors reccomend RAID 6

##### RAID 5 Write Hole
A power failure during writes to a stripe can leave the array in a corrupted state. In order to avoid this problem, battery backed up memory caches and UPSs are used.
In the write hole scenario the parity no longer matches the contents of the data disks. This can cause garbage to be written to a data disk if one of the data disks fails and the incorrect parity is used to regenerate its contents.
Also, RAID 5 does not protect from a single drive with bad blocks from corrupting the array if a disk gives out bad data during parity calculations. Use of "hot spares" is recommended to speed up recovery start.

#### RAID 6 - Block-level striping with double distributed parity

![](@attachment/Clipboard_2021-09-17-11-09-45.png)

- Data is stored on n+2 hdds, where two different parity checksum blocks are stored for each `n` blocks of data
- Tolerate two hdds failure

##### Properties

- Sequential reads and writes are striped over all disks
- Loses two hdds worth of storage to parity
- Sequendial read and write speeds are good, and so is random read IOPS
- Random sall write requires reading one data block + parity block, requiring 6*IOPS in the worst case (battery backed up caches try to minimize this overhead tho)
- Slow during rebuild: when one/two disk/s has/ve failed, each missing data block read requires reading `n` blocks of data while rebuild is in progress
- Can tolerate one additional disk failure during rebuild, contents of any two missing disks can be rebuilt using the remaining n disks

##### RAID 6 Write Hole
A power failure during writes to a stripe can leave the array in a corrupted state. In order to avoid this problem, battery backed up memory caches and UPSs are used.
In two disks failures, RAID 6 does not protect from a single drive with bad blocks for corrupting the array if a disk gives out bad data during parity calculations.

#### RAID 10

![](@attachment/Clipboard_2021-09-17-11-14-31.png)

- Data is stored on `2n` hdds, each mirror pair consists of two hdds and the pairs are striped together to a single logical device

##### Properties

- Tolerate only one hdd failure in the worst case (n-one disk from each mirror pair)
- Loses half of the storage space
- Sequential reads and writes are striped over all disks
- Sequential read and write speeds are good, random read and write IOPS are good
- Each data block is written to two hdds. Random small write require 2*IOPS in the worst case
- Quicker during rebuild: missing data is just copied over from the other dism of the mirror. Use of "hot spares" recommended to speed up recovery start
- Can not tolerate the failure of both disks in the same mirror
- On unclean shutdown missing changes should be copied from the first master drive to the second slave drive, so **no write hole problem** if such ordering can be guaranteed
- If there is no ordering of writes, it is not easy to figure out which hdd has the most recent version of the data. Using an UPS on RAID 10 server is still a very good idea
- Does not protect from a single drive with bad blocks from corrupting the array during rebuild

### RAID usage scenarios

- RAID 0: Temporary space for data that can be discarded
- RAID 1: Small scale installations, server boot disks, etc
- RAID 5: Some fault tolerance with minimum storage overhead, small random writes can be problematic, RAID 5 Write Hole problem without special hardware (battery backed up cache / UPS)
- RAID 6: Fairly good fault tolerance with reasonable storage overhead, small random writes can be even more problematic than in RAID 5, RAID 6 Write Hole problem without special hardware (battery backed up cache / UPS)
- RAID 10: Less fault tolerant than RAID 6 but more fault tolerant than RAID 5. Loses half of the storage capacity. Good small random write performance makes this often the choice for database workloads. Can avoid the “Write hole problem” under write ordering guarantees

**!!!** RAID != BACKUPS

### RAID hardware issues

- RAID 1 and RAID 10 can be made safe with proper software configuration to use without special hardware support such as battery backed up caches and UPSs. We still recommend using UPS
- RAID 5 and RAID 6 are more storage efficient than RAID 10 but require specialized hardware to protect against corruption due to unclean shutdown (battery backed up cache / UPS)

### More on the write hole problem

- RAID 5 and RAID 6 need to atomically update all the drives in a stripe to remain consistent
- We know a way: Atomic Transactions (like in databases) -> RAID 5 and RAID 6 need a persistent transaction log in order to offer atomic transactions for updating a stripe
  - Battery backed up cache is a way to implement persistent transaction log
  - Another way: use a specific filesystem (that is aware of the underlyih storage subsystem) -> ZFS or BTRFS
    - ZFS: filesystem which is given full control over the raw disk devices and uses a copy-on-write transactional object model to achieve atomicity of updates. Has also integrity checksums

### Scaling Up storage

Quite a few problems in scaling up storage of thousands of hard disks:
- Price
- Performance is not so good as with a scale **out** solution
- Best solution only for modest storage needs

### Scaling Out storage

Quite a few problems also here:
- Using standard hw RAID will handle hard disk failures but does not prodect against storage server/RAID controller failures
- When having hundreds of servers and RAID controllers, failure is inevitable
- Data must be replicated on several servers
- If we need to replicate data over several server anyhow, why spend money on specialized hw RAID controllers? We just need the software: HDFS is a prime example

### HDFS Design decisions

- HDFS replication on a block level ~ RAID 10
- For small installations replicating each block twice in HDFS is enough -> RAID 10 storage layout
- For large installations each HDFS block is replicated threee times, also Linux RAID 10 can be configured to use a three hdds per mirror set

#### What about RAID 6 for HDFS?

- RAID 6 can improve the storage efficiency over RAID 10 significantly, as only two disks are lost to parity, and stripes can be made quite wide
- Even worse, HDFS has a 3x storage overhead

#### Erasure codes

- Many RAID 6 implementation use an erasure code, e.g. Reed-Solomon code, to implement the parity calculations and recovery of any two missing disks
- An erasure code can be extended to recover any number of missinig hdds; this is called **forward error correction code**
- e.g. an erasure code can be devised that stores 10 data blocks and 4 parity block (1.4x storage overhead), and this tolerates four disk failures

#### HDFS-EC

- Basically implements an erasure encoding of data: RAID 6 style but more parity block allowing for more simultaneous hard disk failures
- Exactly the same efficiency during rebuild (RAID 10) vs storage efficiency (RAID 6) considerations apply when deciding whether to use erasure codes at scale
- In HDFS-EC the erasure coding/replication can be configured independently for each directory in the HDFS

#### Avoiding the RAID Write Hole problem

- One should avoid "update in place", e.g. using the write-once-read-many pattern (WORM) to never modify data blocks once written
- In general the only hope to avoid the RAID 6 Hole problem is to have database-style transactions built into RAID 6 implementation: either in the hardware RAID controller or in a filesystem that does transaction logging
- One nice thing about a centralized HDFS NameNode is that it can be used as the central place to do atomic transactions on the HDFS filesystem state that are logged using transacrion logs
- ZFS and BTRFS never updates data in place in disk arrays but always do a fresh copy of it before modifications
- HDFS tries to minimize the amount of transactions by only doing them at file close time. No transactions for stripe updates are needed because stripe updates are disallowed

### Hard Disk Read Errors
- Unrecoverable Read Errors (UREs) are quite common. If an URE happens, the hdd gives out an error
- Consumer hdds have at most one error in 10¹⁵ bits read. Some entreprise ones one in 10¹⁶
- Problem: large storage systems read way more than 10¹⁵ bits of data during their lifetime -> additional checksums and error correction are needed at scale, e.g. HDFS stores a CRC32 checksum for every 512 bytes of data













