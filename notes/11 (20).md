---
attachments: [Clipboard_2021-11-22-17-01-36.png, Clipboard_2021-11-22-17-01-56.png, Clipboard_2021-11-22-17-02-13.png, Clipboard_2021-11-22-17-02-19.png, Clipboard_2021-11-22-17-02-28.png, Clipboard_2021-11-22-17-02-35.png, Clipboard_2021-11-22-17-02-47.png, Clipboard_2021-11-22-17-02-54.png, Clipboard_2021-11-22-17-03-14.png, Clipboard_2021-11-22-19-16-08.png, Clipboard_2021-11-22-19-18-05.png, Clipboard_2021-11-22-19-28-51.png, Clipboard_2021-11-22-22-08-59.png, Clipboard_2021-11-22-22-10-28.png, Clipboard_2021-11-22-22-10-53.png, Clipboard_2021-11-22-22-19-17.png, Clipboard_2021-11-22-22-21-22.png, Clipboard_2021-11-22-22-22-28.png, Clipboard_2021-11-22-22-26-52.png]
tags: [Introduction to Machine Learning]
title: Lesson 6 - 19/11
created: '2021-11-21T15:25:54.592Z'
modified: '2021-11-22T20:27:53.551Z'
---

# Lesson 6 - 19/11

## Gaussians in classification

Recap:
![](@attachment/Clipboard_2021-11-22-19-16-08.png)

TMYK: NB overfits less and it's usually more powerful. Classification with Bayes (still recap):
![](@attachment/Clipboard_2021-11-22-19-18-05.png)

- Choose nonevent = 0 and event = 1; let $I_0$ and $I_1$ be the row indices for nonevents and events respectively

![](@attachment/Clipboard_2021-11-22-17-01-36.png)

![](@attachment/Clipboard_2021-11-22-17-01-56.png)

Where class-centered covariance -> LDA and class-specific -> QDA.

### QDA

We use multidimensional gaussian distributions (in this case 2D). The gaussian is centered in the inner circle.

![](@attachment/Clipboard_2021-11-22-17-02-13.png)

![](@attachment/Clipboard_2021-11-22-17-02-19.png)

We apply this last formula (the graph is the probability plotted).

### LDA

![](@attachment/Clipboard_2021-11-22-17-02-28.png)

![](@attachment/Clipboard_2021-11-22-17-02-35.png)

### Naive Bayes

Here the variances could be different.
![](@attachment/Clipboard_2021-11-22-17-02-47.png)

![](@attachment/Clipboard_2021-11-22-17-02-54.png)

Decision boundaries are not linear, they are curved.
For some weird reasons this classifier works very good and needs less parameters.

#### Gaussian Naive Bayes

![](@attachment/Clipboard_2021-11-22-17-03-14.png)

### Number of parameters in the models

![](@attachment/Clipboard_2021-11-22-19-28-51.png)

Questions: How does the flexibility of different models compare? Are the inductive biases (distributional assumptions) reasonable?

### Discrete Naive Bayes

![](@attachment/Clipboard_2021-11-22-22-08-59.png)

So no Bayes assumption -> in order to determine an arbitrary distribution over X we would need |X|-1 parameters.

### Naive Bayes classifier

![](@attachment/Clipboard_2021-11-22-22-10-28.png)

#### Learning a NB classifier

![](@attachment/Clipboard_2021-11-22-22-10-53.png)

## How to move from probabilistic to "discrete" classifier

![](@attachment/Clipboard_2021-11-22-22-19-17.png)

### Discrete classifiers: performance measures

Small reminder on true positive/false negative etc...
![](@attachment/Clipboard_2021-11-22-22-21-22.png)

![](@attachment/Clipboard_2021-11-22-22-22-28.png)

### How to compare classifiers

![](@attachment/Clipboard_2021-11-22-22-26-52.png)
