---
attachments: [Clipboard_2022-01-26-12-30-09.png, Clipboard_2022-01-26-12-31-20.png, Clipboard_2022-01-26-12-34-13.png, Clipboard_2022-01-26-12-36-12.png, Clipboard_2022-01-26-12-38-36.png, Clipboard_2022-01-26-12-42-58.png, Clipboard_2022-01-26-12-44-26.png, Clipboard_2022-01-26-12-46-49.png, Clipboard_2022-01-26-12-50-25.png, Clipboard_2022-01-26-12-56-22.png, Clipboard_2022-01-26-13-00-50.png, Clipboard_2022-01-26-13-03-43.png, Clipboard_2022-01-26-13-03-50.png, Clipboard_2022-01-26-13-04-42.png, Clipboard_2022-01-26-13-06-50.png, Clipboard_2022-01-26-13-26-33.png, Clipboard_2022-01-26-13-27-14.png, Clipboard_2022-01-26-13-28-26.png, Clipboard_2022-01-26-13-31-00.png, Clipboard_2022-01-26-13-34-02.png, Clipboard_2022-01-26-13-34-19.png, Clipboard_2022-01-26-13-34-31.png, Clipboard_2022-01-26-13-34-59.png, Clipboard_2022-01-26-13-36-31.png, Clipboard_2022-01-26-13-36-47.png, Clipboard_2022-01-26-13-41-44.png, Clipboard_2022-01-26-13-42-04.png, Clipboard_2022-01-26-13-42-32.png, Clipboard_2022-01-26-13-43-23.png, Clipboard_2022-01-26-13-44-13.png, Clipboard_2022-01-26-13-46-35.png, Clipboard_2022-01-26-13-47-45.png, Clipboard_2022-01-26-13-51-01.png, Clipboard_2022-01-26-13-51-14.png, Clipboard_2022-01-26-13-54-03.png, Clipboard_2022-01-26-13-55-34.png, Clipboard_2022-01-26-13-56-02.png]
tags: [Information Retrieval]
title: Lesson 3 - 26/01
created: '2022-01-26T10:16:00.343Z'
modified: '2022-01-26T11:57:40.099Z'
---

# Lesson 3 - 26/01

## Web crawling

### Web repositories

- Web search engines create web repositories -> cache the web on local machines
  -> fast access to copies of the pages on the web
- A search engine aims to minimize the potential differences between its local repo and the web
- Mantain only the most recently crawled versions of web pages
- Provide mechanisms for both bulk and random access to stored pages

### Web crawler aka bot aka spider

- Subsystem of the search engine that downloads pages from the Web for storage in the web repository
- A large-scale web crawler has two tasks performed in tandem:
  - Task 1: locate previously undiscovered URLs by iteratively following hyperlinks (->increase coverage)
  - Task 2: (re)fetch from the web contents of the pages that are currently stored in the repository (->mantain freshness)
- In search engine companies there are multiple crawlers for different operations
  - Need to coordinate them: uncoordinated pose risks like DoS and/or saturation of the company's network bandwith
  -> web page fetching system (WPFS), shared by multiple crawlers

### WPFS

Gateway between crawlers and the web
In practice, usually composed of two subsystems:
- HLFS (High-level fetching system)
- LLFS (Low-level fetching system)

![](@attachment/Clipboard_2022-01-26-12-30-09.png)

#### Low level process

![](@attachment/Clipboard_2022-01-26-12-31-20.png)
![](@attachment/Clipboard_2022-01-26-12-34-13.png)

#### High level process

HLFS implements various crawling mechanisms and policies applicable to all operational crawlers in the search engine
- Implements throttling mechanism to prevent LLFS from being flooded with too many download requests
- Implements server-, host-, politeness constraints, to prevent websites being overloaded by page requests
HLFS also ensures LLFS establishes only a single connection to a particular server at any given time
- Assign requested webpages to a download queue
- All requests bound for same website added to same queue
- Each queue to a unique crawling thread
HLFS usually maintains a cache of recently downloaded pages
- Different crawlers take pages from cache if possible

Many websites uses `robots.txt` files with instructions for the web crawlers
![](@attachment/Clipboard_2022-01-26-12-36-12.png)

Sometimes robots file contains a link to a sitemap file:
![](@attachment/Clipboard_2022-01-26-12-38-36.png)

### Extending the web repository

- Crawlers only rely on hyperlinks to discover new URLs
- Web pages and underlying hyperlink structure can be viewed as a directed graph. Vertices = web pages, directed edges = hyperlinks
- Initially crawler needs some seed web pages that act as entry points to the web graph

The **discovery process** splits pages on the web into three diskoint sets:
- Pages whose content is already dowloaded by the crawler
- Pages whose URLs are discovered but whose content is not yet downloaded
- Pages whose URLs are not yet discovered

![](@attachment/Clipboard_2022-01-26-12-42-58.png)

Btw, modern large-scale web crawlers perform discovery incrementally and continuously

In each iteration:
![](@attachment/Clipboard_2022-01-26-12-44-26.png)

![](@attachment/Clipboard_2022-01-26-12-46-49.png)

#### Data structures

Are updated in an incremental fashion and keep growing as new pages are downloaded and links discovered

A **sieve** is a "queue with memory":
- Provides enqueue and dequeue primitives (like a queue)
- Each element enqueued will eventually be dequeued
- Sieve guarantees that if an element is enqueued multiple times it will only be dequeued once

Sieves are a fundamental data structure for crawlers

![](@attachment/Clipboard_2022-01-26-12-50-25.png)

All the operations in the sieve require only sequential access to all files involved. Points of note: ![](@attachment/Clipboard_2022-01-26-12-56-22.png)

### Back on crawler

- Another implementation issue is the order in which pages are downloaded:
  - Random
  - BF order
- Also, from the perspective of the search engine, some web pages are more valuabol then others -> URL prioritization
- Two complementary approaches to measure page quality:
  - ![](@attachment/Clipboard_2022-01-26-13-00-50.png)
  - ![](@attachment/Clipboard_2022-01-26-13-03-43.png), ![](@attachment/Clipboard_2022-01-26-13-03-50.png)

### Refreshing the web repository

What order?: ![](@attachment/Clipboard_2022-01-26-13-04-42.png)

![](@attachment/Clipboard_2022-01-26-13-06-50.png)

## Distributed web crawling

### Faster, better, stronger

Obtaining high coverage of the Web and maintaining freshness of the web repository requires sustaining high page download speeds – the most important efficiency objective for a crawler

### Multithreaded crawling

- Simplest form: the low-level system can be implemented as a single thread that fetches one page at a time
- It can be sped up using multithreaded crawling

### Distributed crawling

- Network bandwith is the bottleneck -> multithread is not enough -> **clusters**!

#### Redundancy

Leads to inefficiency; simple solution: partition the space of URLs among crawling nodes (firewall mode):
![](@attachment/Clipboard_2022-01-26-13-26-33.png)

![](@attachment/Clipboard_2022-01-26-13-27-14.png)

#### Coordination

Lack of it in firewall mode = links whose source and destination pages are assigned to different nodes cannot be followed by any node -> some pages will never be discovered (loss of coverage)

![](@attachment/Clipboard_2022-01-26-13-28-26.png)

To solve this: follow a link (only one hop) even if its destination URL is assigned to another node (cross-over mode). But it reintroduces redundancy

-> **Exchange mode**, the best:
- Discovered non-local links are communicated to the crawling nodes responsible for fetching them
- Volume of links needing to be communicated over the network can be significantly reduced by partitioning based on domain names instead of URLs
- Another optimization: communicate links in batches

![](@attachment/Clipboard_2022-01-26-13-31-00.png)

### Factors affecting crawling performance

- The Web poses various obstacles that can negatively affect the performance of web crawlers
- Some of these obstacles stem from the malicious intent of website owners
- Well-known examples include: delay attacks, spider traps, and link farms
- Although there is no malicious intent involved, certain other situations may also affect crawling performance (such as website mirroring)

![](@attachment/Clipboard_2022-01-26-13-34-02.png)

![](@attachment/Clipboard_2022-01-26-13-34-19.png)

![](@attachment/Clipboard_2022-01-26-13-34-31.png)

![](@attachment/Clipboard_2022-01-26-13-34-59.png)

### Other on web crawlers

Open source web crawlers:
![](@attachment/Clipboard_2022-01-26-13-36-31.png)

Specialized crawlers:
![](@attachment/Clipboard_2022-01-26-13-36-47.png)

## Indexing revisited

![](@attachment/Clipboard_2022-01-26-13-42-04.png)

### Terms, Parsing, Stemming

![](@attachment/Clipboard_2022-01-26-13-42-32.png)

![](@attachment/Clipboard_2022-01-26-13-43-23.png)

Problems:
- One word or two? (e.g. Hewlett-Packard)
- Numbers, dates
- Chinese:
  - No whitespace
  - Ambiguous segmentation (![](@attachment/Clipboard_2022-01-26-13-44-13.png))
- Other cases of no whitespace (e.g. dutch, german, inuit, finnish, swedish, greek, urdu)
- Arabic script:
  - Bidirectionality (![](@attachment/Clipboard_2022-01-26-13-46-35.png)), but luckily we have Unicode

### Normalization

Process of canonicalizing tokens so that matches occur despite superficial differences in character sequences.

![](@attachment/Clipboard_2022-01-26-13-47-45.png)

- Case folding
- Stop words (a, an, and, are, ..., were, will, with, ...)
- Lemmatization: ![](@attachment/Clipboard_2022-01-26-13-51-01.png)
- Stemming: ![](@attachment/Clipboard_2022-01-26-13-51-14.png)


#### Porter algorithm

![](@attachment/Clipboard_2022-01-26-13-54-03.png)

![](@attachment/Clipboard_2022-01-26-13-55-34.png)

![](@attachment/Clipboard_2022-01-26-13-56-02.png)

- In general, stemming increases effectiveness for some queries, and decreases effectiveness for others
- Porter Stemmer equivalence class oper contains all of: operate operating operates operation operative operatives operational
- Query where stemming hurts: “operating AND system”
