---
attachments: [Clipboard_2021-12-05-20-23-10.png, Clipboard_2021-12-05-20-24-13.png, Clipboard_2021-12-05-20-29-19.png, Clipboard_2021-12-05-20-32-23.png, Clipboard_2021-12-05-20-33-17.png, Clipboard_2021-12-05-20-34-31.png, Clipboard_2021-12-05-20-38-48.png, Clipboard_2021-12-05-20-49-32.png, Clipboard_2021-12-05-21-05-06.png, Clipboard_2021-12-05-21-20-13.png]
tags: [Introduction to Machine Learning]
title: Lesson 10 - 03/13
created: '2021-12-03T08:15:55.217Z'
modified: '2021-12-05T19:20:13.664Z'
---

# Lesson 10 - 03/13

## Dimensionality reduction

![](@attachment/Clipboard_2021-12-05-20-24-13.png)

Why? ![](@attachment/Clipboard_2021-12-05-20-29-19.png)

### Goodness criteria

![](@attachment/Clipboard_2021-12-05-20-23-10.png)

## PCA

You find linear projections of the data that have maximal variance:

![](@attachment/Clipboard_2021-12-05-20-32-23.png)

Is one instance of projection pursuit methods: ![](@attachment/Clipboard_2021-12-05-20-33-17.png)

![](@attachment/Clipboard_2021-12-05-20-34-31.png)

### Implementation and practical issues

![](@attachment/Clipboard_2021-12-05-20-38-48.png)

### Proportion of variance explained (PVE)

![](@attachment/Clipboard_2021-12-05-20-49-32.png)

### PCA <3 k-means clustering

![](@attachment/Clipboard_2021-12-05-21-05-06.png)

## Multidimensional scaling (MDS)

![](@attachment/Clipboard_2021-12-05-21-20-13.png)
