---
attachments: [Clipboard_2021-11-17-12-26-43.png, Clipboard_2021-11-17-12-26-51.png, Clipboard_2021-11-17-12-27-00.png, Clipboard_2021-11-17-12-41-30.png, Clipboard_2021-11-17-12-57-29.png, Clipboard_2021-11-17-12-57-38.png, Clipboard_2021-11-17-12-58-42.png, Clipboard_2021-11-20-18-58-15.png, Clipboard_2021-11-20-18-59-54.png, Clipboard_2021-11-20-19-01-30.png, Clipboard_2021-11-20-19-02-07.png, Clipboard_2021-11-20-19-10-41.png, Clipboard_2021-11-20-19-10-57.png, Clipboard_2021-11-21-17-19-25.png, Clipboard_2021-11-21-17-19-34.png, Clipboard_2021-11-21-17-19-44.png, Clipboard_2021-11-21-17-20-11.png, Clipboard_2021-11-21-17-20-19.png, Clipboard_2021-11-21-17-20-31.png, Clipboard_2021-11-21-17-20-59.png, Clipboard_2021-11-21-17-21-50.png, Clipboard_2021-11-21-17-22-11.png, Clipboard_2021-11-21-17-22-33.png, Clipboard_2021-11-21-17-24-41.png, Clipboard_2021-11-21-17-24-53.png]
tags: [Introduction to Machine Learning]
title: Lesson 5 - 17/11
created: '2021-11-17T09:57:31.678Z'
modified: '2021-11-21T15:24:53.511Z'
---

# Lesson 5 - 17/11

## Regularization

### Ridge and Lasso

- Feature subset selection is one way to control the complexity of the model
- We can also “softly” punish more complex models
- Observation: badly overfitting linear models often have large regression coefficients!
- Idea: we can constrain allowed models “softly” by punishing large regression coefficients
  - increase bias
  - decrease variance

![](@attachment/Clipboard_2021-11-20-18-58-15.png)

### Bayesian regularization

![](@attachment/Clipboard_2021-11-17-12-26-43.png)

![](@attachment/Clipboard_2021-11-17-12-26-51.png)

![](@attachment/Clipboard_2021-11-17-12-27-00.png)

## Classifiers

Classification is the same as the regression, with the following differences:
- The response variable is categorical (we will mainly discuss binary classification)

### Some definitions/notations

![](@attachment/Clipboard_2021-11-17-12-41-30.png)

### Logistic regression

![](@attachment/Clipboard_2021-11-17-12-57-29.png)

For convenience we use here class labels 0 and 1

![](@attachment/Clipboard_2021-11-20-18-59-54.png)

![](@attachment/Clipboard_2021-11-17-12-58-42.png)

Maximizing the likelihood (aka minimizing log-loss) is not as straightforward as in the case of linear regression; nevertheless, the problem is **convex**, i.e. gradient-based techniques exist to find the optimum

![](@attachment/Clipboard_2021-11-20-19-01-30.png)

In particular, if data is linearly separable, non-regularised solution tends to infinity.

![](@attachment/Clipboard_2021-11-20-19-02-07.png)

## Generative vs discriminative learning

- Logistic regression is an example of discriminative and probabilistic classifier that directly models the class distribution
- Another probabilistic wat to approach the problem is to use generative learning that builds a model for the whole joint distribution $P(x,y)$, often using the decomposition $P(x,y)=P(y)P(x|y)$
- Both have pros and cons:
  - Discriminative learning only solve the task you need to solve: may provide better accuracy since focused on the specific learning task, but optimization tends to be harder
  - Generative learning: often more natural to build models for $P(x|y)$ than for $P(y|x)$, handles data more naturally and optimization is often easier

Examples of discriminative classifiers:
  - Logistic regression
  - k-NN
  - Decision trees
  - Support Vector Machines

Examples of generative classifiers:
  - Naive Bayes
  - Linear Discriminant Analysis
  - Quadratic Discriminant Analysis

## Generative learning

![](@attachment/Clipboard_2021-11-20-19-10-41.png)

![](@attachment/Clipboard_2021-11-20-19-10-57.png)

### Normal distribution

For probabilistic models for real-valued features, one basic ingredient is the normal (aka Gaussian) distribution.

![](@attachment/Clipboard_2021-11-21-17-19-25.png)

![](@attachment/Clipboard_2021-11-21-17-19-34.png)

![](@attachment/Clipboard_2021-11-21-17-19-44.png)

![](@attachment/Clipboard_2021-11-21-17-20-11.png)

![](@attachment/Clipboard_2021-11-21-17-20-19.png)

![](@attachment/Clipboard_2021-11-21-17-20-31.png)

Why are we talking of this? Well,

![](@attachment/Clipboard_2021-11-21-17-20-59.png)

#### Gaussians in classification

LDA, QDA, Gaussian NB are obtained by modeling positive and negative example both with their own gaussian:

![](@attachment/Clipboard_2021-11-21-17-21-50.png)

Decision boundary is given by

![](@attachment/Clipboard_2021-11-21-17-22-11.png)

By substituting the formula for N into

![](@attachment/Clipboard_2021-11-21-17-22-33.png)

- If $\Sigma_{1}=\Sigma_{0}$ this is a linear equation, so the decision boundary is a hyperplane -> LDA
- In general case, this is a quadratic surface -> QDA
- If the correlation matrices are diagonal QDA becomes Gaussian NB

### Classification with Bayes

![](@attachment/Clipboard_2021-11-21-17-24-41.png)

![](@attachment/Clipboard_2021-11-21-17-24-53.png)




































