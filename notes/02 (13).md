---
attachments: [Clipboard_2022-02-11-12-20-12.png, Clipboard_2022-02-11-12-21-52.png, Clipboard_2022-02-11-12-25-35.png, Clipboard_2022-02-11-12-27-11.png, Clipboard_2022-02-11-12-28-31.png, Clipboard_2022-02-11-12-30-14.png, Clipboard_2022-02-11-12-59-41.png, Clipboard_2022-02-11-13-36-54.png, Clipboard_2022-02-11-13-40-24.png, Clipboard_2022-02-11-13-46-04.png, Clipboard_2022-02-11-13-46-08.png, Clipboard_2022-02-11-13-47-31.png]
tags: [Introduction to Deep Learning]
title: Lesson 8 - 11/02
created: '2022-02-11T10:18:15.665Z'
modified: '2022-02-11T11:51:46.437Z'
---

# Lesson 8 - 11/02

## Pooling layers

![](@attachment/Clipboard_2022-02-11-12-20-12.png)

Max pooling:
![](@attachment/Clipboard_2022-02-11-12-21-52.png)
Key idea: for image recognition, it is often not necessary to know the exact position of an object -> pooling can improve results by increasing spatial tolerance. It also brings long-distance items closer, allowing greater "view fields"

Average pooling:
![](@attachment/Clipboard_2022-02-11-12-25-35.png)

Overall architecture:
![](@attachment/Clipboard_2022-02-11-12-27-11.png)

## Train a CNN

![](@attachment/Clipboard_2022-02-11-12-28-31.png)

And CNN for NLP?
![](@attachment/Clipboard_2022-02-11-12-30-14.png)

## Regularization

Keep variance in control at the expense of increasing bias a bit

*same things on variance and bias *
*same things on how to divide data *

![](@attachment/Clipboard_2022-02-11-12-59-41.png)

First "control your parameters" technique: **L2 norm**
![](@attachment/Clipboard_2022-02-11-13-36-54.png)

![](@attachment/Clipboard_2022-02-11-13-40-24.png)

**L1 norm**
![](@attachment/Clipboard_2022-02-11-13-46-08.png)

![](@attachment/Clipboard_2022-02-11-13-46-04.png)

**L1 vs L2**:
![](@attachment/Clipboard_2022-02-11-13-47-31.png)
