---
attachments: [Clipboard_2021-11-08-10-26-25.png, Clipboard_2021-11-08-10-36-13.png, Clipboard_2021-11-08-10-39-20.png, Clipboard_2021-11-08-10-39-41.png, Clipboard_2021-11-08-10-42-45.png, Clipboard_2021-11-08-10-48-05.png, Clipboard_2021-11-08-10-50-28.png, Clipboard_2021-11-08-10-54-35.png, Clipboard_2021-11-08-10-55-07.png, Clipboard_2021-11-08-11-01-46.png, Clipboard_2021-11-08-11-21-23.png, Clipboard_2021-11-08-11-26-35.png, Clipboard_2021-11-08-11-26-47.png, Clipboard_2021-11-08-11-34-02.png, Clipboard_2021-11-08-11-42-43.png]
tags: [Trustworthy Machine Learning]
title: Lesson 3 - 08/11
created: '2021-11-08T08:11:42.198Z'
modified: '2021-11-08T09:55:47.700Z'
---

# Lesson 3 - 08/11

## Failure with summaries of several people

Assume we are allowed to publish summaries (e.g. averages) of data of n>N individuals abose some fixed bound N (5 is a very common bound). How can we assume we use such averages to reconstruct data for a given individual?

![](@attachment/Clipboard_2021-11-08-10-26-25.png)

## Differential privacy

![](@attachment/Clipboard_2021-11-08-10-36-13.png)

### Choice of neighbours

DP definition relies on neighbouring datasets D~D' differing only by single sample.
- Interpretation 1 (unbounded DP, add/remove ~R): neighbours can be obtained by adding or removing one entry (neighbours are of different size)
- Interpretation 1 (bounded DP, substitute ~S): neighbours can be obtained by replacing one entry (neighbours are of same size)

Need to be careful which definition is used, because there are not directly compatible, and some (theoretical) results only apply for one.

![](@attachment/Clipboard_2021-11-08-10-39-20.png)

### Property: Postprocessing

![](@attachment/Clipboard_2021-11-08-10-39-41.png)

This means that if you apply an algorithm to the data and you don't watch "old data" in relation with "new data", the guaranteed privacy is the same.

### Property: Composition

![](@attachment/Clipboard_2021-11-08-10-42-45.png)

This means that if you perform two analysis (two mechanisms) on the same dataset and release the results you still have DP but a weaker DP (epsilon is the sum oh the two DPs).

### Property: Group Privacy

![](@attachment/Clipboard_2021-11-08-10-48-05.png)

It's pretty self explanatory.

### Approximate differential privacy

![](@attachment/Clipboard_2021-11-08-10-50-28.png)

### Hypotesis testing interpretation of DP

![](@attachment/Clipboard_2021-11-08-10-54-35.png)

![](@attachment/Clipboard_2021-11-08-10-55-07.png)

### Generalisations of DP

- Essentially requires that a certain distance d(M(D), M(D')) is small
- Pure e-DP uses something akin to maximum-norm for probability distributions
- Generalisations possible by using different distance measures
- Generalisations generally allow larger privacy loss with a small probability

#### Most important generalisations

![](@attachment/Clipboard_2021-11-08-11-01-46.png)

### Laplace mechanism: release of a function value f(D)

![](@attachment/Clipboard_2021-11-08-11-21-23.png)

#### Privacy proof

![](@attachment/Clipboard_2021-11-08-11-26-35.png)

![](@attachment/Clipboard_2021-11-08-11-26-47.png)

### Sensitivity examples

![](@attachment/Clipboard_2021-11-08-11-34-02.png)

### Gaussian mechanism

![](@attachment/Clipboard_2021-11-08-11-42-43.png)

### Note on DP under finite precision arithmetic

Inaccuracy of floating point arithmetic may lead to violations of DP because not all inputs necessarily map to the same outputs. Some remedies are:
- Use a mechanism for finite precision
- Accept a small loss in $\delta$ for the inaccuracy

### The nature of DP guarantees

Some important implications of the DP definition are:
- DP stipulates a guarantee for all datasets (not just the one that you have)
- Guarantee can only be proven mathematically (not empirically, you can't try on every database)
- In theory, DP guarantee is information-theoretic, and applies even against computationally unbounded adversaries
- In practice, DP is only as strong as the random number generator used (if you can break the randomness you'd break break privacy)
