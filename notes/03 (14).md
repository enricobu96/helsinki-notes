---
attachments: [Clipboard_2022-03-23-10-40-24.png, Clipboard_2022-03-23-10-45-39.png, Clipboard_2022-03-23-10-50-01.png, Clipboard_2022-03-23-10-59-43.png, Clipboard_2022-03-23-11-01-39.png, Clipboard_2022-03-23-11-05-57.png, Clipboard_2022-03-23-11-12-37.png, Clipboard_2022-03-23-11-15-57.png, Clipboard_2022-03-23-11-22-56.png, Clipboard_2022-03-23-11-29-55.png, Clipboard_2022-03-23-11-39-03.png, Clipboard_2022-03-23-11-42-02.png, Clipboard_2022-03-23-11-42-10.png, Clipboard_2022-03-23-12-04-23.png, Clipboard_2022-03-23-12-08-29.png, Clipboard_2022-03-23-12-09-56.png, Clipboard_2022-03-23-12-11-36.png, Clipboard_2022-03-23-12-12-54.png, Clipboard_2022-03-23-12-13-35.png, Clipboard_2022-03-23-12-13-46.png, Clipboard_2022-03-23-13-16-02.png, Clipboard_2022-03-23-13-17-15.png, Clipboard_2022-03-23-13-31-10.png, Clipboard_2022-03-23-13-33-52.png, Clipboard_2022-03-23-13-36-12.png, Clipboard_2022-03-23-13-38-20.png]
tags: [Advanced Machine Learning]
title: Lesson 3 - 22/03
created: '2022-03-23T08:38:26.412Z'
modified: '2022-03-23T12:09:33.945Z'
---

# Lesson 3 - 22/03

## Probabilistic models

**Risk - Bayesian decision theory**
![](@attachment/Clipboard_2022-03-23-10-40-24.png)
![](@attachment/Clipboard_2022-03-23-10-45-39.png)

### Generative models

- Any probabilistic description of data
- Tells a story of how the data was supposedly generated
- Has some unknown parameters we should ground based on some observed data
- "All models are wrong but some are useful"

![](@attachment/Clipboard_2022-03-23-10-50-01.png)

Still on probabilistic modeling
![](@attachment/Clipboard_2022-03-23-10-59-43.png)

Instead of learning p($\theta$|D) we resort to simple point estimates:
![](@attachment/Clipboard_2022-03-23-11-01-39.png)

Risk minimization and ML/MAP
![](@attachment/Clipboard_2022-03-23-11-05-57.png)

**Loss examples**
![](@attachment/Clipboard_2022-03-23-11-12-37.png)

Probabilistic models often have 3 components:
- Observed variables
- Latent variables
- Parameters

Often:
![](@attachment/Clipboard_2022-03-23-11-15-57.png)

#### Building probabilistic models

![](@attachment/Clipboard_2022-03-23-11-22-56.png)

**Discrete Bayes Networks (example)**
![](@attachment/Clipboard_2022-03-23-11-29-55.png)
![](@attachment/Clipboard_2022-03-23-11-39-03.png)

**Conditional independence in ML models**
![](@attachment/Clipboard_2022-03-23-11-42-10.png)

## Optimization

We are interested in solving an optimization problem -> we need optimization tools

Three axes of complexity:
- Differenctiable vs non-smooth vs discrete
- Convex vs non-convex
- Uncosrained vs constrained

### Discrete optimization

The search space is almost alwats finite -> we can find the optimal solution by testing every value but the space is often too large -> we cannot differentiate, GD is not available

**Techniques**
![](@attachment/Clipboard_2022-03-23-12-04-23.png) 
Common optimization techniques:
![](@attachment/Clipboard_2022-03-23-12-08-29.png)
![](@attachment/Clipboard_2022-03-23-12-09-56.png)

### Convexity

![](@attachment/Clipboard_2022-03-23-12-11-36.png)

Properties:
![](@attachment/Clipboard_2022-03-23-12-12-54.png)

Some convex functions:
![](@attachment/Clipboard_2022-03-23-12-13-35.png)

In ML:
![](@attachment/Clipboard_2022-03-23-12-13-46.png)

**Convex optimization**
![](@attachment/Clipboard_2022-03-23-13-16-02.png)
![](@attachment/Clipboard_2022-03-23-13-17-15.png)

### Gradient Descent

![](@attachment/Clipboard_2022-03-23-13-31-10.png)

Watch out! The step size really matters
![](@attachment/Clipboard_2022-03-23-13-33-52.png)

### Non-convex optimization

![](@attachment/Clipboard_2022-03-23-13-36-12.png)

![](@attachment/Clipboard_2022-03-23-13-38-20.png)
