---
attachments: [Clipboard_2021-11-18-10-41-02.png, Clipboard_2021-11-18-10-48-20.png, Clipboard_2021-11-18-10-57-08.png, Clipboard_2021-11-18-11-02-11.png, Clipboard_2021-11-18-11-07-20.png, Clipboard_2021-11-18-11-08-21.png, Clipboard_2021-11-18-11-21-06.png, Clipboard_2021-11-18-11-23-18.png]
tags: [Trustworthy Machine Learning]
title: Lesson 6 - 18/11
created: '2021-11-18T08:16:43.582Z'
modified: '2021-11-18T09:52:08.141Z'
---

# Lesson 6 - 18/11

## Machine learning security

- Most of ML methods are designed for an assumed friendly environment
- Security research considers adversaries, who seek to circumvent the usual operation of a system for their own purposes
- In privacy, it is implicitly assumed that the adversary's goal is to uncover private information

### Security risks in the ML pipeline

![](@attachment/Clipboard_2021-11-18-10-41-02.png)

#### Data poisoning

![](@attachment/Clipboard_2021-11-18-10-48-20.png)

Changing just one training example -> multiple test prediction changed.Â´

E.g. spam filters:
- Mass-reporting innocent email as spam -> degrade filter accuracy
- Forced misclassification of specific examples
- Defensive techniques:
  - Monitoring
  - Adversarial training
  - Checking and sanitising input data and new deployed models

E.g. backdoors:
- Deep learning has been proved to performed well in learning arbitrary functions
- Data poisoning allows introducing backdoors triggered for example by:
  - A specific pixel pattern in an image
  - A specific colour pattern in a scene
  - A specific word in text
- ! Detecting a backdoor from trained models is essentially impossible without knowing the trigger (the space of possible triggers is huge)
- Defensive techniques:
  - Strong provenance of data and pre-trained models

#### Model stealing

Stealing a private model via API calls
- Potential use:
  - Exposing a confidential model
  - Improved creating of adversarial examples and other attacks
- Defensive techniques:
  - Monitoring
  - Rate limitation and sanitisation of queries

#### Adversarial examples

![](@attachment/Clipboard_2021-11-18-10-57-08.png)

![](@attachment/Clipboard_2021-11-18-11-02-11.png)

- ML systems and expecially DL are highly vulnerable to adversarial examples
- Finite data problem: need to compromise between accuracy and robustness
- Limiting access to the model makes attacks more difficult, but doesn't prevent them

Mitigation: adversarial training, i.e. minimise error against worst adversarial example
![](@attachment/Clipboard_2021-11-18-11-07-20.png)
But has limitations:
- Need to fix the type of perturbation to protect against
- Decreases generalisation performance on fixed data

### Conclusion

![](@attachment/Clipboard_2021-11-18-11-08-21.png)

## Testing DP-SGD privacy

### Basic idea

- Construct a model poisoning attack to inject a backdoor into the model, e.g. images with a specific pixel pattern will be classified in a specific way
- Evaluate the success rate the backdoor in T trials, translate into an estimate of $\epsilon$

![](@attachment/Clipboard_2021-11-18-11-21-06.png)

### Adversary instantiation

![](@attachment/Clipboard_2021-11-18-11-23-18.png)

### Conclusion

- DP-SGD privacy is tight against an adversary capable of arbitrary manipulation of the data set
- DP-bound bery likely understimates the level of privacy against weaker adversaries, especially in the high-$\epsilon$ regime
- How to set $\epsilon$ for practival work? -> small enough needed for strong guarantees can significantly reduce utility
- For reference: $\epsilon$ = -.1 -> strong privacy; = 1 -> moderate privacy; > 2 weak privacy
