---
attachments: [Clipboard_2021-09-14-10-18-03.png, Clipboard_2021-09-14-10-18-59.png, Clipboard_2021-09-14-10-22-31.png, Clipboard_2021-09-14-10-24-24.png, Clipboard_2021-09-14-10-26-21.png, Clipboard_2021-09-14-10-31-27.png, Clipboard_2021-09-14-10-32-24.png, Clipboard_2021-09-14-10-38-14.png, Clipboard_2021-09-14-10-38-31.png, Clipboard_2021-09-14-10-41-56.png, Clipboard_2021-09-14-10-42-37.png, Clipboard_2021-09-14-10-50-26.png, Clipboard_2021-09-14-11-00-22.png, Clipboard_2021-09-14-11-16-30.png, Clipboard_2021-09-14-11-32-14.png, Clipboard_2021-09-14-11-41-56.png]
tags: [Big Data Platforms]
title: Lesson 3 - 14/09
created: '2021-09-14T07:15:55.482Z'
modified: '2021-09-14T08:47:12.597Z'
---

# Lesson 3 - 14/09

## Some references
![](@attachment/Clipboard_2021-09-14-10-18-03.png)
![](@attachment/Clipboard_2021-09-14-10-18-59.png)

## Some libraries

### GraphFrames
- Parallel graph processing library on top of Spark, not part of spark core

![](@attachment/Clipboard_2021-09-14-10-22-31.png)

- Commonly used for large social network analysis problems
- Can represent graphs as GraphFrame, do simple graph operations, and complex graph algorithms using the Pregel BSP computing model
- Also contains several useful graph algorithms
- GraphX is good too

### MLlib
- Parallel ML library

![](@attachment/Clipboard_2021-09-14-10-24-24.png)

- Widely used for large scale ML but not the widest collection of algorithms nor the fastest implementation of any algorithm
- Tight integration with Spark SQL

### Spark SQL & Dataframes
- Parallel SQL databse implementation tailored for analytics (not realtime) workload
- Dataframes is the underlying data structure and processing engine that can also be directly used without the SQL frontend
- Dataframes programs can be optimized by the SQL query optimizer

![](@attachment/Clipboard_2021-09-14-10-26-21.png)

## Variables

### Dataframes
The Spark project has introduced a new data storage and processing framework (the dataframes) to evenually replace RDDs for many applications. In fact there's a problem with RDDs: they are operated by arbitrary user functions in Scala/Java/Python, and optimizing them is very hard.
-> Dataframes allow only a limited number of Dataframe native operations, and allows the Spark SQL optimizer to rewrite user Dataframe code to equivalent but faster codes; instead of executing what the user wrote (as with RDDs), execute an optimized sequence of operations. Also, allows dataframe operations to be also implemented in a **compiled** fashion.

### Broadcast variables
A way to send some read-only data to all PySpark workers in a coordinated fashion.
![](@attachment/Clipboard_2021-09-14-10-31-27.png)

### Accumulators
Allow a way to compute statistics done during operations.
![](@attachment/Clipboard_2021-09-14-10-32-24.png)
Note: if a transformation is rescheduled during operation exectuon, the accumulators are increased several times. Thus the accumulators can also count processing that is "wasted" due to fault tolerance/speculation.
**!!!** Use accumulators only to count statistics on processed items, not to do real computations.

### Buggy code misusing closures
When Spark is run in truly distributed mode, any changes made to worker local variables will be lost when the worker exists. The only way to communicate to other threads from workers is through data produced into RDDs and DataFrames.

### RDD Persistence levels
RDDs can be configured with different persistence levels for caching RDDs: these leves can be used to `persist()` RDD of DataFrame for further use. `unpersist()` frees the memory for other uses.

![](@attachment/Clipboard_2021-09-14-11-41-56.png)

## Advanced MapReduce

### Recap of phases
1. A **Master** (in Hadoop is called Job Tracker) is started -> coordinates the exection of a MapReduce Job. N.B.: it's a single point of failure
2. The master creates a predefined number of **M Map workers**, and assigns each one an input split to work on. It also later starts a predefined number of **R reduce workers**
3. Input is assigned to a free Map worker 64-128MB split at a time, and each **user defined Map function** is def `(key,value)` pairs as input and also produces `(key-value)` pairs
4. Periodically the Map workers flush their `(key,value)` pairs to the local hard disks, partitioning by their `key` to **R partitions**, one per reduce worker
5. When all the input splits have been processed, a **Shuffle phase** starts where _MxR file transfers_ are used to send alkl of the mapper outputs to the reducer handling each key partition. After reducer receives the input files, reducer **sorts (and groups)** the `(key,value)` pairs by the key
6. **User defined Reduce functions** iterate over the `(key,(...,list of values,...))` lists, generating output `(key,value)` pairs files, one per reducer

### Google MapReduce
- The user just supplies the Map and Reduce functions, nothing more
- The only means of communication between nodes is through the shuffle from a mapper to a reducer
- The framework can be used to implement a distributed sorting algorithm by using a custom partitioning function
- The framework does automatic parallelization and fault tolerance by using a centralised Master and a distributed filesystem that stores all data redudantly in compute nodes
- Uses functional programming paradigm to guarantee correctness of parallelization and to implement fault-tolerance by re-execution

## Detailed Hadoop Data Flow
![](@attachment/Clipboard_2021-09-14-10-50-26.png)

### Adding combiners
Combiners and Reuce functions run on the Map side. They can be used if the Reduce function is associative and commutative. N.B.: Spark requires this, and thus always runs "combiners".
![](@attachment/Clipboard_2021-09-14-11-00-22.png)

## Apache Hadoop
- Open-source implementation of the MapReduce framework
- Main principle: ship code to data, not data to code
- Map and Reduce workers are also storage nodes for the underlying distributed filesystem: Job allocation is first tried to a node having a copy of the data, and if that fails, then to a node in the same rack
- HDFS is still in very wide use
-When deciding whether MapReduce is the correct fit for an algorithm, one has to remember the fixed data-flow pattern of MapReduce. The algorithm has to be efficentily mapped to this data-flow pattern in order to efficenlty use the undelying computing hardware
- Builds reliable systems out of the unreliable commodity hardware by replaicating most components (exceptions: Master/Job Tracker and NameNode in HDFS)
- Tuned for large (GB) files
- Designed for very large (PB) datasets
- Designed for streaming data accesses in batch processing, designed for high bandwith instead of low latency
- NOT A POSIX FS!
- Written in Java, runs as a set of user-space daemons

## HDFS
- Distributed replicated filesystem: all data is replicated by default on three different Data Nodes
- Inspired by Google Filesystem
- Each node is usally a Linux compute node with a small number of hard disks (8-24)
- A NameNode that mantains the file locations, many DataNodes (1000+)
- HDFS is still often the filesystem of choice for Spark users
- Any piece of data is available if at least one datanode replica is up and running
- Rack optimized: by default one replica written locally, second in the same rack, thirs in another rack
- Uses large block size, 128MB is a common default (designed for batch processing)
- For scalability: write one, read many filesystem

### Implications of Write Once
All apllications need to be re-engineered to only do sequential writes -> Spark, MapReduce, etc...

### HDFS Architecture
![](@attachment/Clipboard_2021-09-14-11-16-30.png)
- NameNode is a single computer that maintains the namespace (meta-data) of the FS. Implementation detail: keeps all meta-data in memory, writes logs, and does periodic snapshots of the disk
- Recent new feature: namespace can be split between multiple NameNodes in HDFS Federation and ViewFS
- All data accesses are done directly to the DaataNodes
- Replica writes are done in a daisy chained (pipelined) fashion to maximize network utilization

### HDFS Scalabilty Limits
- 20PB+ deployed HDFS installations (10k+ hard disks) in 2009, 100+ PB in 2018
- 4000+ DataNodes
- Single NameNode scalability limits: the HDFS is NameMode scalability limited for write only workloads to around HDFS 10k clients
- The HDFS Federation and ViewFs features have been implemented to address scalability (not fault tolerance) issues

### HDFS Design Goals
Some of the original HDFS targets have been reached already by 2009
![](@attachment/Clipboard_2021-09-14-11-32-14.png)

### Spark and Hadoop Hardware
- Reasonable CPU speed, reasonable RAM amouts for each node (~256GB)
- 8-24 Hard disks per node; like one hard drive per core
- CPU speeds are growing faster than hdd speeds -> newer installations are moving to more hdds/node
- 10 gigabit ethernet
- Spark is able to use RAM for caching, tipically 32GB memory per node minimum (but up to 256GB RAM configurations are common)

#### Network Banwith Consideration
- Spark and Hadoop are fairly network latency insensitive
- Mapper reads can often be read from the local disk or the same rack with HDFS (intra-rack network bandwidth is cheaper than inter-rack bandwidth)
- For jobs with large Map output, Spark and Hadoop need large inter-node network bandwith (shuffle phase)
- To save network bandwidth, a minimum amount of shuffle data should be generated


















